{
  "xiang_fault-tolerant_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": " Fault-tolerant and Cost-efficient Workflow Scheduling Approach Based on Deep Reinforcement Learning for {IT} Operation and Maintenanc",
      "authors": [
        "Xiang, Yunsong",
        "Yang, Xuemei",
        "Sun, Yan",
        "Luo, Hong"
      ],
      "abstract": "With the promotion of cloud computing, a large number of hardware and software systems in the cloud bring massive and complex operation and maintenance (O\\&M) work. To ensure the O\\&M ef\ufb01ciency of {IT} infrastructures, it is necessary to implement automatic and reliable scheduling for the directed acyclic graph ({DAG}) work\ufb02ow which is composed of multiple O\\&M tasks. Considering the changing status of networks and machines in the cloud and the position constraints that some tasks must be executed on the speci\ufb01ed machines in some O\\&M scenarios, we propose a novel work\ufb02ow scheduling approach based on Deep Reinforcement Learning ({DRL}) to minimize the work\ufb02ow execution makespan and implement the fault tolerance with the position constraints of tasks execution. In our proposal, we \ufb01rst design a fault-tolerant mechanism according to the reliability requirement and the probability distributions of the machine failure parameters with consideration of different failure rates in the heterogeneous environment. Then, we employ proximal policy optimization ({PPO}) to optimize the task scheduling strategy and ensure the strategy to satisfy the position constraints of tasks execution by action masking in proximal policy optimization. The experimental results show that our proposal can effectively reduce the makespan of the faulttolerant work\ufb02ow on the premise of 99.9\\% reliability.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/CSCWD57460.2023.10152783"
    },
    "suggestion": {
      "relation": "This paper focuses on fault-tolerant workflow scheduling in IT operations using deep reinforcement learning, which is tangentially related to hardware fault tolerance in deep learning but not directly focused on deep learning computational errors.",
      "suggestion": "Mention this paper as an example of applying deep learning for fault tolerance in IT operations, highlighting the broader applicability of deep learning for fault tolerance.",
      "rating": 3
    },
    "sections": null
  },
  "goldstein_lightweight_2021": {
    "base": {
      "article_type": "inproceedings",
      "title": " Lightweight Error-Resiliency Mechanism for Deep Neural Network",
      "authors": [
        "Goldstein, Brunno F.",
        "Ferreira, Victor C.",
        "Srinivasan, Sudarshan",
        "Das, Dipankar",
        "Nery, Alexandre S.",
        "Kundu, Sandip",
        "Franca, Felipe M. G."
      ],
      "abstract": "In recent years, Deep Neural Networks ({DNNs}) have made inroads into a number of applications involving pattern recognition \u2013 from facial recognition to self-driving cars. Some of these applications, such as self-driving cars, have real-time requirements, where specialized {DNN} hardware accelerators help meet those requirements. Since {DNN} execution time is dominated by convolution, Multiply-and-Accumulate ({MAC}) units are at the heart of these accelerators. As hardware accelerators push the performance limits with strict power constraints, reliability is often compromised. In particular, power-constrained {DNN} accelerators are more vulnerable to transient and intermittent hardware faults due to particle hits, manufacturing variations, and \ufb02uctuations in power supply voltage and temperature. Methods such as hardware replication have been used to deal with these reliability problems in the past. Unfortunately, the duplication approach is untenable in a power constrained environment. This paper introduces a low-cost error-resiliency scheme that targets {MAC} units employed in conventional {DNN} accelerators. We evaluate the reliability improvements from the proposed architecture using a set of 6 {CNNs} over varying bit error rates ({BER}) and demonstrate that our proposed solution can achieve more than 99\\% of fault coverage with a 5-bits arithmetic code, complying with the {ASIL}-D level of {ISO}26262 standards with a negligible area and power overhead. Additionally, we evaluate the proposed detection mechanism coupled with a word masking correction scheme, demonstrating no loss of accuracy up to a {BER} of 10\u22122.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/ISQED51717.2021.9424287"
    },
    "suggestion": {
      "relation": "The paper introduces a lightweight error-resiliency mechanism specifically designed for DNN hardware accelerators, addressing the challenge of hardware faults in power-constrained environments.",
      "suggestion": "Cite this paper as a primary example of designing error-resiliency mechanisms for DNN accelerators to enhance fault tolerance without significant overhead.",
      "rating": 8
    },
    "sections": null
  },
  "chen_low-cost_2021": {
    "base": {
      "article_type": "inproceedings",
      "title": " Low-cost Fault Corrector for Deep Neural Networks through Range Restrictio",
      "authors": [
        "Chen, Zitao",
        "Li, Guanpeng",
        "Pattabiraman, Karthik"
      ],
      "abstract": "The adoption of deep neural networks ({DNNs}) in safety-critical domains has engendered serious reliability concerns. A prominent example is hardware transient faults that are growing in frequency due to the progressive technology scaling, and can lead to failures in {DNNs}. This work proposes Ranger, a low-cost fault corrector, which directly recti\ufb01es the faulty output due to transient faults without re-computation. {DNNs} are inherently resilient to benign faults (which will not cause output corruption), but not to critical faults (which can result in erroneous output). Ranger is an automated transformation to selectively restrict the value ranges in {DNNs}, which reduces the large deviations caused by critical faults and transforms them to benign faults that can be tolerated by the inherent resilience of the {DNNs}. Our evaluation on 8 {DNNs} demonstrates Ranger signi\ufb01cantly increases the error resilience of the {DNNs} (by 3x to 50x), with no loss in accuracy, and with negligible overheads.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/DSN48987.2021.00018"
    },
    "suggestion": {
      "relation": "This work proposes a low-cost fault corrector for DNNs, focusing on mitigating the impact of hardware transient faults, which is directly relevant to hardware error fault tolerance in deep learning.",
      "suggestion": "Use this paper to discuss innovative, cost-effective strategies for correcting hardware-induced errors in DNNs, emphasizing the importance of range restriction.",
      "rating": 9
    },
    "sections": null
  },
  "traiola_machine-learning-guided_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": " machine-learning-guided framework for fault-tolerant {DNNs",
      "authors": [
        "Traiola, Marcello",
        "Kritikakou, Angeliki",
        "Sentieys, Olivier"
      ],
      "abstract": "Deep Neural Networks ({DNNs}) show promising performance in several application domains. Nevertheless, {DNN} results may be incorrect, not only because of the network intrinsic inaccuracy, but also due to faults affecting the hardware. Ensuring the fault tolerance of {DNN} is crucial, but common fault tolerance approaches are not cost-effective, due to the prohibitive overheads for large {DNNs}. This work proposes a comprehensive framework to assess the fault tolerance of {DNN} parameters and costeffectively protect them. As a first step, the proposed framework performs a statistical fault injection. The results are used in the second step with classification-based machine learning methods to obtain a bit-accurate prediction of the criticality of all network parameters. Last, Error Correction Codes ({ECCs}) are selectively inserted to protect only the critical parameters, hence entailing low cost. Thanks to the proposed framework, we explored and protected two Convolutional Neural Networks ({CNNs}), each with four different data encoding. The results show that it is possible to protect the critical network parameters with selective {ECCs} while saving up to 79\\% memory w.r.t. conventional {ECC} approaches.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.23919/DATE56975.2023.10137033"
    },
    "suggestion": {
      "relation": "The paper presents a framework for assessing and enhancing the fault tolerance of DNNs against hardware faults, using machine learning-guided techniques to minimize overhead.",
      "suggestion": "Highlight this paper for its novel approach to evaluating and protecting DNNs from hardware errors, showcasing the use of machine learning for efficient fault tolerance.",
      "rating": 9
    },
    "sections": null
  },
  "gao_methodology_2023": {
    "base": {
      "article_type": "article",
      "title": " Methodology for the Design of Fault Tolerant Parallel Digital Channelizers on {SRAM}-{FPGAs",
      "authors": [
        "Gao, Zhen",
        "Xiao, Jiajun",
        "Liu, Qiang",
        "Ullah, Anees",
        "Reviriego, Pedro"
      ],
      "abstract": "Digital channelizers ({DCs}) based on the Discrete Fourier Transform ({DFT}) and polyphase filter banks are widely used in on-board processing ({OBP}) platforms to extract narrowband sub-channels from a wideband signal efficiently. In high-capacity communication satellite platforms there are always multiple {DCs} extracting narrowband signals from multiple wideband signals in parallel. Field-programmable gate arrays ({FPGAs}) are a popular option for the implementation of {DCs} due to their parallel computing capabilities and good reconfigurability, but {FPGAs} suffer single-event upsets ({SEUs}) on the space platform. This paper focuses on the efficient protection of parallel {DCs} with enhanced coding techniques. We first prove that a linear relationship between parallel {DCs} can be introduced and maintained among the multiple outputs. However, traditional coding schemes cannot be directly applied for the detection of faulty {DCs} due to the quantization noise introduced by fixed point implementations. To address this issue, we propose an enhanced coding scheme by averaging in the space and time domains to minimize the effect of quantization noise, introducing thresholds and a majority voter to further improve the detection probability. Both theoretical analysis and fault injection experiments prove the effectiveness of the proposed protection scheme. Experimental results show that all the {SEUs} that cause an {SNR} lower than 20dB can be detected and recovered, and the resource overheads are about 1.6 times and 1.3 times of that of the unprotected {DCs} for systems with 8 {DCs} and 16 {DCs}, respectively.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TCSI.2023.3239040"
    },
    "suggestion": {
      "relation": "Although focused on fault tolerance in digital channelizers for satellite communications, this paper's methodology for protecting against single-event upsets in FPGAs is indirectly relevant to ensuring reliability in hardware used for deep learning.",
      "suggestion": "Reference this paper to illustrate fault tolerance techniques in FPGA-based systems, drawing parallels to potential applications in deep learning hardware.",
      "rating": 4
    },
    "sections": null
  },
  "condia_multi-level_2022": {
    "base": {
      "article_type": "inproceedings",
      "title": " Multi-level Approach to Evaluate the Impact of {GPU} Permanent Faults on {CNN}'s Reliabilit",
      "authors": [
        "Condia, Josie E. Rodriguez",
        "Guerrero-Balaguera, Juan-David",
        "Dos Santos, Fernando F.",
        "Reorda, Matteo Sonza",
        "Rech, Paolo"
      ],
      "abstract": "Graphics processing units ({GPUs}) are widely used to accelerate Arti\ufb01cial Intelligence applications, such as those based on Convolutional Neural Networks ({CNNs}). Since in some domains in which {CNNs} are heavily employed (e.g., automotive and robotics) the expected lifetime of {GPUs} is over ten years, it is of paramount importance to study the impact of permanent faults (e.g. due to aging). Crucially, while the impact of transient faults on {GPUs} running {CNNs} has been widely studied, an accurate evaluation of the impact of permanent faults is still lacking. Performing this evaluation is challenging due to the complexity of {GPU} devices and the software implementing a {CNN}.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/ITC50671.2022.00036"
    },
    "suggestion": {
      "relation": "This paper evaluates the impact of permanent GPU faults on CNN reliability, which is crucial for understanding how hardware errors affect deep learning performance over time.",
      "suggestion": "Cite this work to discuss the significance of studying permanent faults in GPUs, especially in the context of their long-term use in deep learning applications.",
      "rating": 7
    },
    "sections": null
  },
  "cuenca-asensi_novel_2011": {
    "base": {
      "article_type": "article",
      "title": " Novel Co-Design Approach for Soft Errors Mitigation in Embedded System",
      "authors": [
        "Cuenca-Asensi, Sergio",
        "Martinez-Alvarez, Antonio",
        "Restrepo-Calle, Felipe",
        "Palomo, Francisco R.",
        "Guzman-Miranda, Hip\u00f3lito",
        "Aguirre, Miguel A."
      ],
      "abstract": "There is an increasing concern about the mitigation of radiation effects in embedded systems. This fact is demanding new \ufb02exible design methodologies and tools that allow dealing with design constraints and dependability requirements at the same time. This paper presents a novel proposal to design radiation-tolerant embedded systems combining hardware and software mitigation techniques. A hardening infrastructure, which facilitates the design space exploration and the trade-offs analyses, has been developed to support this fault tolerance co-design approach. The advantages of our proposal are illustrated by means of a case study.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TNS.2011.2112379"
    },
    "suggestion": {
      "relation": "The paper discusses a co-design approach for mitigating soft errors in embedded systems, which is somewhat related but focuses more on embedded system design rather than deep learning-specific hardware.",
      "suggestion": "Mention this paper to provide context on soft error mitigation in embedded systems, noting differences and potential inspirations for deep learning hardware fault tolerance.",
      "rating": 3
    },
    "sections": null
  },
  "bal_novel_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": " Novel Fault-Tolerant Architecture for Tiled Matrix Multiplicatio",
      "authors": [
        "Bal, Sandeep",
        "Mummidi, Chandra Sekhar",
        "Da Cruz Ferreira, Victor",
        "Srinivasan, Sudarshan",
        "Kundu, Sandip"
      ],
      "abstract": "General matrix multiplication ({GEMM}) is common to many scientific and machine-learning applications. Convolution, the dominant computation in Convolutional Neural Networks ({CNNs}), can be formulated as a {GEMM} problem. Due to its widespread use, a new generation of processors features {GEMM} acceleration in hardware. Intel recently announced an Advanced Matrix Multiplication ({AMX}\u00ae) instruction set for {GEMM}, which is supported by 1kB {AMX} registers and a Tile Multiplication unit ({TMUL}) for multiplying tiles (sub-matrices) in hardware. Silent Data Corruption ({SDC}) is a well-known problem that occurs when hardware generates corrupt output. Google and Meta recently reported findings of {SDC} in {GEMM} in their data centers. Algorithm-Based Fault Tolerance ({ABFT}) is an efficient mechanism for detecting and correcting errors in {GEMM}, but classic {ABFT} solutions are not optimized for hardware acceleration. In this paper, we present a novel {ABFT} implementation directly on hardware. Though the exact implementation of Intel {TMUL} is not known, we propose two different {TMUL} architectures representing two design points in the area-power-performance spectrum and illustrate how {ABFT} can be directly incorporated into the {TMUL} hardware. This approach has two advantages: (i) an error can be concurrently detected at the tile level, which is an improvement over finding such errors only after performing the full matrix multiplication; and (ii) we further demonstrate that performing {ABFT} at the hardware level has no performance impact and only a small area, latency, and power overhead.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.23919/DATE56975.2023.10136985"
    },
    "suggestion": {
      "relation": "It introduces a novel fault-tolerant architecture for matrix multiplication in hardware, directly applicable to improving the reliability of computations in deep learning.",
      "suggestion": "Highlight this paper for its contribution to fault-tolerant hardware design for matrix multiplication, a core operation in deep learning computations.",
      "rating": 8
    },
    "sections": null
  },
  "qu_reliability_2021": {
    "base": {
      "article_type": "inproceedings",
      "title": " Reliability Assessment Method Based on Convolutional Neural Networ",
      "authors": [
        "Qu, Hanbing",
        "Wang, Bo",
        "Zhao, Pu",
        "Xu, Tao",
        "Bian, Daokuan",
        "Wang, Yan"
      ],
      "abstract": "With the expansion of the power grid, the performance bottleneck of traditional reliability evaluation methods has gradually become prominent, and improving the speed of reliability evaluation has become an urgent problem to be solved. Reliability evaluation usually uses Monte Carlo simulation for reliability index calculation. In this process, load shedding calculation occupies the main computing resources. In response to this problem, this paper designs an improved united convolutional neural network to perform load shedding calculations and proposes a reliability calculation process based on {UCNN}. According to the characteristics of the power system, a data set construction method for reliability evaluation is proposed. By introducing a hierarchical sorting method, it can effectively avoid the low density of the data set and the high time-consuming construction process. At the same time, the random first-threepermutation sampling algorithm and the n-ary index random method broaden the possible states of the data set and reduce the complexity of sampling time, thereby providing effective support for model training. Finally, the reliability of the {IEEE}-{RTS}79 system is evaluated, and the results show that the method proposed in this paper is fast and effective.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/ICPSAsia52756.2021.9621618"
    },
    "suggestion": {
      "relation": "This paper focuses on using convolutional neural networks for reliability assessment in power grids, which is tangentially related to assessing hardware fault tolerance in deep learning but in a different application domain.",
      "suggestion": "Cite this paper to discuss the potential of using deep learning for reliability assessments beyond the typical domains, highlighting the versatility of DL approaches.",
      "rating": 3
    },
    "sections": null
  },
  "adam_selective_2021": {
    "base": {
      "article_type": "article",
      "title": " Selective Mitigation Technique of Soft Errors for {DNN} Models Used in Healthcare Applications: {DenseNet}201 Case Stud",
      "authors": [
        "Adam, Khalid",
        "Mohamed, Izzeldin Ibrahim",
        "Ibrahim, Younis"
      ],
      "abstract": "Deep neural networks ({DNNs}) have been successfully deployed in widespread domains, including healthcare applications. {DenseNet}201 is a new {DNN} architecture used in healthcare systems (i.e., presence detection of the surgical tool). Specialized accelerators such as {GPUs} have been used to speed up the execution of {DNNs}. Nevertheless, {GPUs} are prone to transient effects and other reliability threats, which can impact {DNN} models\u2019 reliability. Safety-critical systems, such as healthcare applications, must be highly reliable because minor errors might lead to severe injury or death. In this paper, we propose a selective mitigation technique that relies on in-depth analysis. First, we inject the {DenseNet}201 model implemented on a {GPU} via {NVIDIA}\u2019s {SASSIFI} fault injector. Second, we perform a comprehensive analysis from the perspective of kernel and layer to identify the most vulnerable portions of the injected model. Finally, we validate our technique by applying it to the top-vulnerable kernels to selectively protect the only sensitive portions of the model to avoid unnecessary overheads. Our experiments demonstrate that our mitigation technique achieves a signi\ufb01cant reduction in the percentage of errors that cause malfunction (errors that lead to misclassi\ufb01cation) from 6.463\\% to 0.21\\%. Moreover, the performance overhead (the execution time) of our technique is compared with the well-known protection techniques: Algorithm-Based Fault Tolerance ({ABFT}), Double Modular Redundancy ({DMR}), and Triple Modular Redundancy ({TMR}). The proposed solution shows only 0.3035\\% overhead compared to these techniques while correcting up 84.8\\% of the {SDC} errors in {DenseNet}201, remarkably improving the healthcare domain\u2019s model reliability.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/ACCESS.2021.3076716"
    },
    "suggestion": {
      "relation": "The paper presents a technique for mitigating soft errors in DNN models, specifically in healthcare applications, which directly aligns with the topic of hardware error fault tolerance in deep learning.",
      "suggestion": "Use this paper as a case study to illustrate effective fault tolerance techniques in DNNs, especially in critical applications like healthcare.",
      "rating": 8
    },
    "sections": null
  },
  "ruospo_survey_2023": {
    "base": {
      "article_type": "article",
      "title": " Survey on Deep Learning Resilience Assessment Methodologie",
      "authors": [
        "Ruospo, Annachiara",
        "Sanchez, Ernesto",
        "Luza, Lucas Matana",
        "Dilillo, Luigi",
        "Traiola, Marcello",
        "Bosio, Alberto"
      ],
      "abstract": "In the last few decades, deep learning ({DL}) has drastically enhanced the state of the art of applications such as computer vision, object detection, and language translation. For their outstanding computational capabilities, {DL} architectures, such as deep neural networks ({DNNs}), have become attractive solutions in safety-critical areas such as avionics, robotics, medical image analysis, and automotive. For this reason, the research community has shown increasing attention to understanding the resilience of {DL} models, which is defined as the capability to tolerate the presence of hardware faults. It is commonly argued that {DL} models have inherent fault-tolerant properties due to their distributed and parallel structure and their redundancy due to overprovisioning. Indeed, they can withstand the failure of a limited number of neurons and continue to function properly. Unfortunately, the choice of hardware on which {DL} applications run has also been shown to have an impact on resilience.2 In other words, {DL} resilience must be assessed by examining the entire system stack (hardware and software), making the overall process more complex and costly. Furthermore, {DL} models are not 100\\% accurate, which raises an important issue that needs to be considered when addressing the resilience of these systems. As an example, assume a classifier that, due to a fault, decreases its accuracy to 88\\% instead of the original 90\\%. Then, can the results be accepted (considering the device slightly degraded) or not? In other words, the metrics used to assess resilience need to be thoughtfully evaluated. Metrics generally vary depending on the specific {DL} model and the task it performs (for example, image classification or segmentation), but they all have in common the goal of measuring the degradation introduced by software or hardware faults. This article presents a systematic survey of existing methodologies developed for the assessment of {DL} resilience. Some of the most representative state-of-the-art academic and industrial works are analyzed and discussed. The article proposes a classification of evaluation techniques according to the level of abstraction and the models being evaluated. Moreover, a further contribution of the article is a comparative table indicating the costs and implementation efforts required to use any of the methodologies presented as well as the main advantages and disadvantages. This latter analysis is intended to highlight their strengths and weaknesses and to guide future research directions.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/MC.2022.3217841"
    },
    "suggestion": {
      "relation": "This survey covers deep learning resilience assessment methodologies, including the impact of hardware faults, which is directly relevant to the literature review topic.",
      "suggestion": "Reference this paper to provide a broad overview of the current state of research on DL model resilience, including hardware fault tolerance.",
      "rating": 9
    },
    "sections": null
  },
  "salih_survey_2022": {
    "base": {
      "article_type": "inproceedings",
      "title": " Survey on Software/Hardware Fault Injection Tools and Technique",
      "authors": [
        "Salih, Nadir K.",
        "Satyanarayana, D",
        "Alkalbani, Abdullah Said",
        "Gopal, R."
      ],
      "abstract": "Computational systems need to be dependable, but they are vulnerable to defects, errors, failure and faults, and they affect their behaviour. Fault injection is one of the useful techniques that have been used to evaluate the behaviour of the system by intentionally introduced fault, so as to determine its effect on the system behaviour. There are many tools and techniques used to inject faults, categorized under software and hardware techniques. To keep pace with progress and development over time, developers need to be aware off tools and techniques according to their needs, when using either software or hardware. In this paper we make a survey for several tools and techniques which are mostly used these days. As we compare between them based on their basic functions, source need, how to inject faults and feedback about each one.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/ISIEA54517.2022.9873679"
    },
    "suggestion": {
      "relation": "This paper surveys fault injection tools and techniques for evaluating system behavior under faults, which is related to understanding hardware fault tolerance but does not specifically address deep learning systems.",
      "suggestion": "Mention this paper when discussing general methodologies for assessing hardware fault tolerance, to provide context before focusing on deep learning specific tools and techniques.",
      "rating": 5
    },
    "sections": null
  },
  "gambardella_accelerated_2022": {
    "base": {
      "article_type": "inproceedings",
      "title": "ccelerated Radiation Test on Quantized Neural Networks trained with Fault Aware Trainin",
      "authors": [
        "Gambardella, Giulio",
        "Fraser, Nicholas J.",
        "Zahid, Ussama",
        "Furano, Gianluca",
        "Blott, Michaela"
      ],
      "abstract": "Quantized neural networks ({QNNs}) are increasingly considered for adoption on multiple applications, thanks to their high accuracy, but also since they allow for signi\ufb01cantly lower compute and memory footprints. While the theory behind {QNNs} is achieving a high level of maturity, several new challenges have arisen during {QNN} deployment. Reliable and safe implementations of {QNN} accelerators becomes pivotal, especially when targeting safety critical applications like automotive, industrial and aerospace, requiring innovative solutions and their careful evaluation. In this work we compare the accuracy of {QNNs} during accelerated radiation testing when trained with different methodologies and implemented with a data\ufb02ow architecture in \ufb01eld programmable gate arrays ({FPGA}). The initial experiment shows that {QNNs} trained with a novel methodology, called fault-aware training ({FAT}), which accounts for soft errors during neural network ({NN}) training, makes {QNNs} more resilient to single-event-effects ({SEEs}) in {FPGA}.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/AERO53065.2022.9843614"
    },
    "suggestion": {
      "relation": "It discusses the resilience of quantized neural networks to single-event-effects in FPGA, which is highly relevant to the topic of hardware fault tolerance in deep learning.",
      "suggestion": "Highlight this paper as an example of innovative training methodologies (FAT) that enhance hardware fault tolerance in deep learning models.",
      "rating": 7
    },
    "sections": null
  },
  "schorn_accurate_2018": {
    "base": {
      "article_type": "inproceedings",
      "title": "ccurate neuron resilience prediction for a flexible reliability management in neural network accelerator",
      "authors": [
        "Schorn, Christoph",
        "Guntoro, Andre",
        "Ascheid, Gerd"
      ],
      "abstract": "Deep neural networks have become a ubiquitous tool for mastering complex classi\ufb01cation tasks. Current research focuses on the development of power-ef\ufb01cient and fast neural network hardware accelerators for mobile and embedded devices. However, when used in safety-critical applications, for example autonomously operating vehicles, the reliability of such accelerators becomes a further optimization criterion which can stand in contrast to power-ef\ufb01ciency and latency. Furthermore, ensuring hardware reliability becomes increasingly challenging for shrinking structure widths and rising power densities in the nanometer semiconductor technology era. One solution to this challenge is the exploitation of fault tolerant parts in deep neural networks. In this paper we propose a new method for predicting the error resilience of neurons in deep neural networks and show that this method signi\ufb01cantly improves upon existing methods in terms of accuracy as well as interpretability. We evaluate prediction accuracy by simulating hardware faults in networks trained on the {CIFAR}-10 and {ILSVRC} image classi\ufb01cation benchmarks and protecting neurons according to the resilience estimations. In addition, we demonstrate how our resilience prediction can be used for a \ufb02exible trade-off between reliability and ef\ufb01ciency in neural network hardware accelerators.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.23919/DATE.2018.8342151"
    },
    "suggestion": {
      "relation": "This paper proposes a method for predicting neuron resilience in neural network accelerators, directly addressing the topic of hardware fault tolerance in deep learning.",
      "suggestion": "Cite this paper to discuss approaches for enhancing reliability in neural network hardware accelerators, emphasizing the importance of neuron resilience prediction.",
      "rating": 8
    },
    "sections": null
  },
  "akturk_acr_2020": {
    "base": {
      "article_type": "inproceedings",
      "title": "ACR}: Amnesic Checkpointing and Recover",
      "authors": [
        "Akturk, Ismail",
        "Karpuzcu, Ulya R."
      ],
      "abstract": "Systematic checkpointing of the machine state makes restart of execution from a safe state possible upon detection of an error. The time and energy overhead of checkpointing, however, grows with the frequency of checkpointing. Considering the growth of expected error rates, amortizing this overhead becomes especially challenging, as checkpointing frequency tends to increase with increasing error rates. Based on the observation that due to imbalanced technology scaling, recomputing a data value can be more energy ef\ufb01cient than retrieving (i.e., loading) a stored copy, this paper explores how recomputation of data values (which otherwise would be read from a checkpoint from memory or secondary storage) can reduce the machine state to be checkpointed, and thereby, the checkpointing overhead. Even in a relatively small scale system, recomputation-based checkpointing can reduce the storage overhead by up to 23.91\\%; time overhead, by 11.92\\%; and energy overhead, by 12.53\\%, respectively.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/HPCA47549.2020.00013"
    },
    "suggestion": {
      "relation": "Focuses on checkpointing strategies to recover from errors, which is a technique for fault tolerance, but it does not specifically address deep learning systems.",
      "suggestion": "Reference this paper to discuss checkpointing as a general fault tolerance strategy that could be adapted for deep learning systems.",
      "rating": 4
    },
    "sections": null
  },
  "zhao_aep_nodate": {
    "base": {
      "article_type": "article",
      "title": "AEP}: An Error-bearing Neural Network Accelerator for Energy Ef\ufb01ciency and Model Protectio",
      "authors": [
        "Zhao, Lei",
        "Zhang, Youtao",
        "Yang, Jun"
      ],
      "abstract": "Neural Networks ({NNs}) have recently gained popularity in a wide range of modern application domains due to its superior inference accuracy. With growing problem size and complexity, modern {NNs}, e.g., {CNNs} (Convolutional {NNs}) and {DNNs} (Deep {NNs}), contain a large number of weights, which require tremendous efforts not only to prepare representative training datasets but also to train the network. There is an increasing demand to protect the {NN} weight matrices, an emerging Intellectual Property ({IP}) in {NN} \ufb01eld. Unfortunately, adopting conventional encryption method faces signi\ufb01cant performance and energy consumption overheads.",
      "publication_year": "",
      "keywords": "",
      "doi": ""
    },
    "suggestion": {
      "relation": "Discusses protecting neural network weight matrices for energy efficiency and model protection, which is tangentially related to fault tolerance but focuses more on IP protection.",
      "suggestion": "Mention this paper to discuss the broader context of neural network model protection, including aspects beyond fault tolerance.",
      "rating": 3
    },
    "sections": null
  },
  "mirza_mohammed_evaluation_nodate": {
    "base": {
      "article_type": "article",
      "title": "n Evaluation of Major Fault Tolerance Techniques Used on High Performance Computing ({HPC}) Application",
      "authors": [
        "Mirza Mohammed, Akram Baig"
      ],
      "abstract": "High performance computing have a high number of constituent components used to facilitate data movement. Key characteristics of these systems include parallel processing, large memory, multiprocessor or multimode communication, and parallel file systems. Though they can turnaround computing in scenarios that need maximum processing power, {HPCs} face many challenges, key among them being fault tolerance. Today, most applications deal with faults by noting checkpoints frequently. Whenever a fault occurs, all the processes are terminated, and the task is loaded once again from the last checkpoint. Most applications deal with faults by noting checkpoints frequently. Whenever a fault occurs, all the processes are terminated, and the task is loaded once again from the last checkpoint. Key fault tolerance techniques used on {HPC} applications (reactive and proactive) were evaluated in this paper. Reactive protocols discussed include checkpointing/ restarting, replication, retry, and {SGuard}, while proactive techniques include preemptive migration, software rejuvenation, and self-healing strategy. As seen from the discussion on the drawbacks of each approach, efficient management of faults can best be achieved by using a hybrid system applying proactive and reactive measures simultaneously.",
      "publication_year": "",
      "keywords": "",
      "doi": ""
    },
    "suggestion": {
      "relation": "This paper evaluates fault tolerance techniques in High Performance Computing (HPC), focusing on both reactive and proactive strategies, which is tangentially related to deep learning hardware fault tolerance.",
      "suggestion": "Cite this paper to discuss general fault tolerance strategies in computing systems that could be applicable to deep learning systems.",
      "rating": 4
    },
    "sections": null
  },
  "rech_efficient_2013": {
    "base": {
      "article_type": "article",
      "title": "n Efficient and Experimentally Tuned Software-Based Hardening Strategy for Matrix Multiplication on {GPUs",
      "authors": [
        "Rech, P.",
        "Aguiar, C.",
        "Frost, C.",
        "Carro, L."
      ],
      "abstract": "Neutron radiation experiment results on matrix multiplication on graphic processing units ({GPUs}) show that multiple errors are detected at the output in more than 50\\% of the cases. In the presence of multiple errors, the available hardening strategies may become ineffective or inef\ufb01cient. Analyzing radiation-induced error distributions, we developed an optimized and experimentally tuned software-based hardening strategy for {GPUs}. With fault-injection simulations, we compare the performance and correcting capabilities of the proposed technique with the available ones.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TNS.2013.2252625"
    },
    "suggestion": {
      "relation": "The paper presents a software-based strategy for handling hardware-induced errors in GPUs, directly relevant to deep learning computations on similar hardware.",
      "suggestion": "Use this paper to illustrate a case study of fault tolerance in GPU-based deep learning computations.",
      "rating": 7
    },
    "sections": null
  },
  "schorn_efficient_2019": {
    "base": {
      "article_type": "inproceedings",
      "title": "n Efficient Bit-Flip Resilience Optimization Method for Deep Neural Network",
      "authors": [
        "Schorn, Christoph",
        "Guntoro, Andre",
        "Ascheid, Gerd"
      ],
      "abstract": "Deep neural networks usually possess a high overall resilience against errors in their intermediate computations. However, it has been shown that error resilience is generally not homogeneous within a neural network and some neurons might be very sensitive to faults. Even a single bit-\ufb02ip fault in one of these critical neuron outputs can result in a large degradation of the \ufb01nal network output accuracy, which cannot be tolerated in some safety-critical applications. While critical neuron computations can be protected using error correction techniques, a resilience optimization of the neural network itself is more desirable, since it can reduce the required effort for error correction and fault protection in hardware. In this paper, we develop a novel resilience optimization method for deep neural networks, which builds upon a previously proposed resilience estimation technique. The optimization involves only few steps and can be applied to pre-trained networks. In our experiments, we signi\ufb01cantly reduce the worst-case failure rates after a bit-\ufb02ip fault for deep neural networks trained on the {MNIST}, {CIFAR}-10 and {ILSVRC} classi\ufb01cation benchmarks.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.23919/DATE.2019.8714885"
    },
    "suggestion": {
      "relation": "Focuses on optimizing deep neural networks for resilience against bit-flip faults, which is highly relevant to hardware fault tolerance in deep learning.",
      "suggestion": "Highlight this paper as an example of internal neural network optimization for increased fault tolerance.",
      "rating": 8
    },
    "sections": null
  },
  "sun_insight_2020": {
    "base": {
      "article_type": "inproceedings",
      "title": "n Insight into Fault Propagation in Deep Neural Networks: Work-in-Progres",
      "authors": [
        "Sun, Ruoxu",
        "Zhan, Jinyu",
        "Jiang, Wei"
      ],
      "abstract": "Reliability is of critical importance for Deep Neural Networks ({DNNs}) applied in safety-critical applications. Traditional analysis of fault propagation in {DNNs} is not suitable for such applications. In this paper we approach to give the theory-driven analysis of fault propagation in {DNN}. Speci\ufb01cally, the perturbation on weights of layers are formulated and the propagation conditions of faults are obtained through theoretical derivation. All the analysis is based on {DNNs} with 32-bit \ufb02oat numbers. Finally, initial experiments on three typical {DNNs} are conducted to evaluate our theoretical results.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/EMSOFT51651.2020.9244034"
    },
    "suggestion": {
      "relation": "Provides a theoretical analysis of fault propagation in DNNs, directly addressing the reliability concerns in deep learning applications.",
      "suggestion": "Reference this paper for a theoretical perspective on fault propagation and its implications for deep learning reliability.",
      "rating": 7
    },
    "sections": null
  },
  "santos_analyzing_2019": {
    "base": {
      "article_type": "article",
      "title": "nalyzing and Increasing the Reliability of Convolutional Neural Networks on {GPUs",
      "authors": [
        "Santos, Fernando Fernandes Dos",
        "Pimenta, Pedro Foletto",
        "Lunardi, Caio",
        "Draghetti, Lucas",
        "Carro, Luigi",
        "Kaeli, David",
        "Rech, Paolo"
      ],
      "abstract": "Graphics processing units ({GPUs}) are playing a critical role in convolutional neural networks ({CNNs}) for image detection. As {GPU}-enabled {CNNs} move into safety-critical environments, reliability is becoming a growing concern. In this paper, we evaluate and propose strategies to improve the reliability of object detection algorithms, as run on three {NVIDIA} {GPU} architectures. We consider three algorithms: 1) you only look once; 2) a faster region-based {CNN} (Faster R-{CNN}); and 3) a residual network, exposing live hardware to neutron beams. We complement our beam experiments with fault injection to better characterize fault propagation in {CNNs}. We show that a single fault occurring in a {GPU} tends to propagate to multiple active threads, significantly reducing the reliability of a {CNN}. Moreover, relying on error correcting codes dramatically reduces the number of silent data corruptions ({SDCs}), but does not reduce the number of critical errors (i.e., errors that could potentially impact safety-critical applications). Based on observations on how faults propagate on {GPU} architectures, we propose effective strategies to improve {CNN} reliability. We also consider the bene\ufb01ts of using an algorithm-based fault-tolerance technique for matrix multiplication, which can correct more than 87\\% of the critical {SDCs} in a {CNN}, while redesigning maxpool layers of the {CNN} to detect up to 98\\% of critical {SDCs}.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TR.2018.2878387"
    },
    "suggestion": {
      "relation": "Evaluates and proposes strategies to improve CNN reliability on GPUs, focusing on fault propagation and correction, directly relevant to the review topic.",
      "suggestion": "Cite this paper to discuss specific strategies for improving deep learning reliability on GPU hardware.",
      "rating": 8
    },
    "sections": null
  },
  "wu_anatomy_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "natomy of High-Performance {GEMM} with Online Fault Tolerance on {GPUs",
      "authors": [
        "Wu, Shixun",
        "Zhai, Yujia",
        "Liu, Jinyang",
        "Huang, Jiajun",
        "Jian, Zizhe",
        "Wong, Bryan",
        "Chen, Zizhong"
      ],
      "abstract": "General Matrix Multiplication ({GEMM}) is a crucial algorithm for various applications such as machine learning and scientific computing since an efficient {GEMM} implementation is essential for the performance of these calculations. While researchers often strive for faster performance by using large computing platforms, the increased scale of these systems can raise concerns about hardware and software reliability. In this paper, we present a design of a highperformance {GPU}-based {GEMM} that integrates an algorithm-based fault tolerance scheme that detects and corrects silent data corruptions at computing units on-the-fly. We explore fault-tolerant designs for {GEMM} at the thread, warp, and threadblock levels, and also provide a baseline {GEMM} implementation that is competitive with or faster than the state-of-the-art, closed-source {cuBLAS} {GEMM}. We present a kernel fusion strategy to overlap and mitigate the memory latency due to fault tolerance with the original {GEMM} computation. To support a wide range of input matrix shapes and reduce development costs, we present a template-based approach for automatic code generation for both fault-tolerant and non-fault-tolerant {GEMM} implementations. We evaluate our work on {NVIDIA} Tesla T4 and A100 server {GPUs}. Our experimental results demonstrate that our baseline {GEMM} shows comparable or superior performance compared to the closed-source {cuBLAS}. Compared with the prior state-of-the-art non-fused fault-tolerant {GEMM}, our optimal fused strategy achieves a 39.04\\% speedup on average. In addition, our fault-tolerant {GEMM} incurs only a minimal overhead (8.89\\% on average) compared to {cuBLAS} even with hundreds of errors injected per minute. For irregularly shaped inputs, the code generator-generated kernels show remarkable speedups of 160\\% \u223c 183.5\\% and 148.55\\% \u223c 165.12\\% for fault-tolerant and nonfault-tolerant {GEMMs}, respectively, which outperforms {cuBLAS} by up to 41.40\\%.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1145/3577193.3593715"
    },
    "suggestion": {
      "relation": "Discusses integrating fault tolerance into GEMM operations on GPUs, essential for deep learning computations, with a focus on performance and reliability.",
      "suggestion": "Use this paper to explore fault tolerance techniques in core deep learning computational operations on GPUs.",
      "rating": 9
    },
    "sections": null
  },
  "abich_applying_2021": {
    "base": {
      "article_type": "article",
      "title": "pplying Lightweight Soft Error Mitigation Techniques to Embedded Mixed Precision Deep Neural Network",
      "authors": [
        "Abich, Geancarlo",
        "Gava, Jonas",
        "Garibotti, Rafael",
        "Reis, Ricardo",
        "Ost, Luciano"
      ],
      "abstract": "Deep neural networks ({DNNs}) are being incorporated in resource-constrained {IoT} devices, which typically rely on reduced memory footprint and low-performance processors. While {DNNs}\u2019 precision and performance can vary and are essential, it is also vital to deploy trained models that provide high reliability at low cost. To achieve an unyielding reliability and safety level, it is imperative to provide electronic computing systems with appropriate mechanisms to tackle soft errors. This paper, therefore, investigates the relationship between soft errors and model accuracy. In this regard, an extensive soft error assessment of the {MobileNet} model is conducted considering precision bitwidth variations (2, 4, and 8 bits) running on an Arm Cortex-M processor. In addition, this work promotes the use of a register allocation technique ({RAT}) that allocates the critical {DNN} function/layer to a pool of speci\ufb01c general-purpose processor registers. Results obtained from more than 4.5 million fault injections show that {RAT} gives the best relative performance, memory utilization, and soft error reliability trade-offs w.r.t. a more traditional replication-based approach. Results also show that the {MobileNet} soft error reliability varies depending on the precision bitwidth of its convolutional layers.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TCSI.2021.3097981"
    },
    "suggestion": {
      "relation": "Investigates soft error mitigation in DNNs on IoT devices, focusing on precision and reliability, which is indirectly related to the main topic.",
      "suggestion": "Mention this paper to discuss soft error mitigation techniques in low-power deep learning applications.",
      "rating": 5
    },
    "sections": null
  },
  "kosaian_arithmetic-intensity-guided_2021": {
    "base": {
      "article_type": "inproceedings",
      "title": "rithmetic-intensity-guided fault tolerance for neural network inference on {GPUs",
      "authors": [
        "Kosaian, Jack",
        "Rashmi, K. V."
      ],
      "abstract": "Neural networks ({NNs}) are increasingly employed in safety-critical domains and in environments prone to unreliability (e.g., soft errors), such as on spacecraft. Therefore, it is critical to impart fault tolerance to {NN} inference. Algorithm-based fault tolerance ({ABFT}) is emerging as an efficient approach for fault tolerance in {NNs}. We propose an adaptive approach to {ABFT} for {NN} inference that exploits untapped opportunities in emerging deployment scenarios. {GPUs} have high compute-to-memory-bandwidth ratios, while {NN} layers have a wide range of arithmetic intensities. This leaves some layers compute bound and others memory-bandwidth bound, but current approaches to {ABFT} do not consider these differences. We first investigate {ABFT} schemes best suited for each of these scenarios. We then propose intensity-guided {ABFT}, an adaptive, arithmetic-intensity-guided approach that selects the most efficient {ABFT} scheme for each {NN} layer. Intensity-guided {ABFT} reduces execution-time overhead by 1.09\u20135.3\u00d7 across many {NNs} compared to traditional approaches to {ABFT}.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1145/3458817.3476184"
    },
    "suggestion": {
      "relation": "Proposes an adaptive fault tolerance approach for NN inference on GPUs, considering arithmetic intensity, directly applicable to deep learning fault tolerance.",
      "suggestion": "Cite this paper for its novel approach to adaptive fault tolerance in neural network hardware implementations.",
      "rating": 8
    },
    "sections": null
  },
  "gava_assessment_2023": {
    "base": {
      "article_type": "article",
      "title": "ssessment of Radiation-Induced Soft Errors on Lightweight Cryptography Algorithms Running on a Resource-Constrained Devic",
      "authors": [
        "Gava, Jonas",
        "Moura, Nicolas",
        "Lucena, Joaquim",
        "Rocha, Vin\u00eccius Da",
        "Garibotti, Rafael",
        "Calazans, Ney",
        "Cuenca-Asensi, Sergio",
        "Bastos, Rodrigo Possamai",
        "Reis, Ricardo",
        "Ost, Luciano"
      ],
      "abstract": "Most safety-critical edge-computing devices rely on lightweight cryptography ({LWC}) algorithms to provide security at minimum power and performance overhead. {LWC} algorithms are traditionally embedded as a hardware component, but with the advance of the Internet of Things ({IoT}), emerging firmware is more likely to support cryptography algorithms to comply with different security levels and industry standards. This is the first work to present the soft error assessment of five cryptography algorithms executing in a low-power microprocessor running under neutron radiation, considering electronic code book ({ECB}) and counter ({CTR}) mode of operation implementations. Results obtained from two neutron radiation tests suggest that: 1) the {NOEKEON} algorithm gives the best relative soft error reliability, performance, power efficiency, and memory footprint utilization tradeoffs between the five algorithms considering both {ECB} and {CTR} implementations and 2) cryptography solutions based on the {CTR} mode of operation present better {FIT} rate for silent data corruption ({SDC}) and crash with respect to {ECB} implementations.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TNS.2023.3253684"
    },
    "suggestion": {
      "relation": "This paper focuses on assessing the impact of radiation-induced soft errors on lightweight cryptography algorithms in edge-computing devices, which is tangentially related to hardware fault tolerance but does not directly address deep learning computational errors.",
      "suggestion": "Cite this paper to discuss the broader context of hardware fault tolerance in edge-computing devices, particularly in relation to security algorithms under radiation conditions.",
      "rating": 3
    },
    "sections": null
  },
  "lojda_automated_2023": {
    "base": {
      "article_type": "article",
      "title": "utomated design and usage of the Fault-Tolerant dynamic partial reconfiguration controller for {FPGAs",
      "authors": [
        "Lojda, Jakub",
        "Panek, Richard",
        "Sekanina, Lukas",
        "Kotasek, Zdenek"
      ],
      "abstract": "This article presents a new design automation method for Fault-Tolerant ({FT}) systems implemented on dynamically reconfigurable Field Programmable Gate Arrays ({FPGAs}). The method aims at minimizing the human interactions needed to incorporate {FT} mechanisms into an existing system. It starts with a source code of an original unhardened circuit. It continues by automated manipulation of the source code, algorithmic strategic selection of suitable {FT} techniques, design space exploration of candidate {FT} implementations, and selection of the resulting implementation. The method also includes efficient evaluation of achieved {FT} parameters performed on the target {HW}. As a novel approach working on the level of {HW} description languages is employed, the code modification is separated, which differentiates our method from others. The case study utilizing this method targets the design of an experimental {FT} dynamic partial reconfiguration controller for an {FPGA}. This controller is helpful for the restoration of faulty components due to a single-event upset on an {FPGA}. We used the method to generate a set of Pareto-optimal controllers concerning the design\u2019s Mean Time to Failure ({MTTF}) parameter, power consumption, and size. Then, the {FT} controller is connected to several benchmark circuits, and the reliability parameters are evaluated at the entire system level. Our results show that by replacing the standard reconfigurable controller with our automatically-designed {FT} controller for one specific benchmark, the design size increased by 20.1\\%, and {MTTF} increased by 11.7\\%. However, the efficiency is highly dependent on the target system size, {MTTF}, and circuit functionality. We also estimate that a complex system defined by half a million configuration bits would improve {MTTF} by more than 50\\%.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.microrel.2023.114976"
    },
    "suggestion": {
      "relation": "The paper presents a method for designing fault-tolerant systems on FPGAs, including automated design and evaluation of fault tolerance, which is highly relevant to the design aspect of hardware fault tolerance in deep learning systems.",
      "suggestion": "Use this paper to illustrate an example of automated design methods for fault-tolerant hardware that could be applied to deep learning accelerators.",
      "rating": 7
    },
    "sections": null
  },
  "shi_automated_2023": {
    "base": {
      "article_type": "article",
      "title": "utomated Model Hardening with Reinforcement Learning for On-Orbit Object Detectors with Convolutional Neural Network",
      "authors": [
        "Shi, Qi",
        "Li, Lu",
        "Feng, Jiaqi",
        "Chen, Wen",
        "Yu, Jinpei"
      ],
      "abstract": "On-orbit object detection has received extensive attention in the \ufb01eld of arti\ufb01cial intelligence ({AI}) in space research. Deep-learning-based object-detection algorithms are often computationally intensive and rely on high-performance devices to run. However, those devices usually lack spacequali\ufb01ed versions, and they can hardly meet the reliability requirement if directly deployed on a satellite platform, due to software errors induced by the space environment. In this paper, we evaluated the impact of space-environment-induced software errors on object-detection algorithms through large-scale fault injection tests. Aside from silent data corruption ({SDC}), we propose an extended criterial {SDC}-0.1 to better quantify the effect of the transient faults on the object-detection algorithms. Considering that a bit-\ufb02ip error could cause severe detection result corruption in many cases, we propose a novel automated model hardening with reinforcement learning ({AMHR}) framework to solve this problem. {AMHR} searches for error-sensitive kernels in a convolutional neural network ({CNN}) through trial and error with a deep deterministic policy gradient ({DDPG}) agent and has \ufb01ne-grained modular-level redundancy to increase the fault tolerance of the {CNN}-based object detectors. Compared to other selective hardening methods, {AMHR} achieved the lowest {SDC}-0.1 rates for various detectors and could tremendously improve the mean average precision ({mAP}) of the {SSD} detector by 28.8 in the presence of multiple errors.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.3390/aerospace10010088"
    },
    "suggestion": {
      "relation": "This paper evaluates the impact of space-environment-induced software errors on deep learning object detection algorithms and proposes a reinforcement learning framework for model hardening, which indirectly relates to hardware fault tolerance through software-level solutions.",
      "suggestion": "Reference this paper for discussing software-level approaches to enhancing fault tolerance in deep learning systems exposed to harsh environments.",
      "rating": 5
    },
    "sections": null
  },
  "shi_average_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "verage Task Execution Time Minimization under (m, k) Soft Error Constrain",
      "authors": [
        "Shi, Junjie",
        "Ueter, Niklas",
        "Chen, Jian-Jia",
        "Chen, Kuan-Hsun"
      ],
      "abstract": "Safety-critical systems are often subjected to transient faults. Since these transient faults may lead to soft errors that cause catastrophic consequences, error-handling must be addressed by design. Full-protection against faults is too costly in terms of resource usage. A common approach to relax the resource demands and limit the impact of errors is to consider (m, k)-constraints, which requires that at least m jobs out of any k consecutive jobs are error-free. To assure (m, k)-compliance, static patterns are widely used to select the job execution modes, i.e., either in an error-free mode at the cost of increased worst-case execution time or in an error-prone mode with the advantage of less execution time. Although static patterns have been shown to be effective in energy-aware designs, resource over-provision is inevitable due to the relatively low rate of error probability. In this work, we propose two dynamic (and adaptive) approaches that allow the scheduler to opportunistically select execution modes based on the error-history of the past jobs and the actual error probability. We \ufb01rstly propose a Markov chain based solution if the error-probability is known and static and secondly a reinforcement learning-based approach that can handle unknown error probabilities. Experimental evaluations show that our approaches outperform the state-of-the-art in most of the evaluated cases in terms of average utilization for each task and the overall utilization for multitask systems.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/RTAS58335.2023.00008"
    },
    "suggestion": {
      "relation": "The study introduces dynamic approaches to minimize execution time under soft error constraints in safety-critical systems, which indirectly contributes to the discussion on hardware error fault tolerance by optimizing error handling.",
      "suggestion": "Mention this paper when discussing dynamic and adaptive error handling strategies that could complement hardware fault tolerance in deep learning systems.",
      "rating": 6
    },
    "sections": null
  },
  "ozen_boosting_2020": {
    "base": {
      "article_type": "article",
      "title": "oosting Bit-Error Resilience of {DNN} Accelerators Through Median Feature Selectio",
      "authors": [
        "Ozen, Elbruz",
        "Orailoglu, Alex"
      ],
      "abstract": "Deep learning techniques have enjoyed wide adoption in real life, including in various safety-critical embedded applications. While neural network computations require protection against hardware errors, the substantial overheads of conventional error-tolerance techniques limit their use on embedded platforms, which carry out demanding deep neural network computations with limited resources. The utilization of conventional techniques is further constrained in high error rate scenarios, increasingly prevalent under aggressive energy and performance optimizations. To resolve this conundrum, we introduce a novel median feature selection technique to \ufb01lter the impact of bit errors prior to the execution of each layer. While our technique can be deemed as a \ufb01ne-grained modular redundancy scheme, its construction purely out of the inherent redundancy of the network necessitates neither additional parameters nor extra multiply-accumulate operations, squashing the inordinate overheads typically associated with such techniques. Median feature selection can be ef\ufb01ciently performed in hardware and seamlessly integrated into embedded deep learning accelerators as a modular plug-in. Deep learning models can be trained with standard tools and techniques to ensure a graceful operational interface with the feature selection stages. The proposed technique allows the system to perform accurately even at high error rates by improving its resilience up to four orders of magnitude, yet incurs negligible 0.19\\%\u20130.48\\% area and 0.07\\%\u20130.19\\% power overheads for the required operations.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TCAD.2020.3012209"
    },
    "suggestion": {
      "relation": "This paper introduces a novel technique for enhancing the bit-error resilience of DNN accelerators, directly addressing the challenge of hardware error fault tolerance in deep learning computations.",
      "suggestion": "Highlight this paper as a key example of innovative hardware-level fault tolerance techniques specifically designed for deep learning accelerators.",
      "rating": 9
    },
    "sections": null
  },
  "liang_c-dmr_2023": {
    "base": {
      "article_type": "article",
      "title": "-{DMR}: a cache-based fault-tolerant protection method for register fil",
      "authors": [
        "Liang, Zongnan",
        "Nian, Jiawei",
        "Liu, Hongjin",
        "Wang, Xuru",
        "Yang, Mengfei"
      ],
      "abstract": "The processor in the space environment is susceptible to the interference of highenergy particles, resulting in abnormal operation of the processor. These processors require fault-tolerant designs to handle various disturbances in the environment. In this paper, we propose a cache-based fault-tolerant protection method for the register file and we implement it in a {RISC}-V processor on the {FPGA} platform. This method uses spatial redundancy and information redundancy to reduce the propagation of single-bit errors, and it can resolve potential fault accumulation issues in the register file. Compared with other methods for register files, the proposed implementation has advantages in resource consumption by reusing the inherent data cache structure in the processor, only increasing 76\\% look-up tables and causing 6.09\\% extra delay. Finally, we evaluate the impact on system performance after removing some cachelines from the data cache and get conclusion that this design improves the processor\u2019s fault tolerance with small impact on the original data cache performance.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1007/s11227-022-04836-2"
    },
    "suggestion": {
      "relation": "The paper proposes a cache-based fault-tolerant protection method for register files in processors, which is relevant to the general theme of hardware fault tolerance but not specifically tailored to deep learning applications.",
      "suggestion": "Cite this paper to discuss general hardware fault tolerance strategies that could potentially be adapted for deep learning hardware.",
      "rating": 4
    },
    "sections": null
  },
  "yu_cola_nodate": {
    "base": {
      "article_type": "article",
      "title": "COLA}: Orchestrating Error {COding} and {LeArning} for Robust Neural Network Inference Against Hardware Defect",
      "authors": [
        "Yu, Anlan",
        "Lyu, Ning",
        "Yin, Jieming",
        "Yan, Zhiyuan",
        "Wen, Wujie"
      ],
      "abstract": "Error correcting output codes ({ECOCs}) have been proposed to improve the robustness of deep neural networks ({DNNs}) against hardware defects of {DNN} hardware accelerators. Unfortunately, existing efforts suffer from drawbacks that would greatly impact their practicality: 1) robust accuracy (with defects) improvement at the cost of degraded clean accuracy (without defects); 2) no guarantee on better robust or clean accuracy using stronger {ECOCs}. In this paper, we first shed light on the connection between these drawbacks and error correlation, and then propose a novel comprehensive error decorrelation framework, namely {COLA}. Specifically, we propose to reduce inner layer feature error correlation by 1) adopting a separated architecture, where the last portions of the paths to all output nodes are separated, and 2) orthogonalizing weights in common {DNN} layers so that the intermediate features are orthogonal with each other. We also propose a regularization technique based on total correlation to mitigate overall error correlation at the outputs. The effectiveness of {COLA} is first analyzed theoretically, and then evaluated experimentally, e.g., up to 6.7\\% clean accuracy improvement compared with the original {DNNs} and up to 40\\% robust accuracy improvement compared to the state-ofthe-art {ECOC}-enhanced {DNNs}.",
      "publication_year": "",
      "keywords": "",
      "doi": ""
    },
    "suggestion": {
      "relation": "This paper explores error coding and learning techniques to improve neural network inference robustness against hardware defects, directly addressing hardware fault tolerance in the context of DNN accelerators.",
      "suggestion": "Use this paper to discuss error coding and learning as strategies for achieving fault tolerance in deep learning hardware.",
      "rating": 8
    },
    "sections": null
  },
  "hsieh_cost-effective_2023": {
    "base": {
      "article_type": "article",
      "title": "ost-effective memory protection and reliability evaluation based on machine error-tolerance: A case study on no-accuracy-loss {YOLOv}4 object detection mode",
      "authors": [
        "Hsieh, Tong-Yu",
        "Tsai, Ching-Yeh",
        "Hou, Sian-Jhang",
        "Chao, Wei-Ji"
      ],
      "abstract": "In this paper, we investigate and present a cost-effective memory protection and reliability evaluation meth\u00ad odology for machine learning systems based on machine error-tolerance. We show that by well exploiting the inherent error-tolerability, the incurred cost of the typical memory protection methods including {ECC} (Error Correction Code) and {TMR} (Triple Modular Redundancy) can be greatly reduced, making these methods attractive to be adopted to implement a reliable machine learning system. In particular, we also target the up-todate powerful object detection machine learning model {YOLOv}4 as a case study. To the best of our knowledge, there is no work in the literature addressing reliability evaluation and enhancement for {YOLOv}4. Based on identifying the set of error-sensitive (critical) memory blocks and protecting only this set, we develop more efficient {ECC} and {TMR} methods. Our {ECC} method does not require any additional memory cost, while the area cost of our {TMR} method can be reduced from 200 \\% to only 47.5 \\%. The reliability evaluation results show that the Mean Time to Failure ({MTTF}) of the {YOLOv}4 object detection system can be extended by about 12 times by the proposed {ECC} method, while the {TMR} method can even achieve 108 times longer than that of {ECC}. We also present a generic methodology to exploit machine error-tolerance for developing a cost-effective memory pro\u00ad tection method.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.microrel.2023.115039"
    },
    "suggestion": {
      "relation": "The study presents a cost-effective memory protection method for machine learning systems, focusing on error tolerance in memory for YOLOv4 object detection, which is indirectly related to hardware fault tolerance in deep learning.",
      "suggestion": "Reference this paper to discuss cost-effective memory protection methods that could enhance hardware fault tolerance in deep learning systems.",
      "rating": 6
    },
    "sections": null
  },
  "nguyen_craft_2023": {
    "base": {
      "article_type": "article",
      "title": "CRAFT}: Criticality-Aware Fault-Tolerance Enhancement Techniques for Emerging Memories-Based Deep Neural Network",
      "authors": [
        "Nguyen, Thai-Hoang",
        "Imran, Muhammad",
        "Choi, Jaehyuk",
        "Yang, Joon-Sung"
      ],
      "abstract": "Deep neural networks ({DNNs}) have emerged as the most effective programming paradigm for computer vision and natural language processing applications. With the rapid development of {DNNs}, ef\ufb01cient hardware architectures for deploying {DNN}-based applications on edge devices have been extensively studied. Emerging nonvolatile memories ({NVMs}), with their better scalability, nonvolatility, and good read performance, are found to be promising candidates for deploying {DNNs}. However, despite the promise, emerging {NVMs} often suffer from reliability issues, such as stuck-at faults, which decrease the chip yield/memory lifetime and severely impact the accuracy of {DNNs}. A stuck-at cell can be read but not reprogrammed, thus, stuck-at faults in {NVMs} may or may not result in errors depending on the data to be stored. By reducing the number of errors caused by stuck-at faults, the reliability of a {DNN}-based system can be enhanced. This article proposes {CRAFT}, i.e., criticality-aware fault-tolerance enhancement techniques to enhance the reliability of {NVM}-based {DNNs} in the presence of stuck-at faults. A data block remapping technique is used to reduce the impact of stuckat faults on {DNNs} accuracy. Additionally, by performing bit-level criticality analysis on various {DNNs}, the critical-bit positions in network parameters that can signi\ufb01cantly impact the accuracy are identi\ufb01ed. Based on this analysis, we propose an encoding method which effectively swaps the critical bit positions with that of noncritical bits when more errors (due to stuck-at faults) are present in the critical bits. Experiments of {CRAFT} architecture with various {DNN} models indicate that the robustness of a {DNN} against stuck-at faults can be enhanced by up to 105 times on the {CIFAR}-10 dataset and up to 29 times on {ImageNet} dataset with only a minimal amount of storage overhead, i.e., 1.17\\%.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TCAD.2023.3240659"
    },
    "suggestion": {
      "relation": "This paper introduces CRAFT, a method to enhance fault tolerance in NVM-based DNNs by addressing stuck-at faults, directly relevant to improving hardware error fault tolerance in deep learning systems.",
      "suggestion": "Cite this paper to discuss innovative fault-tolerance techniques specifically designed for NVM-based DNN hardware.",
      "rating": 9
    },
    "sections": null
  },
  "hanif_cross-layer_2020": {
    "base": {
      "article_type": "inproceedings",
      "title": "ross-layer approaches for improving the dependability of deep learning system",
      "authors": [
        "Hanif, Muhammad Abdullah",
        "Hoang, Le-Ha",
        "Shafique, Muhammad"
      ],
      "abstract": "Deep Neural Networks ({DNNs}) - the state-of-the-art computational models for many Artificial Intelligence ({AI}) applications - are inherently compute and resource-intensive and, hence, cannot exploit traditional redundancy-based fault mitigation techniques for enhancing the dependability of {DNN}-based systems. Therefore, there is a dire need to search for alternate methods that can improve their reliability without high expenditure of resources by exploiting the intrinsic characteristics of these networks. In this paper, we present cross-layer approaches that, based on the intrinsic characteristics of {DNNs}, employ software and hardware-level modifications for improving the resilience of {DNN}-based systems to hardware-level faults, e.g., soft errors and permanent faults.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1145/3378678.3391884"
    },
    "suggestion": {
      "relation": "It explores software and hardware modifications to improve DNN systems' resilience against hardware-level faults, aligning with the theme of enhancing fault tolerance in deep learning hardware.",
      "suggestion": "Reference this paper for a broader perspective on cross-layer approaches to fault tolerance in DNN hardware.",
      "rating": 8
    },
    "sections": null
  },
  "rudolph_csp_nodate": {
    "base": {
      "article_type": "article",
      "title": "CSP}: A Multifaceted Hybrid Architecture for Space Computin",
      "authors": [
        "Rudolph, Dylan",
        "Wilson, Christopher",
        "Stewart, Jacob",
        "Gauvin, Patrick",
        "George, Alan",
        "Lam, Herman",
        "Crum, Gary",
        "Wirthlin, Mike",
        "Wilson, Alex",
        "Stoddard, Aaron"
      ],
      "abstract": "Research on the {CHREC} Space Processor ({CSP}) takes a multifaceted hybrid approach to embedded space computing. Working closely with the {NASA} Goddard {SpaceCube} team, researchers at the National Science Foundation ({NSF}) Center for High-Performance Reconfigurable Computing ({CHREC}) at the University of Florida and Brigham Young University are developing hybrid space computers that feature an innovative combination of three technologies: commercial-off-the-shelf ({COTS}) devices, radiation-hardened ({RadHard}) devices, and faulttolerant computing. Modern {COTS} processors provide the utmost in performance and energy-efficiency but are susceptible to ionizing radiation in space, whereas {RadHard} processors are virtually immune to this radiation but are more expensive, larger, less energy-efficient, and generations behind in speed and functionality. By featuring {COTS} devices to perform the critical data processing, supported by simpler {RadHard} devices that monitor and manage the {COTS} devices, and augmented with novel uses of fault-tolerant hardware, software, information, and networking within and between {COTS} devices, the resulting system can maximize performance and reliability while minimizing energy consumption and cost. {NASA} Goddard has adopted the {CSP} concept and technology with plans underway to feature flight-ready {CSP} boards on two upcoming space missions.",
      "publication_year": "",
      "keywords": "",
      "doi": ""
    },
    "suggestion": {
      "relation": "While focusing on space computing, it discusses fault-tolerant computing in a hybrid architecture, which is tangentially related to the fault tolerance in deep learning hardware.",
      "suggestion": "Mention this paper as an example of fault-tolerant computing in specialized environments, noting potential parallels in deep learning hardware.",
      "rating": 4
    },
    "sections": null
  },
  "dong_deep_2023": {
    "base": {
      "article_type": "article",
      "title": "eep reinforcement learning for fault-tolerant workflow scheduling in cloud environmen",
      "authors": [
        "Dong, Tingting",
        "Xue, Fei",
        "Tang, Hengliang",
        "Xiao, Chuangbai"
      ],
      "abstract": "Cloud computing is widely used in various fields, which can provide sufficient computing resources to address users\u2019 demands (workflows) quickly and effectively. However, resource failure is inevitable, and a challenge to optimize the workflow scheduling is to consider the fault tolerance. Most of previous algorithms are based on failure prediction and fault-tolerant strategies, which can cause the time delay and waste of resources. In this paper, combining the above two methods through a deep reinforcement learning framework, an adaptive fault-tolerant workflow scheduling framework called {RLFTWS} is proposed, aiming to minimize the makespan and resource usage rate. In this framework, the fault-tolerant workflow scheduling is formulated as a markov decision process. Resubmission and replication strategy are as two actions. A heuristic algorithm is designed for the task allocation and execution according to the selected fault-tolerant strategy. And, double deep Q network framework ({DDQN}) is developed to select the fault-tolerant strategy adaptively for each task under the current environment state, which is not only prediction but also learning in the process of interacting with the environment. Simulation results show that the proposed {RLFTWS} can efficiently balance the makespan and resource usage rate, and achieve fault tolerance.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1007/s10489-022-03963-w"
    },
    "suggestion": {
      "relation": "This paper's focus on fault-tolerant workflow scheduling in cloud environments diverges from the specific topic of hardware error fault tolerance in deep learning.",
      "suggestion": "This paper could be briefly mentioned when discussing the broader context of fault tolerance in computing systems, though it's not directly related.",
      "rating": 2
    },
    "sections": null
  },
  "moghaddasi_dependable_2023": {
    "base": {
      "article_type": "article",
      "title": "ependable {DNN} Accelerator for Safety-Critical Systems: A Review on the Aging Perspectiv",
      "authors": [
        "Moghaddasi, Iraj",
        "Gorgin, Saeid",
        "Lee, Jeong-A"
      ],
      "abstract": "In the modern era, artificial intelligence ({AI}) and deep learning ({DL}) seamlessly integrate into various spheres of our daily lives. These cutting-edge disciplines have given rise to numerous safety-critical applications such as autonomous driving with a paramount concern on ensuring a high promise of dependability because of the high risk of human injury in the case of malfunction. Even the dependability becomes more crucial as shrinking {CMOS} technology feature size enhances resilience concerns due to factors like aging. In the context of {DL} accelerators, which heavily rely on the efficiency and speed of computations, addressing the effects of aging is of utmost significance to ensure their optimal design and performance. This paper addresses the overarching dependability issue of advanced deep neural networks ({DNN}) accelerators from the aging perspective. Especially, a comprehensive survey and taxonomy of techniques used to evaluate and mitigate aging effects are introduced. We cover different aging effects like permanent faults, timing errors, and lifetime issues. We review research by the layer-wise approach and categorize several resilience classes to bring out major features. The concluding part of this review highlights the questions answered and several future research directions. This study is expected to benefit researchers in different areas of {DNN} deployment, especially the dependability of this emergent paradigm.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/ACCESS.2023.3300376"
    },
    "suggestion": {
      "relation": "It reviews the dependability of DNN accelerators from an aging perspective, which is a crucial aspect of fault tolerance in deep learning hardware.",
      "suggestion": "Utilize this paper to discuss the impact of aging on hardware fault tolerance and the methods to mitigate these effects in DNN accelerators.",
      "rating": 7
    },
    "sections": null
  },
  "wang_design_2016": {
    "base": {
      "article_type": "article",
      "title": "esign, evaluation and fault-tolerance analysis of stochastic {FIR} filter",
      "authors": [
        "Wang, Ran",
        "Han, Jie",
        "Cockburn, Bruce F.",
        "Elliott, Duncan G."
      ],
      "abstract": "Stochastic computing utilizes compact arithmetic circuits that can potentially lower the implementation cost in silicon area. In addition, stochastic computing provides inherent fault tolerance at the cost of a less efficient signal encoding. Finite impulse response ({FIR}) filters are key elements in digital signal processing ({DSP}) due to their linear phase-frequency response. In this article, we consider the problem of implementing {FIR} filters using the stochastic approach. Novel stochastic {FIR} filter designs based on multiplexers are proposed and compared to conventional binary designs implemented using Synopsys tools with a 28-nm cell library. Silicon area, power and maximum clock frequency are obtained to evaluate the throughput per area ({TPA}) and the energy per operation ({EPO}). For equivalent filtering performance, the stochastic {FIR} filters underperform in terms of {TPA} and {EPO} compared to the conventional binary design, although the stochastic design shows more graceful degradation in performance with a significant reduction in energy consumption. A detailed analysis is performed to evaluate the accuracy of stochastic {FIR} filters and to determine the required stochastic sequence length. The fault-tolerance of the stochastic design is compared with that of the binary circuit enhanced with triple modular redundancy ({TMR}). The stochastic designs are more reliable than the conventional binary design and its {TMR} implementation with unreliable voters, but they are less reliable than the binary {TMR} implementation when the voters are fault-free.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.microrel.2015.11.017"
    },
    "suggestion": {
      "relation": "Discusses the fault tolerance of stochastic FIR filters, which, while related to fault tolerance, is not directly focused on deep learning hardware.",
      "suggestion": "Cite this paper to illustrate a specific example of fault tolerance in digital signal processing that could inform approaches in deep learning hardware.",
      "rating": 5
    },
    "sections": null
  },
  "li_design-for-reliability_2023": {
    "base": {
      "article_type": "article",
      "title": "esign-for-reliability and on-the-fly fault tolerance procedure for paper-based digital microfluidic biochips with multiple fault",
      "authors": [
        "Li, Jian-De",
        "Wang, Sying-Jyan",
        "Li, Katherine Shu-Min",
        "Ho, Tsung-Yi"
      ],
      "abstract": "Paper-based digital microfluidic biochips ({PB}-{DMFBs}) have emerged as the most promising solution to biochemical applications in resource-limited regions. However, like silicon chips, the reliability of {PB}-{DMFBs} is affected by physical defects. Even worse, since electrodes, conductive wires, and droplet routings are entangled on the same layer, multiple faults may occur simultaneously. Such faults not only cause waste of samples and human resource but also affect the correctness of the diagnostics. In this paper, we propose a reliability scheme with emphasis on design-for-reliability ({DfR}) and probability-based fault tolerance to ensure the correct func\u00ad tionality of {PB}-{DMFBs} with multiple faults.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.vlsi.2022.11.013"
    },
    "suggestion": {
      "relation": "Focuses on fault tolerance in paper-based digital microfluidic biochips, which is a different application area from deep learning hardware fault tolerance.",
      "suggestion": "This paper could be mentioned in a discussion on the diversity of fault tolerance approaches across different hardware platforms.",
      "rating": 3
    },
    "sections": null
  },
  "kundu_diagnnose_2024": {
    "base": {
      "article_type": "article",
      "title": "DiagNNose}: Toward Error Localization in Deep Learning Hardware-Based on {VTA}-{TVM} Stac",
      "authors": [
        "Kundu, Shamik",
        "Banerjee, Suvadeep",
        "Raha, Arnab",
        "Natarajan, Suriyaprakash",
        "Basu, Kanad"
      ],
      "abstract": "Low-level hardware faults manifested in a Deep learning ({DL}) accelerator usher in graceless degradation of high-level classi\ufb01cation accuracy, which can eventuate to catastrophic circumstances. This violates the crucial Functional Safety ({FuSa}) of the {DL} accelerator, maintaining which is imperative in high-assurance applications. Conventional techniques for error localization incur high-test efforts, without regards to the unique challenges posed by {DL} systems. In this direction, we propose {DiagNNose}, a two-tier machine learning-based error localization framework for on-line fault management in {DL} accelerators. We develop a novel diagnostic pattern selection algorithm to obtain a minimal subset of functional test patterns, that are executed in the accelerator in mission mode. By extracting and analyzing data\ufb02ow-based features from the intermediate computations of the general matrix multiply ({GEMM}) core, a lightweight multilayer perceptron accomplishes bit-level error localization in 8-bit, 16-bit, and 32-bit datapath units with high \ufb01delity. We have limited ourselves to a single accelerator design, i.e., the versatile tensor accelerator ({VTA}) architecture to evaluate our proposed {DiagNNose} framework. On executing state-of-the-art deep neural networks trained on {ImageNet}; error localization using only 30 diagnostic functional test patterns demonstrate up to 98.4\\% diagnosability, thereby demonstrating an improvement of 54.63\\% over a random test pattern set, with as low as 4.95\\% overhead in the {DL} accelerator in mission mode.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TCAD.2023.3303851"
    },
    "suggestion": {
      "relation": "Proposes DiagNNose, a framework for error localization in DL accelerators, directly addressing the challenge of identifying and managing hardware errors in deep learning systems.",
      "suggestion": "Highlight this paper for its contribution to error localization techniques in deep learning hardware, enhancing fault tolerance.",
      "rating": 9
    },
    "sections": null
  },
  "fan_disjoint_2023": {
    "base": {
      "article_type": "article",
      "title": "isjoint Paths Construction and Fault-Tolerant Routing in {BCube} of Data Center Network",
      "authors": [
        "Fan, Weibei",
        "Xiao, Fu",
        "Cai, Hui",
        "Chen, Xiaobai",
        "Yu, Shui"
      ],
      "abstract": "{BCube} is a promising structure of data center network, as it can signi\ufb01cantly improve the performance of typical applications. With the expansion of network scale and increasement of complexity, reliability and stability of networks have become more essential. In this paper, we study the fault-tolerant routings in {BCube}. First, we design a fault-tolerant routing algorithm based on node disjoint multi-paths. The proposed multi-path routing has stronger fault tolerance, since each path has no other common nodes except the source node and the destination node. Second, we investigate an effective fault-tolerant routing based on routing capabilities algorithm for {BCube}. The proposed algorithm has higher fault tolerance and success rate of \ufb01nding feasible routes, since it does not limit the faults number. Third, we present an adaptive path \ufb01nding algorithm for establishing virtual links between any two nodes in {BCube}, which can shorten the diameter of {BCube}. Extensive simulation results show that the proposed routing scheme outperforms the existing popular algorithms. Compared with the state-of-the-art fault-tolerant routing algorithms, the proposed algorithm has a 21.5\\% to 25.3\\% improvement on both throughput and packet arrival rate. Meanwhile, it reduces the average latency of 18.6\\% and the maximum latency of 23.7\\% in networks.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TC.2023.3251849"
    },
    "suggestion": {
      "relation": "This paper focuses on fault-tolerant routing in data center networks, which is tangentially related to hardware fault tolerance but does not directly address deep learning computational errors caused by hardware circuit errors.",
      "suggestion": "Mention this paper to discuss broader applications of fault tolerance in networked systems, but clarify its indirect relevance to deep learning hardware fault tolerance.",
      "rating": 3
    },
    "sections": null
  },
  "ramesh_babu_distributed_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "istributed Consensus and Fault Tolerance Mechanisms Using Distributed Machine Learnin",
      "authors": [
        "Ramesh Babu, P",
        "Mohammad Jimalo, Kamal",
        "Gadiparthi, Manjunath",
        "Kiran Kumar, K. R. N."
      ],
      "abstract": "In this paper, we develop a distributed consensus model to improve the process of fault tolerance in cloud. The distributed consensus mechanism uses distributed machine learning as its base estimator to predict the fault instances when a task is allocated for possible offloading of user contents in the cloud. The distributed machine learning model senses the number of tasks and available nodes to complete the possible offloading of task in the cloud. The python simulator is conducted to test the efficacy of the distributed consensus mechanism in allocating the offloaded task without faults in the cloud servers. The results of simulation shows an increased prediction accuracy, reduced latency in offloading the task and classifying the fault tolerant {VMs} or task in process the task update.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/ICDT57929.2023.10151116"
    },
    "suggestion": {
      "relation": "The paper develops a distributed consensus model for fault tolerance in cloud computing, using machine learning for prediction, which indirectly relates to the robustness of deep learning systems but does not focus on hardware error fault tolerance.",
      "suggestion": "Cite this paper as an example of fault tolerance in distributed systems, noting the potential for cross-application of machine learning techniques to hardware fault tolerance.",
      "rating": 4
    },
    "sections": null
  },
  "hu_dual_2023": {
    "base": {
      "article_type": "article",
      "title": "ual neural networks based active fault-tolerant control for electromechanical systems with actuator and sensor failur",
      "authors": [
        "Hu, Jian",
        "Sha, Yingzhe",
        "Yao, Jianyong"
      ],
      "abstract": "The fault detection and fault-tolerant control ability of electromechanical actuator is very important especially for safety-oriented aircraft industry. In this paper, a novel active faulttolerant control strategy based on double neural networks combined with improved fast inte\u00ad gral terminal sliding mode control is proposed. The faults only related to the system states are considered, which is very common in electromechanical system. Considering their strong approximation ability, two neural networks with different inputs are added in a high-gain observer to estimate system model uncertainties and fault respectively. Thus, the fault detec\u00ad tion observer can be sensitive to faults without false alarm. An integral term of position error is introduced into a fast terminal sliding surface to design an improved controller. Model un\u00ad certainties and fault are compensated for in the controller with feedforward cancellation tech\u00ad nique based on neural networks\u2019 estimation. The Lyapunov stability theory can guarantee the bounded stability of the proposed control strategy. Extensive comparative simulations and experimental results are obtained to verify the better performance of the proposed control strategy compared with some other traditional fault tolerant strategy.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.ymssp.2022.109558"
    },
    "suggestion": {
      "relation": "This paper presents a fault-tolerant control strategy for electromechanical systems, which, while focusing on fault tolerance, is specific to electromechanical actuators rather than deep learning hardware.",
      "suggestion": "Reference this paper to illustrate fault tolerance strategies in electromechanical systems, drawing parallels to potential applications in deep learning hardware.",
      "rating": 2
    },
    "sections": null
  },
  "pattanaik_dynamic_nodate": {
    "base": {
      "article_type": "article",
      "title": "ynamic Fault Tolerance Management Algorithm for {VM} Migration in Cloud Data Center",
      "authors": [
        "Pattanaik, Bikash Chandra",
        "Sahoo, Bidush Kumar",
        "Pati, Bibudhendu",
        "Laha, Suprava Ranjan"
      ],
      "abstract": "Fault tolerance is critical in constructing robust cloud computing systems to ensure uninterrupted service delivery and maintain economic benefits despite potential faults. This paper presents a novel layered modeling architecture that combines reactive and proactive fault modeling theories to enable reliable, survivable cloud-based applications by addressing fault tolerance concerns. This paper examines the issues of dynamic fault tolerance management and virtual machine ({VM}) migration in cloud data centers. We introduce a comprehensive algorithm that efficiently manages fault tolerance through proactive measures by leveraging a layered modeling architecture. The algorithm considers defect prediction and resource allocation techniques to minimize service interruptions and maximize resource utilization. It incorporates reactive and proactive fault modeling to identify and respond to faults, anticipates potential faults, and takes preventative measures. This integration makes the cloud computing environment more robust and reliable. However, extensive simulations and evaluations demonstrate the proposed algorithm's effectiveness in reducing service downtime, ensuring application reliability, and sustaining optimal performance. The algorithm's ability to dynamically migrate virtual machines ({VMs}) based on defect prediction contributes to efficient resource allocation and load balancing, mitigating potential bottlenecks and enhancing system resilience. The results demonstrate the applicability and effectiveness of the proposed framework for maintaining cloud-based applications' dependability. Combining reactive and proactive fault modeling theories, the proposed algorithm provides a comprehensive method for keeping cloud-based applications reliable and fault-tolerant.",
      "publication_year": "",
      "keywords": "",
      "doi": ""
    },
    "suggestion": {
      "relation": "It discusses dynamic fault tolerance management in cloud data centers, which is more about software and system-level fault tolerance rather than hardware error fault tolerance in deep learning.",
      "suggestion": "Use this paper to discuss the importance of fault tolerance in cloud computing as a context for the necessity of hardware fault tolerance in deep learning.",
      "rating": 3
    },
    "sections": null
  },
  "zhang_ebscn_2019": {
    "base": {
      "article_type": "article",
      "title": "EBSCN}: An Error Backtracking Method for Soft Errors Based on Clustering and a Neural Networ",
      "authors": [
        "Zhang, Nan",
        "Xu, Jianjun",
        "Meng, Xiankai",
        "Tan, Qingping"
      ],
      "abstract": "With the development of integrated circuit design technology, soft errors have become an important threat to system reliability, and software-based fault-tolerant techniques are gradually attracting people\u2019s attention. In many cases, researchers use fault injection techniques that are less observable and less controllable to verify system reliability, and at this point, analysing where soft errors occur requires considerable work. In this paper, we present {EBSCN}, an error backtracking method. The {EBSCN} method sorts functions by suspiciousness by analysing the erroneous output results, which will help researchers reduce the amount of work required for analysis. The {EBSCN} method includes a feature extraction method based on clustering and a feature analysis method based on a deep neural network. This paper introduces the principle of the two methods as well as methods to improve and extend them with program-related information. We discuss the effect of the scale of the output result and the severity of the error on the {EBSCN} method through experiments and verify the effect of the {EBSCN} method. The results showed that the proportion of the function in which the soft error actually occurs in the ranking of the top 25\\% of the suspiciousness sequence is no less than 82\\%, and the proportion ranked in the top 50\\% is no less than 97\\%.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/ACCESS.2019.2947005"
    },
    "suggestion": {
      "relation": "This paper introduces a software-based method for tracking and correcting soft errors in systems, which is relevant to understanding how software can complement hardware fault tolerance in deep learning.",
      "suggestion": "Cite this paper to discuss software-based approaches to complement hardware fault tolerance strategies in deep learning systems.",
      "rating": 5
    },
    "sections": null
  },
  "guerrero-balaguera_effective_2022": {
    "base": {
      "article_type": "inproceedings",
      "title": "ffective fault simulation of {GPU}\u2019s permanent faults for reliability estimation of {CNNs",
      "authors": [
        "Guerrero-Balaguera, Juan-David",
        "Sierra, Robert Limas",
        "Reorda, Matteo Sonza"
      ],
      "abstract": "Convolutional Neural Networks ({CNNs}) and Graphic Processing Units ({GPUs}) are now increasingly adopted in many cutting edge safety-critical applications. Consequently, it is crucial to evaluate the reliability of these systems, since the hardware can be affected by several phenomena (e.g., wear out of the device), producing permanent defects in the {GPU}. These defects may induce wrong outcomes in the {CNN} that may endanger the application. Traditionally, the study of the effects of permanent faults on {CNNs} has been approached by resorting to application-level fault injection (e.g., acting on the weights). However, this approach has restricted scope, and it may not reveal the actual vulnerabilities in the {GPU} device. Hence, a more accurate evaluation of the fault effects is required, considering more in-depth details of the device\u2019s hardware. This work introduces a more elaborated experimental evaluation of the impact of {GPU}\u2019s permanent faults on the reliability of a {CNN} by resorting to a Software-Implemented Fault Injection ({SWIFI}) strategy, considering faults at the hardware level. The results of the fault simulation campaigns we performed on the {GPU} data-path cores are compared with those at the application level, proving that the latter ones are generally optimistic.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/IOLTS56730.2022.9897823"
    },
    "suggestion": {
      "relation": "It evaluates the impact of hardware-level permanent faults in GPUs on the reliability of CNNs, directly addressing the topic of hardware error fault tolerance in deep learning.",
      "suggestion": "Highlight this paper as a key study on the effects of hardware faults on deep learning performance, particularly in safety-critical applications.",
      "rating": 9
    },
    "sections": null
  },
  "yadav_efficient_2023": {
    "base": {
      "article_type": "article",
      "title": "fficient grouping approach for fault tolerant weight mapping in memristive crossbar arra",
      "authors": [
        "Yadav, Dev Narayan",
        "Thangkhiew, Phrangboklang Lyngton",
        "Chakraborty, Sandip",
        "Sengupta, Indranil"
      ],
      "abstract": "The ability of resistive memory ({ReRAM}) to naturally conduct vector\u2013matrix multiplication ({VMM}), which is the primary operation carried out during the training and inference of neural networks, has caught the interest of researchers. The memristor crossbar is one of the desirable architectures to perform {VMM} because it offers various benefits over other memory technologies, including in-memory computing, low power, and high density. Direct downloading and chip-on-the-loop approaches are typically used to train {ReRAM}-based neural networks. In these methods, all weight computations are carried out by a host machine, and the computed weights are downloaded in the crossbar. It has been seen that the network does not deliver the same precision as promised by the host system once the weights have been downloaded. This is because crossbars contain a significant number of faulty memristors and suffer from cell resistance variations because of immature manufacturing technologies. As a result, a cell may not be able to take the exact weight values that the host system generates, and may lead to incorrect inferences. Existing techniques for fault-tolerant mapping either involve network retraining or employ a graph-matching strategy that comes with hardware, power, and latency overheads. In this paper, we propose a mapping method to tolerate the effect of defective memristors. In order to lessen the impact of faulty memristors, the mapping is done in a way that allows network weights to cover up faulty memristors. Further, this work prioritizes the different faults based on the frequency of occurrence. The mapping efficiency is found to increase significantly with low power, area and latency overheads in the proposed approach. Experimental analyses show considerable improvement as compared to state-of-the-art works.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.memori.2023.100045"
    },
    "suggestion": {
      "relation": "The paper discusses fault-tolerant weight mapping in memristive crossbar arrays for neural networks, directly addressing hardware fault tolerance in a deep learning context.",
      "suggestion": "Emphasize this paper for its direct relevance to hardware fault tolerance in deep learning, particularly in the context of memristive technologies.",
      "rating": 8
    },
    "sections": null
  },
  "libano_efficient_2023": {
    "base": {
      "article_type": "article",
      "title": "fficient Error Detection for Matrix Multiplication With Systolic Arrays on {FPGAs",
      "authors": [
        "Libano, Fabiano",
        "Rech, Paolo",
        "Brunhaver, John"
      ],
      "abstract": "Matrix multiplication has always been a cornerstone in computer science. In fact, linear algebra tools permeate a wide variety of applications: from weather forecasting, to \ufb01nancial market prediction, radio signal processing, computer vision, and more. Since many of the aforementioned applications typically impose strict performance and/or fault tolerance constraints, the demand for fast and reliable matrix multiplication ({MxM}) is at an all-time high. Typically, increased reliability is achieved through redundancy. However, coarse-grain duplication incurs an often prohibitive overhead, higher than 100\\%. Thanks to the peculiar characteristics of the {MxM} algorithm, more ef\ufb01cient algorithmbased hardening solutions have been designed to detect (and even correct) some types of errors with lower overhead. We show that, despite being more ef\ufb01cient, current solutions are still sub-optimal in certain scenarios, particularly when considering persistent faults in Field-Programmable Gate-Arrays ({FPGAs}). Based on a thorough analysis of the fault model, we propose an error detection technique for {MxM} that decreases both algorithmic and architectural costs by over a polynomial degree, when compared to existing algorithm-based strategies. Furthermore, we report arithmetic overheads at the application level to be under 1\\% for three state-of-the-art Convolutional Neural Networks ({CNNs}).",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TC.2023.3248282"
    },
    "suggestion": {
      "relation": "This paper proposes an efficient error detection technique for matrix multiplication in FPGAs, relevant to hardware fault tolerance in deep learning computations.",
      "suggestion": "Cite this paper to discuss specific hardware error detection techniques in deep learning, especially for FPGA-based implementations.",
      "rating": 7
    },
    "sections": null
  },
  "fuengfusin_efficient_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "fficient Repetition Coding for Deep Learning Towards Implementation Using Emerging Non-Volatile Memory with Write-Error",
      "authors": [
        "Fuengfusin, Ninnart",
        "Tamukoh, Hakaru",
        "Tanaka, Yuichiro",
        "Nomura, Osamu",
        "Morie, Takashi"
      ],
      "abstract": "Emerging non-volatile memory devices, such as resistive random access memory ({ReRAM}) and voltage-controlled magnetoresistive random access memory ({VC}-{MRAM}), promise low energy consumption for arti\ufb01cial intelligence applications. However, when implementing deep neural networks ({DNNs}) using such memory devices, write-error may cause millions of bit\ufb02ipping to {DNN}. This easily degrades the {DNN} performance. To address this problem, we propose a novel repetition coding for deep-learning ({RC}-{DL}), which is a repetition coding designed to protect {IEEE} 32-bit \ufb02oating-point ({FP}32) {DNN} models. Compared to conventional repetition coding, the proposed {RC}-{DL} exploits {FP}32 non-uniform magnitude encoding by increasing the repeat rates to protect sensitive bit positions and reduce the repeat rates to insensitive bit positions. Hence, {RC}-{DL} uses a number of bits equivalent to a 3-bit repetition code while delivering the performance close to 11-bit repetition code. We perform extensive Monte Carlo simulations to simulate the write-error property with {ImageNet} 2012 pretrained models. The {DNN} models with {RC}-{DL} are shown to be operable in the extremely imperfect environment while delivering with only minor reductions in {DNN} performance.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/IJCNN54540.2023.10191433"
    },
    "suggestion": {
      "relation": "This paper introduces a novel repetition coding method to protect deep neural networks (DNNs) from write-errors in non-volatile memory devices, directly addressing hardware fault tolerance in deep learning.",
      "suggestion": "Cite this paper to discuss innovative fault tolerance techniques in the context of emerging memory technologies for DNNs.",
      "rating": 8
    },
    "sections": null
  },
  "baltagiannis_efpcaching_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "EFPCaching}: Energy-aware and Fault-tolerant Probabilistic Caching of popular {IoT} content in {ICN",
      "authors": [
        "Baltagiannis, Nikolaos",
        "Kapetanidou, Ioanna Angeliki",
        "Tsaoussidis, Vassilis"
      ],
      "abstract": "The Internet of Things ({IoT}) requires bespoke protocols due to the resource constraints and susceptibility to malfunctions, plaguing {IoT} devices. The Named Data Networking ({NDN}) paradigm is well-suited to meet the particular {IoT} requirements thanks to its native in-network caching feature that facilitates asynchronous data sharing and consumption and provides native mobility support. In this paper, we propose the \u2018Energy-aware and Fault-tolerant Probabilistic Caching\u2019 ({EFPCaching}) policy, which incorporates energy consumption and sensor faults in caching decisions. Introducing fault awareness in caching constitutes the novelty of our approach. We compared the performance of our strategy against state-of-the-art caching policies, and the evaluation results demonstrate that {EFPCaching} has distinct benefits in terms of cache and energy efficiency.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/NOMS56928.2023.10154270"
    },
    "suggestion": {
      "relation": "Although focused on IoT and ICN, this paper's exploration of energy-aware and fault-tolerant caching could offer peripheral insights into fault tolerance considerations in broader computational contexts.",
      "suggestion": "Reference this paper for a broader perspective on fault tolerance and energy efficiency in distributed computing environments.",
      "rating": 3
    },
    "sections": null
  },
  "zhou_elasticdl_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "ElasticDL}: A Kubernetes-native Deep Learning Framework with Fault-tolerance and Elastic Schedulin",
      "authors": [
        "Zhou, Jun",
        "Zhang, Ke",
        "Zhu, Feng",
        "Shi, Qitao",
        "Fang, Wenjing",
        "Wang, Lin",
        "Wang, Yi"
      ],
      "abstract": "The power of artificial intelligence ({AI}) models originates with sophisticated model architecture as well as the sheer size of the model. These large-scale {AI} models impose new and challenging system requirements regarding scalability, reliability, and flexibility. One of the most promising solutions in the industry is to train these large-scale models on distributed deep-learning frameworks. With the power of all distributed computations, it is desired to achieve a training process with excellent scalability, elastic scheduling (flexibility), and fault tolerance (reliability). In this paper, we demonstrate the scalability, flexibility, and reliability of our open-source Elastic Deep Learning ({ElasticDL}) framework. Our {ElasticDL} utilizes an open-source system, i.e., Kubernetes, for automating deployment, scaling, and management of containerized application features to provide fault tolerance and support elastic scheduling for {DL} tasks.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1145/3539597.3573037"
    },
    "suggestion": {
      "relation": "ElasticDL's emphasis on fault tolerance and elastic scheduling in distributed deep learning frameworks aligns with the resilience aspect of hardware fault tolerance in AI applications.",
      "suggestion": "Use this paper to highlight examples of fault tolerance in distributed deep learning systems.",
      "rating": 6
    },
    "sections": null
  },
  "choudhury_energy_2023": {
    "base": {
      "article_type": "article",
      "title": "nergy efficiency in multicore shared cache by fault tolerance using a genetic algorithm based block reuse predicto",
      "authors": [
        "Choudhury, Avishek",
        "Mondal, Brototi",
        "Paul, Kolin",
        "Sikdar, Biplab K."
      ],
      "abstract": "Aggressive voltage scaling to reduce energy consumption in Multicore causes exponential cell failures in {SRAM}. Last-level-cache ({LLC}), the major contender of chip area, exhibits highest sensitivity to low voltage parametric failures and limits energy efficiency. To break the energy barrier, exclusive fault protection mechanism is solicited to ensure on-chip data recovery out of the low voltage failures. This work proposes Cache evolution for energy efficiency by protecting cache blocks from {SRAM} cell failures due to voltage scaling. A set of coherence and reuse aware in-cache selective replication policies are proposed to ensure on-chip data recovery. Reuse likelihood is predicted through a vector optimization technique using Genetic Algorithm ({GA}). Reuse aware selective invalidation is employed to balance cache load due to replications. A replication aware replacement policy is also developed that victimizes the lowest reusable cache block. The proposed scheme is evaluated in Multi2Sim 5.0 simulation framework with {SPEC} {CPU} benchmark programs. Experimental results claim 43.66\\% and 38.80\\% reductions in miss rate and 59.10\\% and 52.73\\% reductions in vulnerability for integer and floating point benchmarks respectively. Energy reduction of 39.21\\% is observed for integer and 25.73\\% is observed for floating point benchmarks. Up to 34.78\\% and 26.98\\% power reductions in integer and floating point benchmarks are also observed. The minimum supply voltage of 350 {mV} is achieved at the cost of 7.05\\% area overhead and 3\\% performance drop-off.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.micpro.2023.104864"
    },
    "suggestion": {
      "relation": "This work proposes a fault tolerance mechanism for multicore shared cache, relevant for understanding hardware fault tolerance strategies in the context of energy efficiency and voltage scaling.",
      "suggestion": "Reference this paper to discuss fault tolerance mechanisms in hardware that support deep learning computations.",
      "rating": 7
    },
    "sections": null
  },
  "wu_enhancing_2023": {
    "base": {
      "article_type": "article",
      "title": "nhancing Fault Injection Testing of Service Systems via Fault-Tolerance Bottlenec",
      "authors": [
        "Wu, Huayao",
        "Yu, Senyao",
        "Niu, Xintao",
        "Nie, Changhai",
        "Pei, Yu",
        "He, Qiang",
        "Yang, Yun"
      ],
      "abstract": "Modern large-scale service systems are usually deployed with redundant components to ensure high dependability in distributed and volatile environments. Fault Injection Testing ({FIT}) is a popular technique for testing such systems, while the application of {FIT} to validating the correctness of redundant components remains a challenging task, especially when the system\u2019s structural information is unavailable when testing starts. In this study, we refer to a minimum set of faults that, when injected, will cut off all execution paths in a service system as a fault-tolerance bottleneck, and we propose a novel Fault-tolerance Bottleneck driven Fault Injection ({FBFI}) approach to the exploration and validation of redundant components without prior knowledge of the system\u2019s business structure. The core idea of {FBFI} is to iteratively infer and inject bottlenecks of the business structure constructed so far. In this way, {FBFI} is able to discover and test redundant components by repeatedly triggering new system behaviors. The effectiveness and ef\ufb01ciency of {FBFI} is evaluated using two microservice benchmark systems with different deployment scales. The results reveal that {FBFI} is more practical and cost-effective than random and lineage-driven {FIT} approaches in testing service systems of high redundancy levels.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TSE.2023.3285357"
    },
    "suggestion": {
      "relation": "The focus on fault injection testing for service systems' redundancy does not directly relate to hardware fault tolerance in deep learning but offers a methodology perspective on fault tolerance testing.",
      "suggestion": "Mention this paper as an example of fault tolerance testing techniques that could be adapted for deep learning hardware.",
      "rating": 2
    },
    "sections": null
  },
  "nema_eris_2022": {
    "base": {
      "article_type": "inproceedings",
      "title": "ris: Fault Injection and Tracking Framework for Reliability Analysis of Open-Source Hardwar",
      "authors": [
        "Nema, Shubham",
        "Kirschner, Justin",
        "Adak, Debpratim",
        "Agarwal, Sapan",
        "Feinberg, Ben",
        "Rodrigues, Arun F.",
        "Marinella, Matthew J.",
        "Awad, Amro"
      ],
      "abstract": "As transistors have been scaled over the past decade, modern systems have become increasingly susceptible to faults. Increased transistor densities and lower capacitances make a particle strike more likely to cause an upset. At the same time, complex computer systems are increasingly integrated into safetycritical systems such as autonomous vehicles. These two trends make the study of system reliability and fault tolerance essential for modern systems. To analyze and improve system reliability early in the design process, new tools are needed for {RTL} fault analysis.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/ISPASS55109.2022.00027"
    },
    "suggestion": {
      "relation": "This paper discusses a framework for fault injection and tracking in open-source hardware, relevant for understanding the reliability analysis of hardware that could be used in deep learning.",
      "suggestion": "Cite this paper to discuss tools and methodologies for analyzing hardware fault tolerance in deep learning systems.",
      "rating": 5
    },
    "sections": null
  },
  "garcia-astudillo_error_2024": {
    "base": {
      "article_type": "article",
      "title": "rror Mitigation Using Optimized Redundancy for Composite Algorithms in {FPGAs",
      "authors": [
        "Garcia-Astudillo, Luis A.",
        "Lindoso, Almudena",
        "Entrena, Luis"
      ],
      "abstract": "Error mitigation techniques have become mandatory in aerospace applications to cope with soft errors. Approximate error mitigation techniques are an attractive solution to reduce the overheads caused by conventional approaches, such as Triple Modular Redundancy. These techniques use approximate redundant copies of the target design which can be implemented with less resources, at the expense of slightly reducing the precision of the result in case of error. In this work, we propose Optimized Redundancy for Composite Algorithms ({ORCA}), a novel hardening technique for algorithms that may be decomposed into more simple parts. Instead of adding identical redundant modules, we use complementary modules that can be composed to implement the target design. These complementary modules can also be compared to detect errors. However, when they are correct, they produce an exact result instead of an approximate result. The experimental results show that this technique can reduce the overhead and provide a better trade-off between overhead and precision than existing approximate techniques.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TAES.2024.3350015"
    },
    "suggestion": {
      "relation": "The proposed optimized redundancy technique for error mitigation in FPGAs is highly relevant for discussing hardware fault tolerance designs in deep learning hardware.",
      "suggestion": "Use this paper to discuss error mitigation techniques in hardware designs for deep learning applications.",
      "rating": 8
    },
    "sections": null
  },
  "rathore_error_2020": {
    "base": {
      "article_type": "article",
      "title": "rror Probability Models for Voltage-Scaled Multiply-Accumulate Unit",
      "authors": [
        "Rathore, Mallika",
        "Milder, Peter",
        "Salman, Emre"
      ],
      "abstract": "Energy ef\ufb01ciency is a critical design objective in deep learning hardware, particularly for real-time machine learning applications where the processing takes place on resource-constrained platforms. The inherent resilience of these applications to error makes voltage scaling an attractive method to enhance ef\ufb01ciency. Timing error probability models are proposed in this article to better understand the effects of voltage scaling on error rates and power consumption of multiplyaccumulate units. The accuracy of the proposed models is demonstrated via Monte Carlo simulations. These models are then used to quantify the related tradeoffs without relying on timeconsuming hardware-level simulations. Both modern {FinFET} and emerging tunneling \ufb01eld-effect transistor ({TFET}) technologies are considered to explore the dependence of the effects of voltage scaling on these two technologies.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TVLSI.2020.2988204"
    },
    "suggestion": {
      "relation": "This paper's focus on error probability models for voltage-scaled units in deep learning hardware offers insights into the trade-offs between energy efficiency and fault tolerance.",
      "suggestion": "Reference this paper to discuss the impact of voltage scaling on hardware error rates and fault tolerance in deep learning.",
      "rating": 7
    },
    "sections": null
  },
  "amarnath_error_2024": {
    "base": {
      "article_type": "article",
      "title": "rror Resilience in Deep Neural Networks Using Neuron Gradient Statistic",
      "authors": [
        "Amarnath, Chandramouli",
        "Mejri, Mohamed",
        "Ma, Kwondo",
        "Chatterjee, Abhijit"
      ],
      "abstract": "Modern Deep Neural Networks ({DNNs}) are deployed across a wide range of applications, from medical robotics to autonomous driving, where safety and reliability are key concerns. The complexity, speed and low-power operation of the underlying hardware makes them vulnerable to soft errors that corrupt the results of computations and memory accesses. Existing approaches to error resilience are either expensive in terms of overhead, require {DNN} re-training or applicable to only specific hardware domains. In contrast, we present a novel error resilience approach that does not require {DNN} re-training and scales across computation as well as weight parameter errors. In the proposed methodology, the statistics of gradients of neuron output values relative to adjacent neurons in an ordering of neurons allow tight theoretically grounded thresholding of neuron outputs to diagnose erroneous neuron outputs. These are then set to zero (suppressed) for error resilience. A low overhead error diagnosis module is used for this purpose and is designed using gradient statistics collected across the training dataset of the {DNN}. Our approach is compared against state of the art error resilience techniques and validated on multiple datasets, networks and error scenarios as well a hardware test case.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TCAD.2023.3335144"
    },
    "suggestion": {
      "relation": "This paper introduces a novel error resilience approach for DNNs that does not require re-training and can handle errors in both computation and weight parameters, focusing on diagnosing and suppressing erroneous neuron outputs.",
      "suggestion": "Cite this paper to discuss innovative error resilience techniques in DNNs that minimize overhead and avoid re-training.",
      "rating": 8
    },
    "sections": null
  },
  "balen_evaluating_2023": {
    "base": {
      "article_type": "article",
      "title": "valuating the Reliability of Different Voting Schemes for Fault Tolerant Approximate System",
      "authors": [
        "Balen, Tiago R.",
        "Gonz\u00e1lez, Carlos J.",
        "Oliveira, Ingrid F. V.",
        "Da Rosa Jr, Leomar S.",
        "Soares, Rafael I.",
        "Schvittz, Rafael B.",
        "Added, Nemitala",
        "Macchione, Eduardo L. A.",
        "Aguiar, Vitor A. P.",
        "Guazzelli, Marcilei A.",
        "Medina, Nilberto H.",
        "Butzen, Paulo F."
      ],
      "abstract": "This work presents a study on the reliability of voters for approximate fault tolerant systems in the context of single event effects and electromagnetic interference. A first case study analyses different topologies of single-bit majority voters for logic circuits employing fault injection by simulation. In these simulations, an analysis is first performed to identify the critical diffusion areas of the physical implementation according to the voter input vector. Additionally, as a second case study, practical heavy ion experiments on different architectures of software-based approximate voters for mixed-signal applications are also presented, and the cross section of each voter is evaluated. The system comprising the voters was irradiated in two distinct experiments with an 16O ion beam, producing an effective {LET} at the active region of 5.5 {MeV}/mg/cm2 . As a complementary study, a conducted electromagnetic interference injection was also performed, considering two distinct voting schemes. Results of the case-studies allow identifying the most tolerant voter architectures (among the studied ones) for approximate computing applications under single event effects and electromagnetic interference.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1007/s10836-023-06072-9"
    },
    "suggestion": {
      "relation": "The study evaluates the reliability of different voting schemes for fault tolerance in approximate computing systems, focusing on their resilience to single event effects and electromagnetic interference.",
      "suggestion": "Reference this paper for insights into hardware fault tolerance through voting schemes in logic circuits and mixed-signal applications.",
      "rating": 5
    },
    "sections": null
  },
  "ruospo_evaluating_2020": {
    "base": {
      "article_type": "inproceedings",
      "title": "valuating Convolutional Neural Networks Reliability depending on their Data Representatio",
      "authors": [
        "Ruospo, Annachiara",
        "Bosio, Alberto",
        "Ianne, Alessandro",
        "Sanchez, Ernesto"
      ],
      "abstract": "Safety-critical applications are frequently based on deep learning algorithms. In particular, Convolutional Neural Networks ({CNNs}) are commonly deployed in autonomous driving applications to ful\ufb01l complex tasks such as object recognition and image classi\ufb01cation. Ensuring the reliability of {CNNs} is thus becoming an urgent requirement since they constantly behave in human environments. A common and recent trend is to replace the full-precision {CNNs} to make way for more optimized models exploiting approximation paradigms such as reduced bit-width data type. If from one hand this is poised to become a sound solution for reducing the memory footprint as well as the computing requirements, it may negatively affect the {CNNs} resilience. The intent of this work is to assess the reliability of a {CNN}-based system when reduced bit-widths are used for the network parameters (i.e., synaptic weights). The approach evaluates the impact of permanent faults in {CNNs} by adopting several bit-width schemes and data types, i.e., \ufb02oating-point and \ufb01xed-point. This determines the trade-off between the {CNN} accuracy and the bits required to represent network weights. The characterization is performed through a fault injection environment built on the darknet open source framework. Experimental results show the effects of permanent fault injections on the weights of {LeNet}-5 {CNN}.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/DSD51259.2020.00109"
    },
    "suggestion": {
      "relation": "This work assesses the reliability of CNNs with reduced bit-width data types for network parameters, exploring the trade-off between CNN accuracy and resilience to permanent faults.",
      "suggestion": "Utilize this paper to discuss the impact of data representation on the fault tolerance and reliability of CNNs in safety-critical applications.",
      "rating": 7
    },
    "sections": null
  },
  "fernandes_dos_santos_evaluation_2017": {
    "base": {
      "article_type": "inproceedings",
      "title": "valuation and Mitigation of Soft-Errors in Neural Network-Based Object Detection in Three {GPU} Architecture",
      "authors": [
        "Fernandes Dos Santos, Fernando",
        "Draghetti, Lucas",
        "Weigel, Lucas",
        "Carro, Luigi",
        "Navaux, Philippe",
        "Rech, Paolo"
      ],
      "abstract": "In this paper, we evaluate the reliability of the You Only Look Once ({YOLO}) object detection framework. We have exposed to controlled neutron beams {GPUs} designed with three different architectures (Kepler, Maxwell, and Pascal) running Darknet, a Convolutional Neural Network for automotive applications, detecting objects in both Caltech and Visual Object Classes data sets. By analyzing the neural network corrupted output, we can distinguish between tolerable errors and critical errors, i.e., errors that could impact on real-time system execution.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/DSN-W.2017.47"
    },
    "suggestion": {
      "relation": "Evaluates the reliability of the YOLO object detection framework on GPUs under neutron beam exposure, distinguishing between tolerable and critical errors in neural network outputs.",
      "suggestion": "Mention this paper when discussing the evaluation of neural network-based object detection systems' resilience to soft-errors across different GPU architectures.",
      "rating": 6
    },
    "sections": null
  },
  "aponte-moreno_evaluation_2023": {
    "base": {
      "article_type": "article",
      "title": "valuation of fault injection tools for reliability estimation of microprocessor-based embedded system",
      "authors": [
        "Aponte-Moreno, Alexander",
        "Isaza-Gonz\u00e1lez, Jos\u00e9",
        "Serrano-Cases, Alejandro",
        "Mart\u00ednez-\u00c1lvarez, Antonio",
        "Cuenca-Asensi, Sergio",
        "Restrepo-Calle, Felipe"
      ],
      "abstract": "Statistical fault injection is widely used to estimate the reliability of mission-critical microprocessor-based systems when exposed to radiation and to evaluate the performance of fault mitigation strategies. However, further research is needed to gain a better understanding of the accuracy of the results and the feasibility of their application under realistic radiation conditions. In this article, an understanding of scenarios in which Instruction Set Architecture simulators or emulators may be relied upon for realistic statistical fault injection campaigns is advanced. An analysis is presented of the results from two simulation-based fault injection tools versus a set of fault emulation results on a real processor. The conclusions of the analysis assist the selection of the most efficient tool and method for testing many different software-based fault mitigation techniques within reasonable time periods and at affordable costs throughout an irradiation campaign. In particular, it was established that a partially ordered set of relations could be defined on the basis of statistical fault injection in relation to the effects of different versions of an application and a given simulator that remained unaltered during the irradiation experiments. The tests were conducted with a Texas Instruments {MSP}430 microcontroller to perform both fault injection campaigns and irradiation experiments using neutrons at the Los Alamos Neutron Science Center ({LANSCE}) Weapons Neutron Research Facility at Los Alamos, {USA}.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.micpro.2022.104723"
    },
    "suggestion": {
      "relation": "Focuses on the evaluation of fault injection tools for estimating the reliability of microprocessor-based systems, contributing to the understanding of software-based fault mitigation techniques.",
      "suggestion": "Cite this paper for a perspective on the role of fault injection tools in testing and improving the reliability of embedded systems under radiation.",
      "rating": 4
    },
    "sections": null
  },
  "fernandes_dos_santos_evaluation_2017-1": {
    "base": {
      "article_type": "inproceedings",
      "title": "valuation and Mitigation of Soft-Errors in Neural Network-Based Object Detection in Three {GPU} Architecture",
      "authors": [
        "Fernandes Dos Santos, Fernando",
        "Draghetti, Lucas",
        "Weigel, Lucas",
        "Carro, Luigi",
        "Navaux, Philippe",
        "Rech, Paolo"
      ],
      "abstract": "In this paper, we evaluate the reliability of the You Only Look Once ({YOLO}) object detection framework. We have exposed to controlled neutron beams {GPUs} designed with three different architectures (Kepler, Maxwell, and Pascal) running Darknet, a Convolutional Neural Network for automotive applications, detecting objects in both Caltech and Visual Object Classes data sets. By analyzing the neural network corrupted output, we can distinguish between tolerable errors and critical errors, i.e., errors that could impact on real-time system execution.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/DSN-W.2017.47"
    },
    "suggestion": {
      "relation": "This paper is a duplicate of ID 4, evaluating the YOLO framework's reliability on different GPU architectures under neutron exposure.",
      "suggestion": "Since this is a duplicate of ID 4, it should not be cited again to avoid redundancy.",
      "rating": 1
    },
    "sections": null
  },
  "roffe_evaluation_2020": {
    "base": {
      "article_type": "inproceedings",
      "title": "valuation of Algorithm-Based Fault Tolerance for Machine Learning and Computer Vision under Neutron Radiatio",
      "authors": [
        "Roffe, Seth",
        "George, Alan D."
      ],
      "abstract": "In the past decade, there has been a push for deployment of commercial-off-the-shelf ({COTS}) avionics due in part to cheaper costs and the desire for more performance. Traditional radiation-hardened processors are expensive and only provide limited processing power. With smaller mission budgets and the need for more computational power, low-cost and highperformance {COTS} solutions become more attractive for these missions. Due to the computational capacity enhancements provided by {COTS} technology, machine-learning and computervision applications are now being deployed on modern space missions. However, {COTS} electronics are highly susceptible to radiation environments. As a result, reliability in the underlying computations becomes a concern. Matrix multiplication is used in machine-learning and computer-vision applications as the main computation for decisions, making it a critical part of the application. Therefore, the large time and memory footprint of the matrix multiplication in machine-learning and computervision applications makes them even more susceptible to singleevent upsets.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/AERO47225.2020.9172799"
    },
    "suggestion": {
      "relation": "Investigates the reliability of machine learning and computer vision applications under neutron radiation, emphasizing the critical role of matrix multiplication in these applications' fault tolerance.",
      "suggestion": "Reference this paper to discuss the challenges and solutions for fault tolerance in machine learning and computer vision applications in radiation-prone environments.",
      "rating": 6
    },
    "sections": null
  },
  "siddique_exposing_2023": {
    "base": {
      "article_type": "article",
      "title": "xposing Reliability Degradation and Mitigation in Approximate {DNNs} Under Permanent Fault",
      "authors": [
        "Siddique, Ayesha",
        "Hoque, Khaza Anuarul"
      ],
      "abstract": "Approximate computing is known for enhancing deep neural network accelerators\u2019 energy efficiency by introducing inexactness with a tolerable accuracy loss. However, small accuracy variations may increase the sensitivity of these accelerators toward undesired subtle disturbances, such as permanent faults. The impact of permanent faults in accurate deep neural network ({AccDNN}) accelerators has been thoroughly investigated in the literature. Conversely, the impact of permanent faults and their mitigation in approximate {DNN} ({AxDNN}) accelerators is vastly underexplored. Toward this, we first present an extensive fault resilience analysis of approximate multilayer perceptrons ({MLPs}) and convolutional neural networks ({CNNs}) using the state-of-the-art Evoapprox8b multipliers in graphic processing unit ({GPU}) and tensor processing unit ({TPU}) accelerators. Then, we propose a novel fault mitigation method, i.e., fault-aware retuning of weights (Fal-{reTune}). Fal-{reTune} retunes the weights using a weight mapping function in the presence of faults for improved classification accuracy. To evaluate the fault resilience and the effectiveness of our proposed mitigation method, we used the most widely used {MNIST}, Fashion-{MNIST}, and {CIFAR}10 datasets. Our results demonstrate that the permanent faults exacerbate the accuracy loss in {AxDNNs} compared with the {AccDNN} accelerators. For instance, a permanent fault in {AxDNNs} can lead to 56\\% accuracy loss, whereas the same faulty bit can lead to only 4\\% accuracy loss in {AccDNN} accelerators. We empirically show that our proposed Fal-{reTune} mitigation method improves the performance of {AxDNNs} up to 98\\%, even with fault rates up to 50\\%. Furthermore, we observe that the fault resilience in {AxDNNs} is orthogonal to their energy efficiency.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TVLSI.2023.3238907"
    },
    "suggestion": {
      "relation": "Presents an analysis of fault resilience in approximate DNNs under permanent faults and proposes a novel fault mitigation method, highlighting the significant impact of faults on accuracy.",
      "suggestion": "Utilize this paper to discuss fault resilience and mitigation strategies in approximate DNNs, emphasizing the contrast with accurate DNN accelerators.",
      "rating": 8
    },
    "sections": null
  },
  "lee_fault_2014": {
    "base": {
      "article_type": "inproceedings",
      "title": "ault tolerance analysis of digital feed-forward deep neural network",
      "authors": [
        "Lee, Minjae",
        "Hwang, Kyuyeon",
        "Sung, Wonyong"
      ],
      "abstract": "As the homeostatis characteristics of nerve systems show, arti\ufb01cial neural networks are considered to be robust to variation of circuit components and interconnection faults. However, the tolerance of neural networks depends on many factors, such as the fault model, the network size, and the training method. In this study, we analyze the fault tolerance of \ufb01xedpoint feed-forward deep neural networks for the implementation in {CMOS} digital {VLSI}. The circuit errors caused by the interconnection as well as the processing units are considered. In addition to the conventional and dropout training methods, we develop a new technique that randomly disconnects weights during the training to increase the error resiliency. Feed-forward deep neural networks for phoneme recognition are employed for the experiments.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/ICASSP.2014.6854560"
    },
    "suggestion": {
      "relation": "This paper explores the fault tolerance of feed-forward deep neural networks in CMOS digital VLSI, focusing on circuit errors from interconnections and processing units, and introduces a novel training technique for error resiliency.",
      "suggestion": "Cite this paper to discuss the impact of hardware circuit errors on deep neural networks and the effectiveness of novel training methods for fault tolerance.",
      "rating": 8
    },
    "sections": null
  },
  "kulakov_fault_2015": {
    "base": {
      "article_type": "misc",
      "title": "ault Tolerance in Distributed Neural Computin",
      "authors": [
        "Kulakov, Anton",
        "Zwolinski, Mark",
        "Reeve, Jeff"
      ],
      "abstract": "With the increasing complexity of computing systems, complete hardware reliability can no longer be guaranteed. We need, however, to ensure overall system reliability. One of the most important features of arti\ufb01cial neural networks is their intrinsic fault-tolerance. The aim of this work is to investigate whether such networks have features that can be applied to wider computational systems. This paper presents an analysis, in both the learning and operational phases, of a distributed feedforward neural network with decentralised event-driven time management, which is insensitive to intermittent faults caused by unreliable communication or faulty hardware components. The learning rules used in the model are local in space and time, which allows ef\ufb01cient scalable distributed implementation. We investigate the overhead caused by injected faults and analyse the sensitivity to limited failures in the computational hardware in different areas of the network.",
      "publication_year": "",
      "keywords": "Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Neural and Evolutionary Computing",
      "doi": "10.13140/RG.2.1.1387.0800"
    },
    "suggestion": {
      "relation": "The paper investigates the intrinsic fault tolerance of artificial neural networks in distributed computing environments, particularly focusing on intermittent faults due to hardware unreliability.",
      "suggestion": "Reference this paper for a broader understanding of fault tolerance in neural networks beyond deep learning specific hardware.",
      "rating": 5
    },
    "sections": null
  },
  "bucker_2020_2020": {
    "base": {
      "article_type": "collection",
      "title": "020 Proceedings of the {SIAM} Workshop on Combinatorial Scientific Computin",
      "authors": [
        "dont, know"
      ],
      "abstract": "The increase in machine size and the decrease in operating voltage have made errors more common, both soft (bit \ufb02ip) and hard (component failure). Checkpointrestart solutions address the problem of hard errors, but incur signi\ufb01cant costs in time and hardware. For some algorithms, classical matrix multiplication included, tailored solutions incur much lower overhead. Existing e\ufb03cient algorithmic tolerance techniques typically aim for iterative algorithms, and thus cannot be applied to recursive algorithms, such as fast matrix multiplication algorithms. By utilizing combinatorial aspects of the computational graphs of these algorithms, we obtain fault resilience with small overhead costs, for Strassen\u2019s and other recursive fast matrix multiplication algorithms. To the best of our knowledge, this is the \ufb01rst faulttolerant solution tailored for fast matrix multiplication algorithm. Our solution is asymptotically better than any of the previous (classical based) fault-tolerant solutions, unless the error rate is extremely high. Our technique can be used to obtain fault tolerance for other recursive algorithms. In addition, we show how to reduce communication costs using additional processors, and discuss inherent fault tolerance capabilities of fast matrix multiplication algorithms.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1137/1.9781611976229"
    },
    "suggestion": {
      "relation": "This study presents a fault-tolerant solution for fast matrix multiplication algorithms, addressing hardware errors in large-scale computational systems.",
      "suggestion": "Utilize this paper to discuss algorithmic approaches to hardware fault tolerance in computational tasks relevant to deep learning.",
      "rating": 6
    },
    "sections": null
  },
  "chen_fault_2018": {
    "base": {
      "article_type": "inproceedings",
      "title": "ault Tolerant One-sided Matrix Decompositions on Heterogeneous Systems with {GPUs",
      "authors": [
        "Chen, Jieyang",
        "Li, Hongbo",
        "Li, Sihuan",
        "Liang, Xin",
        "Wu, Panruo",
        "Tao, Dingwen",
        "Ouyang, Kaiming",
        "Liu, Yuanlai",
        "Zhao, Kai",
        "Guan, Qiang",
        "Chen, Zizhong"
      ],
      "abstract": "Current algorithm-based fault tolerance ({ABFT}) approach for one-sided matrix decomposition on heterogeneous systems with {GPUs} have following limitations: (1) they do not provide suf\ufb01cient protection as most of them only maintain checksum in one dimension; (2) their checking scheme is not ef\ufb01cient due to redundant checksum veri\ufb01cations; (3) they fail to protect {PCIe} communication; and (4) the checksum calculation based on a special type of matrix multiplication is far from ef\ufb01cient. By overcoming the above limitations, we design an ef\ufb01cient {ABFT} approach providing stronger protection for onesided matrix decomposition methods on heterogeneous systems. First, we provide full matrix protection by using checksums in two dimensions. Second, our checking scheme is more ef\ufb01cient by prioritizing the checksum veri\ufb01cation according to the sensitivity of matrix operations to soft errors. Third, we protect {PCIe} communication by reordering checksum veri\ufb01cations and decomposition steps. Fourth, we accelerate the checksum calculation by 1.7x via better utilizing {GPUs}.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/SC.2018.00071"
    },
    "suggestion": {
      "relation": "The paper introduces an efficient algorithm-based fault tolerance (ABFT) approach for matrix decomposition on GPUs, addressing several limitations of previous methods including protection against soft errors and PCIe communication faults.",
      "suggestion": "Cite this paper to discuss advancements in fault tolerance methods for deep learning computations on heterogeneous systems with GPUs.",
      "rating": 7
    },
    "sections": null
  },
  "li_fault-tolerant_2023": {
    "base": {
      "article_type": "article",
      "title": "ault-Tolerant Computation Meets Network Coding: Optimal Scheduling in Parallel Computin",
      "authors": [
        "Li, Congduan",
        "Zhang, Yiqian",
        "Tan, Chee Wei"
      ],
      "abstract": "In large-scale parallel computing systems, machines and the network suffer from non-negligible faults, often leading to system crashes. The traditional method to increase reliability is to restart the failed jobs. To avoid unnecessary time wasted on reboots, we propose an optimal scheduling strategy to enable fault-tolerant reliable computation to protect the integrity of computation. Specifically, we determine the optimal redundancy-failure rate tradeoff to incorporate redundancy into parallel computing units running multiple-precision arithmetics, like the Chinese Remainder Theorem, that are useful for applications such as asymmetric cryptography and fast integer multiplication. Inspired by network coding in distributed storage for disk failures, we propose coding matrices to strategically map partial computation to available computing units, so that the central unit can reliably reconstruct the results of any failed machine without recalculations to yield the final correct computation output. We propose optimization-based algorithms to efficiently construct the optimal coding matrices subject to fault tolerance specifications. Performance evaluation demonstrates that the optimal scheduling effectively reduces the overall running time of parallel computing while resisting wide-ranging failure rates.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TCOMM.2023.3275166"
    },
    "suggestion": {
      "relation": "It proposes an optimal scheduling strategy for fault-tolerant computation in parallel computing systems, using network coding to manage machine and network faults.",
      "suggestion": "Reference this paper for discussing fault-tolerant strategies in parallel computing environments that could be applied to deep learning.",
      "rating": 4
    },
    "sections": null
  },
  "barbirotta_fault-tolerant_2023": {
    "base": {
      "article_type": "article",
      "title": "ault-Tolerant Hardware Acceleration for High-Performance Edge-Computing Node",
      "authors": [
        "Barbirotta, Marcello",
        "Cheikh, Abdallah",
        "Mastrandrea, Antonio",
        "Menichelli, Francesco",
        "Angioli, Marco",
        "Jamili, Saeid",
        "Olivieri, Mauro"
      ],
      "abstract": "High-performance embedded systems with powerful processors, specialized hardware accelerators, and advanced software techniques are all key technologies driving the growth of the {IoT}. By combining hardware and software techniques, it is possible to increase the overall reliability and safety of these systems by designing embedded architectures that can continue to function correctly in the event of a failure or malfunction. In this work, we fully investigate the integration of a con\ufb01gurable hardware vector acceleration unit in the fault-tolerant {RISC}-V Klessydra-{fT}03 soft core, introducing two different redundant vector co-processors coupled with the Interleaved-{MultiThreading} paradigm on which the microprocessor is based. We then illustrate the pros and cons of both approaches, comparing their impacts on performance and hardware utilization with their vulnerability, presenting a quantitative large-fault-injection simulation analysis on typical vector computing benchmarks, and comparing and classifying the obtained results. The results demonstrate, under speci\ufb01c conditions, that it is possible to add a hardware co-processor to a fault-tolerant microprocessor, improving performance without degrading safety and reliability.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.3390/electronics12173574"
    },
    "suggestion": {
      "relation": "This work explores fault-tolerant hardware acceleration in high-performance edge-computing nodes, specifically focusing on the integration of redundant vector co-processors in a RISC-V architecture.",
      "suggestion": "Cite this paper to discuss the integration of fault-tolerant hardware accelerators in edge-computing for deep learning applications.",
      "rating": 7
    },
    "sections": null
  },
  "bombin_fault-tolerant_2024": {
    "base": {
      "article_type": "article",
      "title": "ault-Tolerant Postselection for Low-Overhead Magic State Preparatio",
      "authors": [
        "Bomb\u00edn, H\u00e9ctor",
        "Pant, Mihir",
        "Roberts, Sam",
        "Seetharam, Karthik I."
      ],
      "abstract": "We introduce a framework for fault-tolerant postselection ({FTPS}) of fault-tolerant codes and channels\u2014such as those based on surface codes\u2014using soft-information metrics based on visible syndrome and erasure information. We introduce several metrics for ranking configurations of syndromes and erasures. In particular, we introduce the logical gap (and variants thereof) as a powerful soft-information metric for predicting logical error rates of fault-tolerant channels based on topological error-correcting codes. The logical gap is roughly the unsigned weight difference between inequivalent logical corrections and is adaptable to any tailored noise model or decoder. We deploy this framework to prepare high-quality surface-code magic states with low overhead under a model of independent and identically distributed ({IID}) Pauli and erasure errors. Postselection strategies based on the logical gap can suppress the encoding error rate ({EER}) of a magic state preparation channel to the level of the physical error rate with low overhead. For example, when operating at 60\\% of the bulk threshold of the corresponding surface code, an overall reduction of the {EER} by a factor of 15 is achievable with a relative overhead factor of {\\textless} 2 (approximately 23 times less than that of simple syndrome-counting rules). We analyze a schematic buffer architecture for implementing postselection rules on magic state factories in the context of magic state distillation. The {FTPS} framework can be utilized for mitigating errors in more general fault-tolerant logical channels.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1103/PRXQuantum.5.010302"
    },
    "suggestion": {
      "relation": "The paper introduces a fault-tolerant postselection framework for quantum computing, which is less directly related to conventional hardware fault tolerance in deep learning.",
      "suggestion": "This paper might be mentioned to showcase fault tolerance techniques in quantum computing as a parallel or future consideration for deep learning systems.",
      "rating": 2
    },
    "sections": null
  },
  "ahmed_fault-tolerant_2023": {
    "base": {
      "article_type": "article",
      "title": "ault-Tolerant Neuromorphic Computing With Memristors Using Functional {ATPG} for Efficient Recalibratio",
      "authors": [
        "Ahmed, Soyed Tuhin",
        "Tahoori, Mehdi B."
      ],
      "abstract": "This article focuses on recalibration of neural networks implemented in neuromorphic in-memory computing with memristors. The primary goal of the article is to reduce the amount of data required for recalibration which makes it particularly useful in scenarios where data availability is limited or where recalibration overhead is a concern. Moreover, the proposed approach is robust against both process and temperature variations at a significantly lower overhead compared to related works. This practical method addresses an important issue that can affect the accuracy of neural networks implemented using emerging resistive nonvolatile memories.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/MDAT.2023.3270126"
    },
    "suggestion": {
      "relation": "Focuses on recalibration techniques for neuromorphic computing with memristors, aiming at reducing recalibration overhead and enhancing robustness against variations.",
      "suggestion": "Cite this paper to discuss fault tolerance and recalibration strategies in neuromorphic computing systems for deep learning.",
      "rating": 6
    },
    "sections": null
  },
  "yerima_fault-tolerant_2023": {
    "base": {
      "article_type": "article",
      "title": "ault-Tolerant Spiking Neural Network Mapping Algorithm and Architecture to 3D-{NoC}-Based Neuromorphic System",
      "authors": [
        "Yerima, Williams Yohanna",
        "Ikechukwu, Ogbodo Mark",
        "Dang, Khanh N.",
        "Abdallah, Abderazek Ben"
      ],
      "abstract": "Neuromorphic computing uses spiking neuron network models to solve machine learning problems in a more energy-efficient way when compared to conventional artificial neural networks. However, mapping the various network components to the neuromorphic hardware is not trivial to realize the desired model for an actual simulation. Moreover, neurons and synapses could be affected by noise due to external interference or random actions of other components (i.e., neurons), which eventually lead to unreliable results. This work proposes a fault-tolerant spiking neural network mapping algorithm and architecture to a 3D network-on-chip ({NoC})-based neuromorphic system (R-{NASH}-{II}) based on a rank and selection mapping mechanism ({RSM}). The {RSM} allows the ranking and rapid selection of neurons for fault-tolerant mapping. Evaluation results show that with our proposed mechanism, we could maintain a mapping efficiency of 100\\% with 20\\% spare rate and a fault rate (40\\%) more than in the previous mapping framework. The Monte Carlo simulation evaluation of reliability shows that the {RSM} mechanism has increased the mean time to failure ({MTTF}) of the previous mapping technique by 43\\% on average. Furthermore, the operational availability of the {RSM} for mapping to a 4 \u00d7 4 \u00d7 4 (smallest) and 6 \u00d7 6 \u00d7 6 (largest) {NoC} is 88\\% and 67\\% respectively.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/ACCESS.2023.3278802"
    },
    "suggestion": {
      "relation": "This paper focuses on a fault-tolerant mapping algorithm and architecture for neuromorphic systems, which is indirectly related to deep learning hardware fault tolerance by addressing noise and external interference in neuromorphic computing.",
      "suggestion": "Cite this paper to discuss fault tolerance in neuromorphic systems as a parallel or complementary approach to traditional deep learning hardware.",
      "rating": 4
    },
    "sections": null
  },
  "he_fidelity_2020": {
    "base": {
      "article_type": "inproceedings",
      "title": "FIdelity}: Efficient Resilience Analysis Framework for Deep Learning Accelerator",
      "authors": [
        "He, Yi",
        "Balaprakash, Prasanna",
        "Li, Yanjing"
      ],
      "abstract": "We present a resilience analysis framework, called {FIdelity}, to accurately and quickly analyze the behavior of hardware errors in deep learning accelerators. Our framework enables resilience analysis starting from the very beginning of the design process to ensure that the reliability requirements are met, so that these accelerators can be safely deployed for a wide range of applications, including safety-critical applications such as self-driving cars.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/MICRO50266.2020.00033"
    },
    "suggestion": {
      "relation": "The paper introduces a framework for analyzing hardware errors in deep learning accelerators, directly addressing the user's interest in hardware error fault tolerance in deep learning systems.",
      "suggestion": "Use this paper to highlight the importance of resilience analysis frameworks in the design of fault-tolerant deep learning accelerators.",
      "rating": 8
    },
    "sections": null
  },
  "weng_fkeras_nodate": {
    "base": {
      "article_type": "article",
      "title": "FKeras}: A Sensitivity Analysis Tool for Edge Neural Network",
      "authors": [
        "Weng, Olivia",
        "Meza, Andres",
        "Campos, Javier",
        "Tran, Nhan",
        "Bock, Quinlan",
        "Hawks, Benjamin",
        "Duarte, Javier Mauricio",
        "Kastner, Ryan"
      ],
      "abstract": "Edge computation often requires robustness to faults, e.g., to reduce the effects of transient errors and to function correctly in high radiation environments. In these cases, the edge device must be designed with fault tolerance as a primary objective. {FKeras} is a tool that helps design fault-tolerant edge neural networks. {FKeras} provides metrics that give a bit-level ranking of neural network weights with respect to their sensitivity to faults. {FKeras} includes these sensitivity metrics to guide efficient faulty injection campaigns to help evaluate the robustness of a neural network architecture. We show how to use {FKeras} in the co-design of edge {NNs} trained on the high-granularity endcap calorimeter dataset, which represents high energy physics data, as well as the {CIFAR}-10 dataset. We use {FKeras} to analyze network\u2019s fault tolerance to consider alongside its accuracy, performance, and resource consumption. The results show that the different architectures have vastly differing resilience to faults.",
      "publication_year": "",
      "keywords": "",
      "doi": ""
    },
    "suggestion": {
      "relation": "FKeras is a tool designed for enhancing fault tolerance in edge neural networks, which aligns with the topic by focusing on reducing the impact of transient errors in hardware for deep learning.",
      "suggestion": "Reference this paper to discuss tools and methodologies for improving hardware fault tolerance in edge computing scenarios.",
      "rating": 7
    },
    "sections": null
  },
  "dey_fpga-based_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "FPGA}-based Design of a Fault Injector for Binarized Convolutional Neural Networ",
      "authors": [
        "Dey, Sowvik",
        "Patra, Arijit",
        "Karmakar, Amiya"
      ],
      "abstract": "Artificial Intelligence ({AI}) is used in every modern system with the advancement of technology. Convolutional Neural Network ({CNN}) is a popular and advanced {AI} technique for feature extraction and is widely used in safety-critical embedded systems. Fault tolerance must be a significant aspect of such embedded systems. To test various algorithms related to fault tolerance, the system must have a faulty condition. The Fault injection technique is used in the embedded system for testing purposes. This technique introduces faults into the system to ensure erroneous behaviour of that system. This paper presents a fault injection model to incorporate faults into the binarized convolution Neural Network ({BCNN}). It may help researchers to reduce the effort of fault-tolerant {CNN} simulation. An {FPGA}-based design has also been developed to validate the proposed model.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/DevIC57758.2023.10134830"
    },
    "suggestion": {
      "relation": "This paper presents a fault injection model for testing fault tolerance in convolutional neural networks, relevant to understanding how hardware errors can be simulated and managed in deep learning systems.",
      "suggestion": "Cite this paper to discuss fault injection techniques as a method for evaluating and enhancing fault tolerance in deep learning hardware.",
      "rating": 6
    },
    "sections": null
  },
  "zhai_ft-blas_2023": {
    "base": {
      "article_type": "article",
      "title": "FT}-{BLAS}: A Fault Tolerant High Performance {BLAS} Implementation on x86 {CPUs",
      "authors": [
        "Zhai, Yujia",
        "Giem, Elisabeth",
        "Zhao, Kai",
        "Liu, Jinyang",
        "Huang, Jiajun",
        "Wong, Bryan M.",
        "Shelton, Christian R.",
        "Chen, Zizhong"
      ],
      "abstract": "Basic Linear Algebra Subprograms ({BLAS}) serve as a foundational library for scienti\ufb01c computing and machine learning. In this article, we present a new {BLAS} implementation, {FT}-{BLAS}, that provides performance comparable to or faster than state-ofthe-art {BLAS} libraries, while being capable of tolerating soft errors on-the-\ufb02y. At the algorithmic level, we propose a hybrid strategy to incorporate fault-tolerant functionality. For memory-bound Level1 and Level-2 {BLAS} routines, we duplicate computing instructions and re-use data at the register level to avoid memory overhead when validating the runtime correctness. Here we novelly propose to utilize mask registers on {AVX}512-enabled processors and {SIMD} registers on {AVX}2-enabled processors to store intermediate comparison results. For compute-bound Level-3 {BLAS} routines, we fuse memory-intensive operations such as checksum encoding and veri\ufb01cation into the {GEMM} assembly kernels to optimize the memory footprint. We also design cache-friendly parallel algorithms for our fault-tolerant library. Through a series of architectural-aware optimizations, we manage to maintain the fault-tolerant overhead at a negligible order ({\\textless}3\\%). Experimental results obtained on widely-used processors such as Intel Skylake, Intel Cascade Lake, and {AMD} Zen2 demonstrate that {FT}-{BLAS} offers high reliability and high performance \u2013 faster than Intel {MKL}, {OpenBLAS}, and {BLIS} by up to 3.50\\%, 22.14\\%, and 21.70\\%, respectively, for both serial and parallel routines spanning all three levels of {BLAS} we benchmarked, even under hundreds of errors injected per minute.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TPDS.2023.3316011"
    },
    "suggestion": {
      "relation": "Although focused on BLAS implementations, this paper's discussion on fault-tolerant high-performance computing is tangentially related to deep learning by addressing soft errors in scientific computing and machine learning libraries.",
      "suggestion": "Mention this paper to provide a broader context on fault tolerance strategies in computational libraries used by deep learning frameworks.",
      "rating": 5
    },
    "sections": null
  },
  "hoang_ft-clipact_2020": {
    "base": {
      "article_type": "inproceedings",
      "title": "FT}-{ClipAct}: Resilience Analysis of Deep Neural Networks and Improving their Fault Tolerance using Clipped Activatio",
      "authors": [
        "Hoang, Le-Ha",
        "Hanif, Muhammad Abdullah",
        "Shafique, Muhammad"
      ],
      "abstract": "Deep Neural Networks ({DNNs}) are widely being adopted for safety-critical applications, e.g., healthcare and autonomous driving. Inherently, they are considered to be highly error-tolerant. However, recent studies have shown that hardware faults that impact the parameters of a {DNN} (e.g., weights) can have drastic impacts on its classi\ufb01cation accuracy. In this paper, we perform a comprehensive error resilience analysis of {DNNs} subjected to hardware faults (e.g., permanent faults) in the weight memory. The outcome of this analysis is leveraged to propose a novel error mitigation technique which squashes the highintensity faulty activation values to alleviate their impact. We achieve this by replacing the unbounded activation functions with their clipped versions. We also present a method to systematically de\ufb01ne the clipping values of the activation functions that result in increased resilience of the networks against faults. We evaluate our technique on the {AlexNet} and the {VGG}-16 {DNNs} trained for the {CIFAR}-10 dataset. The experimental results show that our mitigation technique signi\ufb01cantly improves the resilience of the {DNNs} to faults. For example, the proposed technique offers on average 68.92\\% improvement in the classi\ufb01cation accuracy of resilience-optimized {VGG}-16 model at 1 \u00d7 10\u22125 fault rate, when compared to the base network without any fault mitigation.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.23919/DATE48585.2020.9116571"
    },
    "suggestion": {
      "relation": "The paper directly addresses fault tolerance in deep neural networks by analyzing hardware faults' impact on DNNs and proposing a mitigation technique, which is highly relevant to the literature review topic.",
      "suggestion": "Use this paper to discuss specific strategies for improving fault tolerance in deep learning systems through clipped activations.",
      "rating": 9
    },
    "sections": null
  },
  "abdi_ft-ealu_2023": {
    "base": {
      "article_type": "article",
      "title": "FT}-{EALU}: fault-tolerant arithmetic and logic unit for critical embedded and real-time system",
      "authors": [
        "Abdi, Athena",
        "Shahoveisi, Sina"
      ],
      "abstract": "This paper presents a fault-tolerant {ALU} (\u201c{FT}-{EALU}\u201d) based on time redundancy and reward/punishment-based learning approaches for real-time embedded systems that face limitations in hardware and power consumption budgets. In this method, operations are diversified to three versions in order to correct permanent faults along with the transient ones. The diversities of versions considered in {FT}-{EALU} are provided by lightweight modifications to differentiate them and clear the effect of permanent faults. Selecting lightweight modifications such as shift and swap would avoid high timing overhead in computation while providing significant differences which are necessary for fault detection. Next, the replicated versions are executed serially in time, and their corresponding results are voted based on the derived learned weights. The proposed weighted voting module generates the final output based on the results and their weights. In the proposed weighted voting module, a reward/punishment strategy is employed to provide the weight of each version of execution indicating its effectiveness in the final output. To this aim, in the method defined for each version of execution, a weight is defined according to its correction capability confronting several faulty scenarios. Thus, this weight defines the reliability of the temporal results as well as their effect on the final result. The final result is generated bit by bit based on the weight of each version of execution and its computed result. Based on the proposed learning scheme, positive or negative weights are assigned to execution versions. These weights are derived in bit level based on the capability of execution versions in mitigating permanent faults in several fault injection scenarios. Thus, our proposed method is low cost and more efficient compared to related research which are mainly based on information and hardware redundancy due to employing time redundancy and static learning approach in correcting permanent faults. Several experiments are performed to reveal the efficiency of our proposed approach based on which {FT}-{EALU} is capable of correcting about 84.93\\% and 69.71\\% of permanent injected faults on single and double bits of input data.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1007/s11227-022-04698-8"
    },
    "suggestion": {
      "relation": "This paper presents a fault-tolerant arithmetic and logic unit design for embedded systems, which is somewhat related to the topic by addressing hardware fault tolerance but focuses on a more general computing context rather than deep learning specifically.",
      "suggestion": "Reference this paper to discuss fault tolerance in embedded systems as a foundational aspect that can influence deep learning hardware design.",
      "rating": 3
    },
    "sections": null
  },
  "wu_ft-gemm_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "FT}-{GEMM}: A Fault Tolerant High Performance {GEMM} Implementation on x86 {CPUs",
      "authors": [
        "Wu, Shixun",
        "Zhai, Yujia",
        "Huang, Jiajun",
        "Jian, Zizhe",
        "Chen, Zizhong"
      ],
      "abstract": "General matrix/matrix multiplication ({GEMM}) is crucial for scientific computing and machine learning. However, the increased scale of the computing platforms raises concerns about hardware and software reliability. In this poster, we present {FT}-{GEMM}, a high-performance {GEMM} being capable of tolerating soft errors on-the-fly. We incorporate the fault tolerant functionality at algorithmic level by fusing the memory-intensive operations into the {GEMM} assembly kernels. We design a cache-friendly scheme for parallel {FT}-{GEMM}. Experimental results on Intel Cascade Lake demonstrate that {FT}-{GEMM} offers high reliability and performance \u2013 faster than Intel {MKL}, {OpenBLAS}, and {BLIS} by 3.50\\%\u223c 22.14\\% for both serial and parallel {GEMM}, even under hundreds of errors injected per minute.",
      "publication_year": "",
      "keywords": "Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance",
      "doi": "10.1145/3588195.3595947"
    },
    "suggestion": {
      "relation": "FT-GEMM discusses a fault-tolerant implementation for matrix multiplication on CPUs, relevant to deep learning as GEMM operations are central to many deep learning algorithms, addressing hardware fault tolerance indirectly.",
      "suggestion": "Cite this paper to discuss how fault tolerance can be integrated into core computational operations used in deep learning.",
      "rating": 6
    },
    "sections": null
  },
  "chen_ftpipehd_2024": {
    "base": {
      "article_type": "article",
      "title": "FTPipeHD}: A Fault-Tolerant Pipeline-Parallel Distributed Training Approach for Heterogeneous Edge Device",
      "authors": [
        "Chen, Yuhao",
        "Yang, Qianqian",
        "He, Shibo",
        "Shi, Zhiguo",
        "Chen, Jiming",
        "Guizani, Mohsen"
      ],
      "abstract": "With the increasing proliferation of Internet-of-Things ({IoT}) devices, there is a growing trend towards distributing the power of deep learning ({DL}) among edge devices rather than centralizing it at the cloud. To deploy deep and complex models at edge devices with limited resources, model partitioning of deep neural network ({DNN}) models has been widely studied. However, most of the existing literature only considers distributing the inference model while still training the model at the cloud. In this paper, we propose {FTPipeHD}, a novel {DNN} training approach that trains {DNN} models across distributed heterogeneous devices with the fault-tolerance mechanism.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TMC.2023.3272567"
    },
    "suggestion": {
      "relation": "FTPipeHD introduces a fault-tolerant, distributed deep neural network training approach for edge devices, addressing hardware error resilience in distributed deep learning scenarios.",
      "suggestion": "Cite this paper to discuss fault tolerance in distributed deep learning training across heterogeneous edge devices.",
      "rating": 7
    },
    "sections": null
  },
  "zhang_ftsc_2023": {
    "base": {
      "article_type": "article",
      "title": "FTSC}: Fault-tolerant scheduling and control co-design for distributed real-time syste",
      "authors": [
        "Zhang, Yuanhai",
        "Xu, Zijin",
        "Zhang, Yibo",
        "Guan, Nan",
        "Zhao, Shuai",
        "Chen, Gang",
        "Huang, Kai"
      ],
      "abstract": "Ensuring control stability is essential for safety-critical systems. Task redundancy is one commonly employed technique to enhance control performance in applications afflicted by intermittent faults. While increasing the number of replicas can improve control performance, it also introduces challenges in terms of scheduling and mapping. Prior research primarily focuses on controller design to enhance control performance, with limited consideration given to fault tolerance and scheduling in a combined manner. In this paper, we propose a co-design scheme of Fault-Tolerant Scheduling and Control ({FTSC}), which aims to provide stable control performance along with feasible mapping and scheduling solutions. We present a comprehensive formulation that encompasses mapping, scheduling, and fault tolerance, optimizing control performance into a Mixed Integer Linear Programming ({MILP}) framework. The problem of task redundancy can either be addressed through a heuristic algorithm efficiently or be solved in a {MILP}-based approach with a better solution. A case study is conducted to demonstrate the effectiveness of our proposed scheme, highlighting its ability to reduce system control costs while meeting real-time, reliability, and schedulability constraints.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.sysarc.2023.102934"
    },
    "suggestion": {
      "relation": "FTSC focuses on fault-tolerant scheduling and control for distributed real-time systems, integrating fault tolerance with scheduling, which indirectly supports deep learning applications' reliability.",
      "suggestion": "Reference this paper for discussing the broader implications of fault tolerance in scheduling for deep learning applications.",
      "rating": 4
    },
    "sections": null
  },
  "didehban_generic_2024": {
    "base": {
      "article_type": "article",
      "title": "eneric Soft Error Data and Control Flow Error Detection by Instruction Duplicatio",
      "authors": [
        "Didehban, Moslem",
        "So, Hwisoo",
        "Gali, Prudhvi",
        "Shrivastava, Aviral",
        "Lee, Kyoungwoo"
      ],
      "abstract": "Transient faults or soft errors are considered one of the most daunting reliability challenges for microprocessors. Software solutions for soft error protection are attractive because they can provide flexible and effective error protection. For instance, {nZDC} [1] state-of-the-art instruction duplication error protection scheme achieves a high degree of error detection by verifying the results of memory write operations and utilizes an effective control-flow checking mechanism. However, {nZDC} control-flow checking mechanism is architecture-dependent and suffers from some vulnerability holes. In this work, we address these issues by substituting {nZDC} control-flow checking mechanism with a general ({ISA}-independent) scheme and propose two transformations, coarse-grained scheduling and asymmetric control-flow signatures, for hard-to-detect control flow errors. Fault injection experiments on different hardware components of synthesizable Verilog description of an {OpenRISC}-based microprocessor reveal that the proposed transformation shows 85\\% less silent data corruptions compared to {nZDC}. In addition, programs protected by the proposed scheme run on average around 37\\% faster than {nZDC}-protected programs.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TDSC.2023.3245842"
    },
    "suggestion": {
      "relation": "This paper proposes a software solution for detecting soft errors in microprocessors, which can be relevant for ensuring the reliability of hardware running deep learning models.",
      "suggestion": "Use this paper to discuss software-level solutions for hardware error detection in the context of deep learning.",
      "rating": 5
    },
    "sections": null
  },
  "chen_gpu-abft_2016": {
    "base": {
      "article_type": "inproceedings",
      "title": "GPU}-{ABFT}: Optimizing Algorithm-Based Fault Tolerance for Heterogeneous Systems with {GPUs",
      "authors": [
        "Chen, Jieyang",
        "Li, Sihuan",
        "Chen, Zizhong"
      ],
      "abstract": "For matrix operations, the algorithm-based fault tolerance ({ABFT}) brings much lower fault tolerance overhead than the traditional Triple Modular Redundancy or Double Modular Redundancy approaches. Many works have been done to develop and optimize {ABFT} schemes on general purpose microprocessors. However, the {ABFT} schemes on heterogeneous systems with {GPUs} are not fully developed and optimized. Moreover, existing {ABFT} schemes can correct computing errors brings by the logic parts, however, many memory storage errors cannot be detected and corrected by current {ABFT} schemes. In this work, we designed a new {ABFT} scheme with both computing and memory storage protection. Then, we apply it to Cholesky decomposition on heterogeneous systems with {GPUs}. In addition, we develop several fault tolerance overhead reduction techniques speci\ufb01cally for heterogeneous systems with {GPUs} accelerators. Experimental results show that our {ABFT} scheme is able to correct both computing error and memory storage error with low overhead and comparable overall performance.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/NAS.2016.7549404"
    },
    "suggestion": {
      "relation": "GPU-ABFT presents an optimized fault tolerance scheme for GPUs, directly applicable to deep learning computations on heterogeneous systems.",
      "suggestion": "Cite this work to discuss algorithm-based fault tolerance in deep learning computations on GPUs.",
      "rating": 8
    },
    "sections": null
  },
  "wei_g-seap_2020": {
    "base": {
      "article_type": "article",
      "title": "-{SEAP}: Analyzing and characterizing soft-error aware approximation in {GPGPUs",
      "authors": [
        "Wei, Xiaohui",
        "Yue, Hengshan",
        "Gao, Shang",
        "Li, Lina",
        "Zhang, Ruyu",
        "Tan, Jingweijia"
      ],
      "abstract": "As General-Purpose Graphics Processing Units ({GPGPUs}) become pervasive for the High-Performance Computing ({HPC}), ensuring that programs can be protected from soft errors has become increasingly important. Soft errors may cause Silent Data Corruptions ({SDCs}), which produces erroneous execution results silently. Due to the massive parallelism of {GPGPUs}, fully protecting them against soft errors introduces nontrivial overhead. Fortunately, imprecise execution outcomes are inherently tolerable for some {HPC} programs due to the nature of these applications. Leveraging the feature, selective soft error protection can be applied to reduce energy consumptions.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.future.2020.03.040"
    },
    "suggestion": {
      "relation": "-SEAP explores soft-error aware approximation in GPGPUs, relevant for understanding how soft errors can impact deep learning computations on GPUs.",
      "suggestion": "Reference this paper to discuss the impact of soft errors on deep learning computations in GPGPUs and potential mitigation strategies.",
      "rating": 6
    },
    "sections": null
  },
  "mahmoud_hardnn_2020": {
    "base": {
      "article_type": "misc",
      "title": "HarDNN}: Feature Map Vulnerability Evaluation in {CNNs",
      "authors": [
        "Mahmoud, Abdulrahman",
        "Hari, Siva Kumar Sastry",
        "Fletcher, Christopher W.",
        "Adve, Sarita V.",
        "Sakr, Charbel",
        "Shanbhag, Naresh",
        "Molchanov, Pavlo",
        "Sullivan, Michael B.",
        "Tsai, Timothy",
        "Keckler, Stephen W."
      ],
      "abstract": "As Convolutional Neural Networks ({CNNs}) are increasingly being employed in safety-critical applications, it is important that they behave reliably in the face of hardware errors. Transient hardware errors may percolate undesirable state during execution, resulting in software-manifested errors which can adversely affect high-level decision making. This paper presents {HarDNN}, a software-directed approach to identify vulnerable computations during a {CNN} inference and selectively protect them based on their propensity towards corrupting the inference output in the presence of a hardware error. We show that {HarDNN} can accurately estimate relative vulnerability of a feature map (fmap) in {CNNs} using a statistical error injection campaign, and explore heuristics for fast vulnerability assessment. Based on these results, we analyze the tradeoff between error coverage and computational overhead that the system designers can use to employ selective protection. Results show that the improvement in resilience for the added computation is superlinear with {HarDNN}. For example, {HarDNN} improves {SqueezeNet}\u2019s resilience by 10\u00d7 with just 30\\% additional computations.",
      "publication_year": "",
      "keywords": "Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning",
      "doi": ""
    },
    "suggestion": {
      "relation": "HarDNN evaluates feature map vulnerability in CNNs to hardware errors, offering insights into selective protection strategies for deep learning models.",
      "suggestion": "Cite this paper to discuss methods for assessing and mitigating the impact of hardware errors on CNNs.",
      "rating": 9
    },
    "sections": null
  },
  "traiola_hardnning_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "harDNNing}: a machine-learning-based framework for fault tolerance assessment and protection of {DNNs",
      "authors": [
        "Traiola, Marcello",
        "Kritikakou, Angeliki",
        "Sentieys, Olivier"
      ],
      "abstract": "Deep Neural Networks ({DNNs}) show promising performance in several application domains, such as robotics, aerospace, smart healthcare, and autonomous driving. Nevertheless, {DNN} results may be incorrect, not only because of the network intrinsic inaccuracy, but also due to faults affecting the hardware. Indeed, hardware faults may impact the {DNN} inference process and lead to prediction failures. Therefore, ensuring the fault tolerance of {DNN} is crucial. However, common fault tolerance approaches are not cost-effective for {DNNs} protection, because of the prohibitive overheads due to the large size of {DNNs} and of the required memory for parameter storage. In this work, we propose a comprehensive framework to assess the fault tolerance of {DNNs} and cost-effectively protect them. As a first step, the proposed framework performs datatype-and-layer-based fault injection, driven by the {DNN} characteristics. As a second step, it uses classification-based machine learning methods in order to predict the criticality, not only of network parameters, but also of their bits. Last, dedicated Error Correction Codes ({ECCs}) are selectively inserted to protect the critical parameters and bits, hence protecting the {DNNs} with low cost. Thanks to the proposed framework, we explored and protected two Convolutional Neural Networks ({CNNs}), each with four different data encoding. The results show that it is possible to protect the critical network parameters with selective {ECCs} while saving up to 83\\% memory w.r.t. conventional {ECC} approaches.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/ETS56758.2023.10174178"
    },
    "suggestion": {
      "relation": "harDNNing proposes a machine-learning-based framework for assessing and protecting DNNs from hardware faults, focusing on cost-effective fault tolerance.",
      "suggestion": "Use this paper to discuss machine learning approaches to fault tolerance in DNNs, highlighting cost-effective strategies.",
      "rating": 9
    },
    "sections": null
  },
  "rogenmoser_hybrid_2023": {
    "base": {
      "article_type": "article",
      "title": "ybrid Modular Redundancy: Exploring Modular Redundancy Approaches in {RISC}-V Multi-Core Computing Clusters for Reliable Processing in Spac",
      "authors": [
        "Rogenmoser, Michael",
        "Tortorella, Yvan",
        "Rossi, Davide",
        "Conti, Francesco",
        "Benini, Luca"
      ],
      "abstract": "Space Cyber-Physical Systems (S-{CPS}) such as spacecraft and satellites strongly rely on the reliability of onboard computers to guarantee the success of their missions. Relying solely on radiation-hardened technologies is extremely expensive, and developing inflexible architectural and microarchitectural modifications to introduce modular redundancy within a system leads to significant area increase and performance degradation. To mitigate the overheads of traditional radiation hardening and modular redundancy approaches, we present a novel Hybrid Modular Redundancy ({HMR}) approach, a redundancy scheme that features a cluster of {RISC}-V processors with a flexible on-demand dual-core and triple-core lockstep grouping of computing cores with runtime split-lock capabilities. Further, we propose two recovery approaches, software-based and hardware-based, trading off performance and area overhead. Running at 430 {MHz}, our fault-tolerant cluster achieves up to 1160 {MOPS} on a matrix multiplication benchmark when configured in non-redundant mode and 617 and 414 {MOPS} in dual and triple mode, respectively. A software-based recovery in triple mode requires 363 clock cycles and occupies 0.612 mm2, representing a 1.3\\% area overhead over a non-redundant 12-core {RISC}-V cluster. As a high-performance alternative, a new hardware-based method provides rapid fault recovery in just 24 clock cycles and occupies 0.660 mm2, namely {\\textasciitilde}9.4\\% area overhead over the baseline non-redundant {RISC}-V cluster. The cluster is also enhanced with split-lock capabilities to enter one of the redundant modes with minimum performance loss, allowing execution of a mission-critical or a performance section, with {\\textless}400 clock cycles overhead for entry and exit. The proposed system is the first to integrate these functionalities on an open-source {RISC}-V-based compute device, enabling finely tunable reliability vs. performance trade-offs.",
      "publication_year": "",
      "keywords": "Computer Science - Hardware Architecture, Electrical Engineering and Systems Science - Systems and Control",
      "doi": "10.1145/3635161"
    },
    "suggestion": {
      "relation": "Hybrid Modular Redundancy explores redundancy schemes in RISC-V multi-core clusters for reliable processing, which can be applied to ensure deep learning model reliability.",
      "suggestion": "Reference this paper to discuss modular redundancy approaches for enhancing the reliability of deep learning computations.",
      "rating": 6
    },
    "sections": null
  },
  "jeon_hybrid_2023": {
    "base": {
      "article_type": "article",
      "title": "ybrid Precision in Resistive Memory-Based Convolutional Kernel for Fault-Resilient Neuromorphic System",
      "authors": [
        "Jeon, Seonuk",
        "Hong, Eunryeong",
        "Kang, Heebum",
        "Kim, Hyun Wook",
        "Kim, Nayeon",
        "Woo, Jiyong"
      ],
      "abstract": "As simple convolution computation is intensively and iteratively performed to extract features from input images, cross-point arrays with resistive random access memory ({RRAM}) serving as a kernel weight can accelerate the relevant mathematical operations in hardware. However, considering actual {RRAM} characteristics, either variability or unexpected permanent failure from the filamentary switching mechanism is observed, degrading recognition performance. This study investigates the impact of fault in a conventional kernel structure, where two adjacent columns in the array represent a single weight, on feature extraction using {MATLAB}. First, the fault types of {HfOx}-based multilevel {RRAM} is categorized. The results reveal that the unidirectional fault of {RRAM} primarily worsens the accuracy of image recognition. This is because the subtraction of negative weights from positive ones is crucial for identifying the edges of images through convolution operations. Therefore, we exploit a kernel structure, in which a single column dedicated to negative weights is located next to a matrix of positive weights. In addition, we reduce the weight precision for negative weights, while quantizing positive weights to higher bits. By mitigating the subtraction errors achieved by the kernel structure with hybrid precision, we improved fault tolerance, minimizing accuracy degradation.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TED.2023.3244761"
    },
    "suggestion": {
      "relation": "This paper explores fault tolerance in neuromorphic systems using hybrid precision in resistive memory-based convolutional kernels, focusing on minimizing accuracy degradation due to hardware faults.",
      "suggestion": "Cite this paper to discuss hardware fault tolerance strategies in neuromorphic systems and the role of hybrid precision in enhancing fault resilience.",
      "rating": 8
    },
    "sections": null
  },
  "liu_hyca_2021": {
    "base": {
      "article_type": "misc",
      "title": "HyCA}: A Hybrid Computing Architecture for Fault Tolerant Deep Learnin",
      "authors": [
        "Liu, Cheng",
        "Chu, Cheng",
        "Xu, Dawen",
        "Wang, Ying",
        "Wang, Qianlong",
        "Li, Huawei",
        "Li, Xiaowei",
        "Cheng, Kwang-Ting"
      ],
      "abstract": "Hardware faults on the regular 2-D computing array of a typical deep learning accelerator ({DLA}) can lead to dramatic prediction accuracy loss. Prior redundancy design approaches typically have each homogeneous redundant processing element ({PE}) to mitigate faulty {PEs} for a limited region of the 2-D computing array rather than the entire computing array to avoid the excessive hardware overhead. However, they fail to recover the computing array when the number of faulty {PEs} in any region exceeds the number of redundant {PEs} in the same region. The mismatch problem deteriorates when the fault injection rate rises and the faults are unevenly distributed. To address the problem, we propose a hybrid computing architecture ({HyCA}) for faulttolerant {DLAs}. It has a set of dot-production processing units ({DPPUs}) to recompute all the operations that are mapped to the faulty {PEs} despite the faulty {PE} locations. According to our experiments, {HyCA} shows signi\ufb01cantly higher reliability, scalability, and performance with less chip area penalty when compared to the conventional redundancy approaches. Moreover, by taking advantage of the \ufb02exible recomputing, {HyCA} can also be utilized to scan the entire 2-D computing array and detect the faulty {PEs} effectively at runtime.",
      "publication_year": "",
      "keywords": "Computer Science - Hardware Architecture",
      "doi": ""
    },
    "suggestion": {
      "relation": "The paper introduces a hybrid computing architecture designed to enhance fault tolerance in deep learning accelerators by addressing the limitations of conventional redundancy approaches.",
      "suggestion": "Reference this paper for an example of innovative architecture design aimed at improving hardware fault tolerance in deep learning systems.",
      "rating": 9
    },
    "sections": null
  },
  "khanfir_ibir_2023": {
    "base": {
      "article_type": "article",
      "title": "iBiR}: Bug-report-driven Fault Injectio",
      "authors": [
        "Khanfir, Ahmed",
        "Koyuncu, Anil",
        "Papadakis, Mike",
        "Cordy, Maxime",
        "Bissyand\u00e9, Tegawende F.",
        "Klein, Jacques",
        "Le Traon, Yves"
      ],
      "abstract": "Much research on software engineering relies on experimental studies based on fault injection. Fault injection, however, is not often relevant to emulate real-world software faults since it \u201cblindly\u201d injects large numbers of faults. It remains indeed challenging to inject few but realistic faults that target a particular functionality in a program. In this work, we introduce {iBiR}, a fault injection tool that addresses this challenge by exploring change patterns associated to user-reported faults. To inject realistic faults, we create mutants by re-targeting a bug-report-driven automated program repair system, i.e., reversing its code transformation templates. {iBiR} is further appealing in practice since it requires deep knowledge of neither code nor tests, just of the program\u2019s relevant bug reports. Thus, our approach focuses the fault injection on the feature targeted by the bug report. We assess {iBiR} by considering the Defects4J dataset. Experimental results show that our approach outperforms the fault injection performed by traditional mutation testing in terms of semantic similarity with the original bug, when applied at either system or class levels of granularity, and provides better, statistically significant estimations of test effectiveness (fault detection). Additionally, when injecting 100 faults, {iBiR} injects faults that couple with the real ones in around 36\\% of the cases, while mutation testing achieves less than 4\\%.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1145/3542946"
    },
    "suggestion": {
      "relation": "Although focused on software fault injection based on bug reports, this paper's methodology and insights are less directly applicable to hardware fault tolerance in deep learning.",
      "suggestion": "This paper could be mentioned in a broader discussion on fault injection techniques, albeit with a clear distinction between its software focus and the hardware context of the review.",
      "rating": 3
    },
    "sections": null
  },
  "yuan_improving_2021": {
    "base": {
      "article_type": "misc",
      "title": "mproving {DNN} Fault Tolerance using Weight Pruning and Differential Crossbar Mapping for {ReRAM}-based Edge {AI",
      "authors": [
        "Yuan, Geng",
        "Liao, Zhiheng",
        "Ma, Xiaolong",
        "Cai, Yuxuan",
        "Kong, Zhenglun",
        "Shen, Xuan",
        "Fu, Jingyan",
        "Li, Zhengang",
        "Zhang, Chengming",
        "Peng, Hongwu",
        "Liu, Ning",
        "Ren, Ao",
        "Wang, Jinhui",
        "Wang, Yanzhi"
      ],
      "abstract": "Recent research demonstrated the promise of using resistive random access memory ({ReRAM}) as an emerging technology to perform inherently parallel analog domain in-situ matrix-vector multiplication\u2014the intensive and key computation in deep neural networks ({DNNs}). However, hardware failure, such as stuck-at-fault defects, is one of the main concerns that impedes the {ReRAM} devices to be a feasible solution for real implementations. The existing solutions to address this issue usually require an optimization to be conducted for each individual device, which is impractical for mass-produced products (e.g., {IoT} devices). In this paper, we rethink the value of weight pruning in {ReRAM}-based {DNN} design from the perspective of model fault tolerance. And a differential mapping scheme is proposed to improve the fault tolerance under a high stuck-on fault rate. Our method can tolerate almost an order of magnitude higher failure rate than the traditional two-column method in representative {DNN} tasks. More importantly, our method does not require extra hardware cost compared to the traditional two-column mapping scheme. The improvement is universal and does not require the optimization process for each individual device.",
      "publication_year": "",
      "keywords": "Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Performance",
      "doi": ""
    },
    "suggestion": {
      "relation": "This study proposes a method to improve fault tolerance in ReRAM-based DNNs through weight pruning and differential crossbar mapping, addressing hardware failure concerns.",
      "suggestion": "Use this paper to illustrate a specific approach to enhancing fault tolerance in hardware implementations of DNNs, particularly in the context of ReRAM technology.",
      "rating": 8
    },
    "sections": null
  },
  "garrett_improving_2021": {
    "base": {
      "article_type": "inproceedings",
      "title": "mproving Dependability of Onboard Deep Learning with Resilient {TensorFlow",
      "authors": [
        "Garrett, Tyler",
        "George, Alan D."
      ],
      "abstract": "As the dawn of a new age in space\ufb02ight approaches, the drive to equip future spacecraft with high-performance computing capabilities is increasing. Many within the industry are looking to leverage solutions enabled by machine learning ({ML}) and arti\ufb01cial intelligence to enhance mission ef\ufb01ciency. Tasks such as image processing and object tracking are desired for long-duration space\ufb02ight and extravehicular activities. In order to realize these applications in practice, enhancements to onboard processing are needed. {ML} applications require state-of-the-art processors and hardware accelerators, such as {GPUs}. However, {GPUs} are heavily susceptible to radiation-induced single-event effects ({SEEs}). Additionally, missions require a level of safetycriticality, which is unable to be met by existing commercialoff-the-shelf ({COTS}) {GPUs}. In an effort to create an end-to-end solution, this work aims to bridge {ML}-application development with device-architectural awareness to deliver a fault-aware implementation of the {TensorFlow} framework called Resilient {TensorFlow} ({RTF}). By building customized operations into the {TensorFlow} framework and employing them within the graph of various models, {RTF} demonstrates an ability to mask faults that occur during processing while minimizing overhead. Reducing faults during the processing of deep-learning applications brings the space computing industry closer to realizing onboard highperformance computing.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/SCC49971.2021.00021"
    },
    "suggestion": {
      "relation": "Focuses on enhancing the dependability of onboard deep learning systems through a fault-aware TensorFlow implementation, addressing hardware faults in GPUs due to radiation.",
      "suggestion": "Cite this work to discuss fault tolerance in the specific context of space computing and the adaptation of software frameworks to mitigate hardware faults.",
      "rating": 7
    },
    "sections": null
  },
  "zhan_improving_2022": {
    "base": {
      "article_type": "article",
      "title": "mproving Fault Tolerance for Reliable {DNN} Using Boundary-Aware Activatio",
      "authors": [
        "Zhan, Jinyu",
        "Sun, Ruoxu",
        "Jiang, Wei",
        "Jiang, Yucheng",
        "Yin, Xunzhao",
        "Zhuo, Cheng"
      ],
      "abstract": "In this article, we approach to construct reliable deep neural networks ({DNNs}) for safety-critical arti\ufb01cial intelligent applications. We propose to modify recti\ufb01ed linear unit ({ReLU}), a commonly used activation function in {DNNs}, to tolerate the faults incurred by bit-\ufb02ip perturbation on weights. Through theoretic analysis of the fault propagation in the layers with {ReLU} activation, we observe that bounding the output of {ReLU} activation can help to tolerate the weight faults. Then, we propose a novel {ReLU} design called boundary-aware {ReLU} ({BReLU}) to improve the reliability of {DNNs}, in which an upper bound of {ReLU} is determined such that the deviation between the boundary and original outputs cannot affect the \ufb01nal result. We propose a gradient-ascent-based algorithm to \ufb01nd the boundaries for {BReLU} activations of all {DNN} layers. Without retraining the network, our approach is cost effective and practical when deployed in safety-critical arti\ufb01cial intelligent systems. Detailed experiments and real-life application benchmarking demonstrate that our approach can improve the accuracy of {DNN} {VGG}16 from 16.7\\% to 82.6\\% on average assuming the practical weight faults, with only 13\\% memory and 2.78\\% time overhead, respectively.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TCAD.2021.3129114"
    },
    "suggestion": {
      "relation": "Proposes a novel activation function design to improve DNN fault tolerance against weight perturbations, indirectly related to hardware fault tolerance through its impact on computational reliability.",
      "suggestion": "Reference this paper when discussing computational strategies and activation function modifications for improving fault tolerance in DNNs.",
      "rating": 6
    },
    "sections": null
  },
  "jin_intermediate_2023": {
    "base": {
      "article_type": "article",
      "title": "ntermediate data fault-tolerant method of cloud computing accounting service platform supporting cost-benefit analysi",
      "authors": [
        "Jin, Taolan",
        "Zhang, Bo"
      ],
      "abstract": "This study mainly aims at the intermediate data fault-tolerant method of cloud computing accounting service platform supporting cost-benefit analysis, which aims at providing cost-benefit analysis function for the platform and strengthening the fault-tolerant ability of the intermediate data of the platform. The invention discloses a method for constructing an enterprise cloud computing accounting service platform. Collect the internal and external accounting service system data of an enterprise by using an accounting service platform network environment provided by a cloud service provider. Transmit the data to a data processing and storage layer data warehouse for storage after data cleaning, extraction and processing. Then, call the collected data through the data processing and storage layer to analyze the transaction cost-benefit of the enterprise. The intermediate data fault-tolerant model is constructed. After being solved by the ant colony algorithm, the intermediate data generated in the process of cost-benefit analysis and other accounting services are fault-tolerant processed. Finally, the platform accounting service results are output to the interactive interface through the data output display layer. The highest data availability probability of the method proposed in this study is 0. 98, which indicates that the method has high data availability after fault-tolerant processing, and can effectively realize the interaction with users. The experimental analysis shows that the method proposed in this study can effectively analyze the transaction costs and benefits of enterprises. The probability of data availability after fault-tolerant processing is higher, and a load of reading and writing is lower.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1186/s13677-022-00385-4"
    },
    "suggestion": {
      "relation": "This paper's focus on fault tolerance in cloud computing platforms for accounting services, though related to data reliability, diverges from the specific context of hardware fault tolerance in deep learning.",
      "suggestion": "It may be briefly mentioned in discussions on fault tolerance in cloud computing but is not directly relevant to the review's focus on hardware fault tolerance in deep learning.",
      "rating": 2
    },
    "sections": null
  },
  "ranjbar_learning-oriented_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "earning-Oriented Reliability Improvement of Computing Systems From Transistor to Application Leve",
      "authors": [
        "Ranjbar, Behnaz",
        "Klemme, Florian",
        "Genssler, Paul R.",
        "Amrouch, Hussam",
        "Jung, Jinhyo",
        "Dave, Shail",
        "So, Hwisoo",
        "Lee, Kyongwoo",
        "Shrivastava, Aviral",
        "Lin, Ji-Yung",
        "Weckx, Pieter",
        "Mishra, Subrat",
        "Catthoor, Francky",
        "Biswas, Dwaipayan",
        "Kumar, Akash"
      ],
      "abstract": "Due to technology scaling in modern computing platforms, the safety and reliability issues have increased tremendously, which often accelerate aging, lead to permanent faults, and cause unreliable execution of applications. Failure in some computing systems like avionics may cause catastrophic consequences. Therefore, managing reliability under all circumstances of stress and environmental changes is crucial in all abstraction layers, from application to transistor levels. Machine learning techniques are recently being employed for dynamic reliability estimation and optimization. They can adapt to varying workloads and system conditions. This paper presents reliability improvement approaches from multiple perspectives\u2014from transistor-level to application-level\u2014and discusses their effectiveness and limitations as well as open challenges.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.23919/DATE56975.2023.10137182"
    },
    "suggestion": {
      "relation": "Discusses reliability improvement from transistor to application level in computing systems using machine learning, relevant to understanding broader reliability challenges in hardware.",
      "suggestion": "Cite this paper for a comprehensive view on reliability improvement strategies across different levels of computing systems, including a mention of machine learning applications.",
      "rating": 5
    },
    "sections": null
  },
  "kempf_leveraging_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "everaging Adaptive Redundancy in Multi-Core Processors for Realizing Adaptive Fault Tolerance in Mixed-Criticality System",
      "authors": [
        "Kempf, Fabian",
        "Becker, Juergen"
      ],
      "abstract": "Nowadays, embedded systems are ubiquitous and their functionality is becoming increasingly intertwined with various critical demands. Integrating functionality with different criticality demands has led to the emergence of mixed-criticality systems ({MCS}), which require adaptive fault tolerance to ensure the correctness of critical tasks. In this paper, we propose a hardware-based adaptive redundancy approach for multi-core systems, which aims to enhance the reliability and safety of {MCS}. Our approach involves the reconfiguration of two physical processor cores into a single logical core that executes the same program on demand. The logical core provides adaptive redundancy to detect and mask faults. However, this reconfiguration can potentially result in deadlocks. To address this issue, we identify the scenarios where deadlocks may occur and provide a countermeasure to prevent their emergence. By adopting this runtime adaptive and hardware-based adaptive redundancy method, we can improve the reliability and safety of mixed-criticality systems. At the same time we utilize the processor architecture to abstract the reconfiguration process.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/MECO58584.2023.10154986"
    },
    "suggestion": {
      "relation": "This paper focuses on enhancing the reliability and safety of mixed-criticality systems through a hardware-based adaptive redundancy approach, which is relevant for ensuring fault tolerance in deep learning computations on multi-core processors.",
      "suggestion": "Cite this paper to discuss adaptive redundancy techniques in multi-core processors for fault tolerance in deep learning systems.",
      "rating": 7
    },
    "sections": null
  },
  "sun_lightning_2024": {
    "base": {
      "article_type": "article",
      "title": "ightning: Leveraging {DVFS}-induced Transient Fault Injection to Attack Deep Learning Accelerator of {GPUs",
      "authors": [
        "Sun, Rihui",
        "Qiu, Pengfei",
        "Lyu, Yongqiang",
        "Dong, Jian",
        "Wang, Haixia",
        "Wang, Dongsheng",
        "Qu, Gang"
      ],
      "abstract": "Graphics Processing Units ({GPU}) are widely used as deep learning accelerators because of its high performance and low power consumption. Additionally, it remains secure against hardware-induced transient fault injection attacks, a classic type of attacks that have been developed on other computing platforms. In this work, we demonstrate that well-trained machine learning models are robust against hardware fault injection attacks when the faults are generated randomly. However, we discover that these models have components, which we refer to as sensitive targets, that are vulnerable to faults. By exploiting this vulnerability, we propose the Lightning attack, which precisely strikes the model\u2019s sensitive targets with hardware-induced transient faults based on the Dynamic Voltage and Frequency Scaling ({DVFS}). We design a sensitive targets search algorithm to find the most critical processing units of Deep Neural Network ({DNN}) models determining the inference results, and develop a genetic algorithm to automatically optimize the attack parameters for {DVFS} to induce faults. Experiments on three commodity Nvidia {GPUs} for four widely-used {DNN} models show that the proposed Lightning attack can reduce the inference accuracy by 69.1\\% on average for non-targeted attacks, and, more interestingly, achieve a success rate of 67.9\\% for targeted attacks.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1145/3617893"
    },
    "suggestion": {
      "relation": "Although primarily focused on demonstrating a vulnerability in GPUs used for deep learning through fault injection, this paper indirectly highlights the importance of fault tolerance in protecting deep learning accelerators against targeted hardware faults.",
      "suggestion": "Reference this paper to illustrate the significance of fault tolerance in deep learning hardware against deliberate fault injection attacks.",
      "rating": 5
    },
    "sections": null
  },
  "agarwal_lltfi_2022": {
    "base": {
      "article_type": "inproceedings",
      "title": "LLTFI}: Framework Agnostic Fault Injection for Machine Learning Applications (Tools and Artifact Track",
      "authors": [
        "Agarwal, Udit Kumar",
        "Chan, Abraham",
        "Pattabiraman, Karthik"
      ],
      "abstract": "As machine learning ({ML}) has become more prevalent across many critical domains, so has the need to understand {ML} applications\u2019 resilience. While prior work like {TensorFI} [1], {MindFI} [2], and {PyTorchFI} [3] has focused on building {ML} fault injectors for speci\ufb01c {ML} frameworks, there has been little work on performing fault injection ({FI}) for {ML} applications written in multiple frameworks. We present {LLTFI}, a framework-agnostic fault injection tool for {ML} applications, allowing users to run {FI} experiments on {ML} applications at the {LLVM} {IR} level. {LLTFI} provides users with \ufb01ner {FI} granularity at the level of instructions, and a better understanding of how faults manifest and propagate between different {ML} components. We evaluate {LLTFI} on six {ML} programs and compare it with {TensorFI}. We found signi\ufb01cant differences in the Silent Data Corruption ({SDC}) rates for similar faults between the two tools. Finally, we use {LLTFI} to evaluate the ef\ufb01cacy of selective instruction duplication - an error mitigation technique - for {ML} programs.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/ISSRE55969.2022.00036"
    },
    "suggestion": {
      "relation": "This paper introduces a framework-agnostic fault injection tool for machine learning applications, which can be instrumental in evaluating and enhancing the fault tolerance of deep learning systems across different frameworks.",
      "suggestion": "Use this paper to discuss tools for assessing and improving fault tolerance in machine learning applications, including deep learning.",
      "rating": 6
    },
    "sections": null
  },
  "filippas_low-cost_2022": {
    "base": {
      "article_type": "article",
      "title": "ow-Cost Online Convolution Checksum Checke",
      "authors": [
        "Filippas, Dionysios",
        "Margomenos, Nikolaos",
        "Mitianoudis, Nikolaos",
        "Nicopoulos, Chrysostomos",
        "Dimitrakopoulos, Giorgos"
      ],
      "abstract": "Managing random hardware faults requires the faults to be detected online, thus simplifying recovery. Algorithm-based fault tolerance has been proposed as a low-cost mechanism to check online the result of computations against random hardware failures. In this case, the checksum of the actual result is checked against a predicted checksum computed in parallel by a hardware checker. In this work, we target the design of such checkers for convolution engines that are currently the most critical building block in image processing and computer vision applications. The proposed convolution checksum checker, named {ConvGuard}, utilizes a newly introduced invariance condition of convolution to predict implicitly the output checksum using only the pixels at the border of the input image. In this way, {ConvGuard} reduces the power required for accumulating the input pixels without requiring large buffers to hold intermediate checksum results. The design of {ConvGuard} is generic and can be con\ufb01gured for different output sizes and strides. The experimental results show that {ConvGuard} utilizes only a small percentage of the area/power of an ef\ufb01cient convolution engine while being signi\ufb01cantly smaller and more power ef\ufb01cient than a state-of-the-art checksum checker for various practical cases.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TVLSI.2021.3119511"
    },
    "suggestion": {
      "relation": "The paper presents a low-cost mechanism for detecting hardware faults in convolution engines, crucial for deep learning applications, thereby contributing to the discussion on hardware fault tolerance in deep learning computations.",
      "suggestion": "Cite this work to discuss cost-effective hardware fault detection methods in deep learning, particularly in convolution operations.",
      "rating": 8
    },
    "sections": null
  },
  "hsiao_mavfi_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "MAVFI}: An End-to-End Fault Analysis Framework with Anomaly Detection and Recovery for Micro Aerial Vehicle",
      "authors": [
        "Hsiao, Yu-Shun",
        "Wan, Zishen",
        "Jia, Tianyu",
        "Ghosal, Radhika",
        "Mahmoud, Abdulrahman",
        "Raychowdhury, Arijit",
        "Brooks, David",
        "Wei, Gu-Yeon",
        "Reddi, Vijay Janapa"
      ],
      "abstract": "Safety and resilience are critical for autonomous unmanned aerial vehicles ({UAVs}). We introduce {MAVFI}, the micro aerial vehicles ({MAVs}) resilience analysis methodology to assess the effect of silent data corruption ({SDC}) on {UAVs}\u2019 mission metrics, such as flight time and success rate, for accurately measuring system resilience. To enhance the safety and resilience of robot systems bound by size, weight, and power ({SWaP}), we offer two low-overhead anomaly-based {SDC} detection and recovery algorithms based on Gaussian statistical models and autoencoder neural networks. Our anomaly error protection techniques are validated in numerous simulated environments. We demonstrate that the autoencoder-based technique can recover up to all failure cases in our studied scenarios with a computational overhead of no more than 0.0062\\%. Our applicationaware resilience analysis framework, {MAVFI}, can be utilized to comprehensively test the resilience of other Robot Operating System ({ROS})-based applications and is publicly available at https://github.com/harvard-edge/{MAVBench}/tree/mavfi.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.23919/DATE56975.2023.10137246"
    },
    "suggestion": {
      "relation": "While focused on micro aerial vehicles, this paper's discussion on anomaly detection and recovery from silent data corruption offers insights into fault tolerance techniques that could be applied to deep learning systems.",
      "suggestion": "Reference this paper for examples of anomaly detection and recovery techniques in the context of fault tolerance for deep learning.",
      "rating": 4
    },
    "sections": null
  },
  "celis_methodology_2013": {
    "base": {
      "article_type": "inproceedings",
      "title": "ethodology for designing highly reliable Fault Tolerance Space Systems based on {COTS} device",
      "authors": [
        "Celis, Juan Andres Perez",
        "De La Rosa Nieves, Saul",
        "Fuentes, Carlos Romo",
        "Gutierrez, Saul Daniel Santillan",
        "Saenz-Otero, Alvar"
      ],
      "abstract": "The use of microsatellites for remote sensing missions is becoming more attractive. The cost-reliability issue is still a constraint. In this work we present a methodology for the development of Fault Tolerant Space Systems ({FTSS}) focusing on the On Board Data Handling ({OBDH}) system for microsatellite's remote sensing payloads with {COTS} components. The methodology presented is based on Fault Tolerance Techniques applied to Field Programmable Gate Arrays. The proposed methodology ends with a guideline for simulating the system reliability, providing statistics to support decisions regarding the necessary techniques to be implemented.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/SysCon.2013.6549942"
    },
    "suggestion": {
      "relation": "This paper presents a methodology for developing fault-tolerant space systems using COTS devices, which, although not directly related to deep learning, highlights general principles of fault tolerance that could be applicable.",
      "suggestion": "Mention this paper to provide a broader perspective on fault tolerance methodologies that could inspire approaches in deep learning systems.",
      "rating": 3
    },
    "sections": null
  },
  "ponader_milr_2021": {
    "base": {
      "article_type": "inproceedings",
      "title": "MILR}: Mathematically Induced Layer Recovery for Plaintext Space Error Correction of {CNNs",
      "authors": [
        "Ponader, Jonathan",
        "Thomas, Kyle",
        "Kundu, Sandip",
        "Solihin, Yan"
      ],
      "abstract": "The increased use of Convolutional Neural Networks ({CNN}) in mission-critical systems has increased the need for robust and resilient networks in the face of both naturally occurring faults as well as security attacks. The lack of robustness and resiliency can lead to unreliable inference results. Current methods that address {CNN} robustness require hardware modi\ufb01cation, network modi\ufb01cation, or network duplication. This paper proposes {MILR} a software-based {CNN} error detection and error correction system that enables recovery from single and multi-bit errors. The recovery capabilities are based on mathematical relationships between the inputs, outputs, and parameters(weights) of the layers; exploiting these relationships allows the recovery of erroneous parameters (weights) throughout a layer and the network. {MILR} is suitable for plaintext-space error correction ({PSEC}) given its ability to correct whole-weight and even whole-layer errors in {CNNs}.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/DSN48987.2021.00024"
    },
    "suggestion": {
      "relation": "Proposes a software-based error detection and correction system for CNNs, directly addressing the need for fault tolerance in deep learning without requiring hardware modifications.",
      "suggestion": "Cite this paper to discuss software-based fault tolerance solutions for deep learning, specifically in CNNs.",
      "rating": 9
    },
    "sections": null
  },
  "selg_ml-based_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "ML}-Based Online Design Error Localization for {RISC}-V Implementation",
      "authors": [
        "Selg, Hardi",
        "Jenihhin, Maksim",
        "Ellervee, Peeter",
        "Raik, Jaan"
      ],
      "abstract": "The accelerated growth of computing systems\u2019 complexity makes comprehensive design veri\ufb01cation challenging and time-consuming. In practice, hard-to-model complex environments are unfeasible to be simulated exhaustively within a reasonable time frame. Therefore, some corner-case conditions can be overlooked and design errors might escape to the \ufb01nal product. This means that it is imperative for the system to be able to detect and locate bugs to enable self-repair. This is particularly crucial during long-term remote missions in order to apply graceful degradation. This paper proposes a novel online design error localization methodology for microprocessors by immediate analysis of traced and buffered signals upon a failure detection event, using a pre-trained Neural Network ({NN}) and existing processor components, i.e. trace buffers and {AI} accelerators. An in-house Neural Architecture Search ({NAS}) framework is used to train a tailored Multi-Layer Perceptron ({MLP}) {NN} for error localization at the microprocessor module-level resolution. The proposed approach is validated by simulating a {RISC}-V implementation with different workload programs. It is demonstrated to be capable of localizing the microprocessor module of bug origin with 92.81\\% accuracy, on average.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/IOLTS59296.2023.10224864"
    },
    "suggestion": {
      "relation": "Focuses on design error localization in microprocessors using machine learning, which indirectly contributes to the broader discussion on fault tolerance and error correction in computing systems that support deep learning.",
      "suggestion": "Reference this paper to discuss the role of machine learning in identifying and correcting design errors in hardware that could affect deep learning computations.",
      "rating": 5
    },
    "sections": null
  },
  "liu_monitor_2024": {
    "base": {
      "article_type": "misc",
      "title": "onitor Placement for Fault Localization in Deep Neural Network Accelerator",
      "authors": [
        "Liu, Wei-Kai"
      ],
      "abstract": "Systolic arrays are a prominent choice for deep neural network ({DNN}) accelerators because they offer parallelism and efficient data reuse. Improving the reliability of {DNN} accelerators is crucial as hardware faults can degrade the accuracy of {DNN} inferencing. Systolic arrays make use of a large number of processing elements ({PEs}) for parallel processing, but when one {PE} is faulty, the error propagates and affects the outcomes of downstream {PEs}. Due to the large number of {PEs}, the cost associated with implementing hardware-based runtime monitoring of every single {PE} is infeasible. We present a solution to optimize the placement of hardware monitors within systolic arrays. We first prove that 2N \u2212 1 monitors are needed to localize a single faulty {PE} and we also derive the monitor placement. We show that a second placement optimization problem, which minimizes the set of candidate faulty {PEs} for a given number of monitors, is {NPhard}. Therefore, we propose a heuristic approach to balance the reliability and hardware resource utilization in {DNN} accelerators when number of monitors is limited. Experimental evaluation shows that to localize a single faulty {PE}, an area overhead of only 0.33\\% is incurred for a 256 \u00d7 256 systolic array.",
      "publication_year": "",
      "keywords": "Computer Science - Artificial Intelligence, Computer Science - Hardware Architecture",
      "doi": ""
    },
    "suggestion": {
      "relation": "This paper addresses the optimization of hardware monitor placement in systolic arrays for DNN accelerators to enhance fault localization, directly contributing to hardware fault tolerance in deep learning systems.",
      "suggestion": "Cite this paper to discuss methods for improving hardware fault tolerance in DNN accelerators through optimized monitor placement.",
      "rating": 8
    },
    "sections": null
  },
  "tao_new-sum_2016": {
    "base": {
      "article_type": "inproceedings",
      "title": "ew-Sum: A Novel Online {ABFT} Scheme For General Iterative Method",
      "authors": [
        "Tao, Dingwen",
        "Song, Shuaiwen Leon",
        "Krishnamoorthy, Sriram",
        "Wu, Panruo",
        "Liang, Xin",
        "Zhang, Eddy Z.",
        "Kerbyson, Darren",
        "Chen, Zizhong"
      ],
      "abstract": "Emerging high-performance computing platforms, with large component counts and lower power margins, are anticipated to be more susceptible to soft errors in both logic circuits and memory subsystems. We present an online algorithm-based fault tolerance ({ABFT}) approach to e\ufb03ciently detect and recover soft errors for general iterative methods. We design a novel checksum-based encoding scheme for matrix-vector multiplication that is resilient to both arithmetic and memory errors. Our design decouples the checksum updating process from the actual computation, and allows adaptive checksum overhead control. Building on this new encoding mechanism, we propose two online {ABFT} designs that can e\ufb00ectively recover from errors when combined with a checkpoint/rollback scheme. These designs are capable of addressing scenarios under di\ufb00erent error rates. Our {ABFT} approaches apply to a wide range of iterative solvers that primarily rely on matrix-vector multiplication and vector linear operations. We evaluate our designs through comprehensive analytical and empirical analysis. Experimental evaluation on the Stampede supercomputer demonstrates the low performance overheads incurred by our two {ABFT} schemes for preconditioned {CG} (0.4\\% and 2.2\\%) and preconditioned {BiCGSTAB} (1.0\\% and 4.0\\%) for the largest {SPD} matrix from {UFL} Sparse Matrix Collection. The evaluation also demonstrates the \ufb02exibility and e\ufb00ectiveness of our proposed designs for detecting and recovering various types of soft errors in general iterative methods.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1145/2907294.2907306"
    },
    "suggestion": {
      "relation": "The paper introduces an online ABFT scheme for detecting and recovering soft errors in high-performance computing, relevant for ensuring computational accuracy in deep learning under hardware faults.",
      "suggestion": "Reference this paper for examples of online fault tolerance techniques that can be applied to deep learning computations.",
      "rating": 7
    },
    "sections": null
  },
  "jang_oobleck_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "obleck: Resilient Distributed Training of Large Models Using Pipeline Template",
      "authors": [
        "Jang, Insu",
        "Yang, Zhenning",
        "Zhang, Zhen",
        "Jin, Xin",
        "Chowdhury, Mosharaf"
      ],
      "abstract": "Oobleck enables resilient distributed training of large {DNN} models with guaranteed fault tolerance. It takes a planningexecution co-design approach, where it first generates a set of heterogeneous pipeline templates and instantiates at least \ud835\udc53 + 1 logically equivalent pipeline replicas to tolerate any \ud835\udc53 simultaneous failures. During execution, it relies on alreadyreplicated model states across the replicas to provide fast recovery. Oobleck provably guarantees that some combination of the initially created pipeline templates can be used to cover all available resources after \ud835\udc53 or fewer simultaneous failures, thereby avoiding resource idling at all times. Evaluation on large {DNN} models with billions of parameters shows that Oobleck provides consistently high throughput, and it outperforms state-of-the-art fault tolerance solutions like Bamboo and Varuna by up to 13.9\u00d7.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1145/3600006.3613152"
    },
    "suggestion": {
      "relation": "Oobleck focuses on fault tolerance in distributed training of DNNs, which, while related to deep learning, emphasizes distributed system resilience more than hardware fault tolerance.",
      "suggestion": "Mention this paper when discussing fault tolerance in distributed deep learning systems, noting the difference from hardware-centric approaches.",
      "rating": 4
    },
    "sections": null
  },
  "mansoor_optimized_2023": {
    "base": {
      "article_type": "article",
      "title": "ptimized reverse converters with multibit soft error correction support at 7nm technolog",
      "authors": [
        "Mansoor, Ali",
        "Fazeli, Mahdi",
        "Masoud Rahmani, Amir",
        "Reshadi, Midia"
      ],
      "abstract": "Residue number system ({RNS}) speeds up digital signal processing systems involving dominant addition and multiplication. Addition and multiplication are accelerated further and performed with a balanced performance on one-hot coded ({OHC}) residue digits. However, the high complexity of the {RNS} reverse converter ({RC}) may kill the performance gain. This paper proposes a high-speed and scalable {RNS}-{RC} for both regular and one-hot {RNS}. For redundant {RNS} ({RRNS}), an {RRNS}-{RC} is proposed, which based on majority-voting between {OHC} residue digits, corrects multibit soft errors occurring in a single residue channel. With pass-transistor logic and lowpower {FinFETs}, the proposed {RCs} are optimized. The simulated {RRNS}-{RC} corrected 98.5\\% of soft errors, while consuming 7, 6.2 and 0.4\\% of the system\u2019s area, leakage power and dynamic power, respectively. As compared to the leading lookup table {RC} in the literature, {RNS}-{RC} exhibited 10.3X, 4.4X and 1.8X savings in area, average power, and delay, respectively.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.compeleceng.2023.108654"
    },
    "suggestion": {
      "relation": "This paper proposes a solution for correcting soft errors in digital signal processing systems, which can be applied to deep learning hardware accelerators for improved fault tolerance.",
      "suggestion": "Include this paper to discuss error correction techniques in hardware that can enhance the reliability of deep learning computations.",
      "rating": 6
    },
    "sections": null
  },
  "tuli_pregan_2024": {
    "base": {
      "article_type": "article",
      "title": "PreGAN}+: Semi-Supervised Fault Prediction and Preemptive Migration in Dynamic Mobile Edge Environment",
      "authors": [
        "Tuli, Shreshth",
        "Casale, Giuliano",
        "Jennings, Nicholas R."
      ],
      "abstract": "Typical mobile edge computing infrastructures have to contend with unreliable computing devices at their end-points. The limited resource capacities of mobile edge devices gives rise to frequent contentions, node overloads or failures. This is exacerbated by the strict deadlines of modern applications. To avoid failures, fault-tolerant approaches utilize preemptive migration to transfer active tasks across nodes and prevent nodes running at capacity. However, prior work struggles to dynamically adapt in settings with highly volatile workloads or even accurately detect and diagnose anomalies for optimal remediation. To meet the strict service level objectives of contemporary workloads, there is a need for dynamic fault-tolerant methods that can quickly adapt to changes in edge environments while having parsimonious remediation in the form of preemptive migration to avoid stressing the system network. This work proposes {PreGAN}, featuring a Generative Adversarial Network ({GAN}) based approach to predict contentions, pinpoint specific resource types with high chance of overload, and generate migration decisions to proactively avoid system downtime. {PreGAN} leverages coupled-simulations to train the {GAN} model at run-time and a few-shot fault classifier to update decisions of an underpinning scheduler. We also extend it to {PreGAN}+ that also periodically tunes the decision model using semi-supervised training and a Transformer based neural network for low tuning time, albeit with higher memory overheads. Experiments on a Raspberry-Pi based edge environment demonstrate that both models outperform state-of-the-art baselines in fault detection and diagnosis scores by up to 12.5\\% and 31.2\\% respectively. This also translates in improvements in Quality of Service against baseline approaches.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TMC.2023.3330679"
    },
    "suggestion": {
      "relation": "Although focused on fault prediction and preemptive migration in mobile edge computing, this paper's relevance to hardware error fault tolerance in deep learning is tangential.",
      "suggestion": "Briefly mention this paper in a discussion on dynamic fault-tolerant methods in edge computing environments, differentiating from core hardware fault tolerance in deep learning.",
      "rating": 3
    },
    "sections": null
  },
  "chen_pruning_2021": {
    "base": {
      "article_type": "inproceedings",
      "title": "runing of Deep Neural Networks for Fault-Tolerant Memristor-based Accelerator",
      "authors": [
        "Chen, Ching-Yuan",
        "Chakrabarty, Krishnendu"
      ],
      "abstract": "Hardware-level reliability is a major concern when deep neural network ({DNN}) models are mapped to neuromorphic accelerators such as memristor-based crossbars. Manufacturing defects and variations lead to hardware faults in the crossbar. Although memristor-based {DNNs} are inherently tolerant to these faults and many faults are benign for a given inferencing application, there is still a non-negligible number of critical faults ({CFs}) in the memristor crossbars that can lead to misclassi\ufb01cation. It is therefore important to ef\ufb01ciently identify these {CFs} so that fault-tolerance solutions can focus on them. In this paper, we present an ef\ufb01cient technique based on machine learning to identify these {CFs}; {CFs} can be identi\ufb01ed with over 98\\% accuracy and at a rate that is 20 times faster than a baseline using random fault injection. We next present a fault-tolerance technique that iteratively prunes a {DNN} by targeting weights that are mapped to {CFs} in the memristor crossbars. Our results for the {CIFAR}-10 data set and several benchmark {DNNs} show that the proposed pruning technique eliminates up to 95\\% of the {CFs} with less than 1\\% {DNN} inferencing accuracy loss. This reduction in the total number of {CFs} leads to a 99\\% savings in the hardware redundancy required for fault tolerance.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/DAC18074.2021.9586269"
    },
    "suggestion": {
      "relation": "This paper directly addresses hardware-level reliability in DNN models on memristor-based accelerators, presenting a technique for identifying and mitigating critical faults.",
      "suggestion": "Cite this paper to discuss specific fault-tolerance techniques for memristor-based deep learning accelerators.",
      "rating": 9
    },
    "sections": null
  },
  "nie_fault_2018": {
    "base": {
      "article_type": "inproceedings",
      "title": "ault Site Pruning for Practical Reliability Analysis of {GPGPU} Application",
      "authors": [
        "Nie, Bin",
        "Yang, Lishan",
        "Jog, Adwait",
        "Smirni, Evgenia"
      ],
      "abstract": "Graphics Processing Units ({GPUs}) have rapidly evolved to enable energy-ef\ufb01cient data-parallel computing for a broad range of scienti\ufb01c areas. While {GPUs} achieve exascale performance at a stringent power budget, they are also susceptible to soft errors, often caused by high-energy particle strikes, that can signi\ufb01cantly affect the application output quality. Understanding the resilience of general purpose {GPU} applications is the purpose of this study. To this end, it is imperative to explore the range of application output by injecting faults at all the potential fault sites. This problem is especially challenging because unlike {CPU} applications, which are mostly single-threaded, {GPGPU} applications can contain hundreds to thousands of threads, resulting in a tremendously large fault site space \u2013 in the order of billions even for some simple applications.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/MICRO.2018.00066"
    },
    "suggestion": {
      "relation": "The study explores the resilience of GPU applications to soft errors, which indirectly contributes to understanding hardware fault tolerance in deep learning, given GPUs' role in DNN training.",
      "suggestion": "Reference this paper for insights into GPU resilience and soft error impacts, contextualizing within deep learning hardware fault tolerance discussions.",
      "rating": 5
    },
    "sections": null
  },
  "xu_r2f_2021": {
    "base": {
      "article_type": "misc",
      "title": "2F: A Remote Retraining Framework for {AIoT} Processors with Computing Error",
      "authors": [
        "Xu, Dawen",
        "He, Meng",
        "Liu, Cheng",
        "Wang, Ying",
        "Cheng, Long",
        "Li, Huawei",
        "Li, Xiaowei",
        "Cheng, Kwang-Ting"
      ],
      "abstract": "{AIoT} processors fabricated with newer technology nodes suffer rising soft errors due to the shrinking transistor sizes and lower power supply. Soft errors on the {AIoT} processors particularly the deep learning accelerators ({DLAs}) with massive computing may cause substantial computing errors. These computing errors are dif\ufb01cult to be captured by the conventional training on general purposed processors like {CPUs} and {GPUs} in a server. Applying the of\ufb02ine trained neural network models to the edge accelerators with errors directly may lead to considerable prediction accuracy loss.",
      "publication_year": "",
      "keywords": "Computer Science - Artificial Intelligence, Computer Science - Hardware Architecture",
      "doi": ""
    },
    "suggestion": {
      "relation": "It discusses the challenge of soft errors in AIoT processors, especially in DLAs, and the need for retraining frameworks to maintain accuracy, highlighting issues in hardware fault tolerance.",
      "suggestion": "Use this paper to illustrate the impact of soft errors on deep learning accelerators and the importance of retraining as a fault tolerance strategy.",
      "rating": 7
    },
    "sections": null
  },
  "zhang_realizing_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "ealizing Extreme Endurance Through Fault-aware Wear Leveling and Improved Toleranc",
      "authors": [
        "Zhang, Jiangwei",
        "Wang, Chong",
        "Zhu, Zhenhua",
        "Kline, Donald",
        "Jones, Alex K.",
        "Yang, Huazhong",
        "Wang, Yu"
      ],
      "abstract": "Phase-change memory ({PCM}) and resistive memory ({RRAM}) are promising alternatives to traditional memory technologies. However, both {PCM} and {RRAM} suffer from limited write endurance. Wear-leveling ({WL}) techniques are essential to extend the lifetime of these memories before experiencing endurance faults. Beyond the additional usage afforded by {WL}, row-sparing and focused error correction can extend the lifetime further after wear faults appear. Unfortunately, the need for extended {WL} techniques continues to become more pressing as scaling exacerbates process variation. Similarly, scaling causes challenges such as more severe noise and crosstalk to traditional {DRAM}.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/HPCA56546.2023.10071093"
    },
    "suggestion": {
      "relation": "This paper discusses enhancing the endurance of phase-change and resistive memories through wear-leveling techniques and error correction, which are crucial for maintaining the reliability of hardware used in deep learning computations.",
      "suggestion": "Cite this paper to discuss the importance of wear-leveling and error correction techniques in extending the lifetime of memory technologies crucial for deep learning.",
      "rating": 7
    },
    "sections": null
  },
  "battistel_real-time_2023": {
    "base": {
      "article_type": "article",
      "title": "eal-Time Decoding for Fault-Tolerant Quantum Computing: Progress, Challenges and Outloo",
      "authors": [
        "Battistel, Francesco",
        "Chamberland, Christopher",
        "Johar, Kauser",
        "Overwater, Ramon W. J.",
        "Sebastiano, Fabio",
        "Skoric, Luka",
        "Ueno, Yosuke",
        "Usman, Muhammad"
      ],
      "abstract": "Quantum computing is poised to solve practically useful problems which are computationally intractable for classical supercomputers. However, the current generation of quantum computers are limited by errors that may only partially be mitigated by developing higher-quality qubits. Quantum error correction ({QEC}) will thus be necessary to ensure fault tolerance. {QEC} protects the logical information by cyclically measuring syndrome information about the errors. An essential part of {QEC} is the decoder, which uses the syndrome to compute the likely effect of the errors on the logical degrees of freedom and provide a tentative correction. The decoder must be accurate, fast enough to keep pace with the {QEC} cycle (e.g., on a microsecond timescale for superconducting qubits) and with hard real-time system integration to support logical operations. As such, real-time decoding is essential to realize fault-tolerant quantum computing and to achieve quantum advantage. In this work, we highlight some of the key challenges facing the implementation of real-time decoders while providing a succinct summary of the progress to-date. Furthermore, we lay out our perspective for the future development and provide a possible roadmap for the field of real-time decoding in the next few years. As the quantum hardware is anticipated to scale up, this perspective article will provide a guidance for researchers, focusing on the most pressing issues in real-time decoding and facilitating the development of solutions across quantum and computer science.",
      "publication_year": "",
      "keywords": "Quantum Physics",
      "doi": "10.1088/2399-1984/aceba6"
    },
    "suggestion": {
      "relation": "Although focused on quantum computing, this paper's exploration of quantum error correction and real-time decoding for fault tolerance shares conceptual parallels with fault tolerance in deep learning hardware.",
      "suggestion": "Reference this paper for a broader perspective on fault tolerance, highlighting parallels between quantum computing and deep learning hardware fault tolerance strategies.",
      "rating": 4
    },
    "sections": null
  },
  "yin_real-time_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "eal-Time Fault-Tolerant Computing with Machine Learning Enhancement",
      "authors": [
        "Yin, Meng-Lai",
        "Aroush, Hovig"
      ],
      "abstract": "{CONCLUSIONS} Machine learning enhancements applied to fault-tolerant computing are presented here. Classification of non-identical inputs and real-time environmental threats to fault-tolerant systems are handled using machine learning techniques. Efficient learning methods are developed so that they are suitable for real-time applications. The advantages of the enhancements are demonstrated through case studies. This work shows that not only the performance of machine learning can be improved by fault tolerance, but also fault-tolerant computing can benefit from machine learning.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/RAMS51473.2023.10088209"
    },
    "suggestion": {
      "relation": "This paper presents machine learning enhancements for fault-tolerant computing, indirectly relevant to deep learning by showing how machine learning can improve fault tolerance mechanisms.",
      "suggestion": "Use this paper to illustrate the potential of machine learning enhancements in improving fault tolerance in computing systems, including those used for deep learning.",
      "rating": 5
    },
    "sections": null
  },
  "marchisio_red-cane_2020": {
    "base": {
      "article_type": "inproceedings",
      "title": "ReD}-{CaNe}: A Systematic Methodology for Resilience Analysis and Design of Capsule Networks under Approximation",
      "authors": [
        "Marchisio, Alberto",
        "Mrazek, Vojtech",
        "Hanif, Muhammad Abudllah",
        "Shafique, Muhammad"
      ],
      "abstract": "Recent advances in Capsule Networks ({CapsNets}) have shown their superior learning capability, compared to the traditional Convolutional Neural Networks ({CNNs}). However, the extremely high complexity of {CapsNets} limits their fast deployment in real-world applications. Moreover, while the resilience of {CNNs} have been extensively investigated to enable their energy-efficient implementations, the analysis of {CapsNets}\u2019 resilience is a largely unexplored area, that can provide a strong foundation to investigate techniques to overcome the {CapsNets}\u2019 complexity challenge. Following the trend of Approximate Computing to enable energyefficient designs, we perform an extensive resilience analysis of the {CapsNets} inference subjected to the approximation errors. Our methodology models the errors arising from the approximate components (like multipliers), and analyze their impact on the classification accuracy of {CapsNets}. This enables the selection of approximate components based on the resilience of each operation of the {CapsNet} inference. We modify the {TensorFlow} framework to simulate the injection of approximation noise (based on the models of the approximate components) at different computational operations of the {CapsNet} inference. Our results show that the {CapsNets} are more resilient to the errors injected in the computations that occur during the dynamic routing (the softmax and the update of the coefficients), rather than other stages like convolutions and activation functions. Our analysis is extremely useful towards designing efficient {CapsNet} hardware accelerators with approximate components. To the best of our knowledge, this is the first proof-of-concept for employing approximations on the specialized {CapsNet} hardware.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.23919/DATE48585.2020.9116393"
    },
    "suggestion": {
      "relation": "It explores the resilience of Capsule Networks to approximation errors, relevant for designing fault-tolerant deep learning hardware that can handle computational inaccuracies.",
      "suggestion": "Cite this paper to discuss the resilience of deep learning architectures, like Capsule Networks, to hardware approximation errors and their implications for fault tolerance.",
      "rating": 8
    },
    "sections": null
  },
  "deveautour_reducing_2021": {
    "base": {
      "article_type": "inproceedings",
      "title": "educing Overprovision of Triple Modular Reduncancy Owing to Approximate Computin",
      "authors": [
        "Deveautour, Bastien",
        "Traiola, Marcello",
        "Virazel, Arnaud",
        "Girard, Patrick"
      ],
      "abstract": "Until recently, Approximate Computing ({AxC}) was considered to be a trend topic mainly for resilient applications. Its use aimed at reducing area and power consumption of Integrated Circuits ({ICs}) at the cost of a reduced accuracy. Beside, {AxC}-based fault-tolerance has also emerged recently to save area et power consumption w.r.t. conventional fault-tolerance at the cost of a reduced reliability level. Therefore, approximate fault-tolerance is restricted to non-critical applications. In a previous work, a Quadruple Approximate Modular Redundancy ({QAMR}) scheme, based on using four approximate circuit copies, was proposed. In a single fault scenario, it provides the same reliability level as Triple Modular Redundancy ({TMR}). Moreover, {QAMR} allows reducing area and power consumption costs w.r.t. {TMR} in many cases. In this paper, we study the occurrence of multiple faults, and compare the {QAMR} and {TMR} behaviors. Experimental results highlight the {TMR} overprovision (i.e. it provides more redundancy than necessary) and show how the {QAMR} approach can reduce it. Moreover, a thorough analysis of the results shows that a high tolerance to multiple faults is associated to a large circuit area and to a lower percentage of logic shared among different fan-in cones of the circuit outputs. Index Terms\u2014Approximate computing; fault tolerance; modular redundancy; fault injection.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/IOLTS52814.2021.9486699"
    },
    "suggestion": {
      "relation": "Discusses the use of Approximate Computing for fault tolerance in non-critical applications, which can be related to reducing hardware redundancy in deep learning systems.",
      "suggestion": "Reference this paper to explore the trade-offs between fault tolerance and hardware efficiency in deep learning systems through Approximate Computing.",
      "rating": 6
    },
    "sections": null
  },
  "radu_reliability_2014": {
    "base": {
      "article_type": "inproceedings",
      "title": "eliability and fault tolerance analysis of {FPGA} platform",
      "authors": [
        "Radu, Mihaela"
      ],
      "abstract": "This paper presents reliability and fault tolerance analyses of {FPGA} platforms. {FPGA} stands for Field Programmable Gate Arrays. It is a digital technology designed to be configured by a customer or a designer after manufacturinghence \"field-programmable\u201d. The {FPGA} configuration is generally specified using a Hardware Description Language ({HDL}), similar to that used for an Application-Specific integrated Circuit ({ASIC}). The {FPGA} platforms that are investigated are Digilent Nexys digital design platforms, built around Xilinx {FPGA} technology. They are ideal platforms for any engineer to gain experience with Xilinx's latest technologies, and are perfectly suited to the classroom. These platforms are intensively used in learning environments, such as colleges and universities world-wide, so they need to be reliable and robust, surviving intensive use over the years. Advanced reliability estimations for these {FPGA} platforms are presented, using various modules and prediction methods of {CARE}-{CAD} tool, a powerful software tool for reliability analysis (Mean Time Between Failures Module, Fault Tree Analysis Module, Reliability Block Diagram Module). After various reliability estimations are performed considering every functional block of the {FPGA} platforms, fault tolerance analyses are performed. Fault tolerant techniques, based on hardware redundancy, are suggested for the less reliable blocks of the platforms. New reliability analyses are performed to see if the addition of redundant blocks and components is justified, improving the reliability of the platforms for short and long mission times. Estimating the reliability of various blocks of the {FPGA} platforms help the designer to make the necessary changes, designing more robust products.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/LISAT.2014.6845211"
    },
    "suggestion": {
      "relation": "This paper provides a detailed analysis of FPGA platform reliability and fault tolerance, directly relevant to ensuring the reliability of hardware platforms for deep learning.",
      "suggestion": "Cite this paper to discuss the importance of hardware reliability and fault tolerance techniques in FPGA platforms used for deep learning.",
      "rating": 7
    },
    "sections": null
  },
  "id_reliability_2023": {
    "base": {
      "article_type": "article",
      "title": "eliability evaluation of Convolutional Neural Network's basic operations on a {RISC}-V processo",
      "authors": [
        "Id, {HAL}"
      ],
      "abstract": "Thanks to {RISC}-V open-source Instruction Set Architecture, researchers and developers can efficiently propose new solutions at a low-cost and low-power consumption. {RISC}-Vbased architectures can, then, be customized to run Machine Learning ({ML}) algorithms efficiently and be inserted on safety and mission-critical domains, where the execution must be reliable. However, a fault in the hardware resources can compromise the system\u2019s ability to operate correctly. Thus, it is necessary to characterize the {ML} applications\u2019 vulnerabilities on {RISCV} processors, as it is not as thoroughly characterized as other accelerators. This work is the first to evaluate the neutroninduced error rate of Convolutional Neural Network ({CNN}) basic operations running on a {RISC}-V processor, {GAP}8. Our results show that executing the algorithm in parallel increases performance, and memory errors are the major contributors to the device error rate. We show that the error rate of the {GAP}8 unprotected memories is one order of magnitude higher than the {CNN} basic operations.",
      "publication_year": "",
      "keywords": "",
      "doi": ""
    },
    "suggestion": {
      "relation": "Evaluates the reliability of Convolutional Neural Network operations on RISC-V processors, directly relevant to understanding hardware vulnerabilities in deep learning computations.",
      "suggestion": "Use this paper to discuss the reliability evaluation of hardware platforms, like RISC-V processors, running deep learning algorithms.",
      "rating": 8
    },
    "sections": null
  },
  "guerrero-balaguera_reliability_2022": {
    "base": {
      "article_type": "inproceedings",
      "title": "eliability Assessment of Neural Networks in {GPUs}: A Framework For Permanent Faults Injection",
      "authors": [
        "Guerrero-Balaguera, Juan-David",
        "Galasso, Luigi",
        "Sierra, Robert Limas",
        "Reorda, Matteo Sonza"
      ],
      "abstract": "Currently, Deep learning and especially Convolutional Neural Networks ({CNNs}) have become a fundamental computational approach applied in a wide range of domains, including some safety-critical applications (e.g., automotive, robotics, and healthcare equipment). Therefore, the reliability evaluation of those computational systems is mandatory. The reliability evaluation of {CNNs} is performed by fault injection campaigns at different levels of abstraction, from the application level down to the hardware level. Many works have focused on evaluating the reliability of neural networks in the presence of transient faults. However, the effects of permanent faults have been investigated at the application level, only, e.g., targeting the parameters of the network. This paper intends to propose a framework, resorting to a binary instrumentation tool to perform fault injection campaigns, targeting different components inside the {GPU}, such as the register files and the functional units. This environment allows for the first time assessing the reliability of {CNNs} deployed on a {GPU} considering the presence of permanent faults.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/ISIE51582.2022.9831549"
    },
    "suggestion": {
      "relation": "Focuses on assessing the reliability of neural networks in GPUs considering permanent faults, directly relevant to evaluating hardware fault tolerance in deep learning applications.",
      "suggestion": "Reference this paper for insights into the reliability assessment of GPUs used in deep learning, especially in the context of permanent faults.",
      "rating": 9
    },
    "sections": null
  },
  "fernandes_dos_santos_reliability_2019": {
    "base": {
      "article_type": "inproceedings",
      "title": "eliability Evaluation of Mixed-Precision Architecture",
      "authors": [
        "Fernandes Dos Santos, Fernando",
        "Lunardi, Caio",
        "Oliveira, Daniel",
        "Libano, Fabiano",
        "Rech, Paolo"
      ],
      "abstract": "Novel computing architectures offer the possibility to execute float point operations with different precisions. The execution of reduced precision operations, when acceptable for certain applications, is likely to reduce both the execution time and the power consumption. However, the application's error rate and the device's reliability can also be impacted by these precision changes. In this paper, we study the impact of data and operation precision changes on the reliability of modern architectures. We consider Xilinx Field-Programmable Gate-Arrays ({FPGA}), Intel Xeon Phis, and {NVIDIA} Graphics Processing Units ({GPUs}) executing a set of codes implemented in double, single, and half-precision {IEEE}754-compliant float point data. On {FPGAs}, the reduced area and performance improvements brought by reduced precision operations increase reliability. On Xeon Phis the compiler biases significantly double and single-precision instructions execution. This raises the drawback of increasing single-precision error rates when compared to double-precision operations. {NVIDIA} {GPUs} make use of dedicated mixed-precision cores, which draw nontrivial effects on the device reliability. Generically speaking, on {GPUs} half-precision allows a higher number of executions to be correctly completed before experimenting a failure. Finally, we also evaluate how transient faults impact the output correctness. Our study shows that for most applications faults in a single or half-precision data or operation are more likely to significantly modify the output value than errors in double-precision data.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/HPCA.2019.00041"
    },
    "suggestion": {
      "relation": "This paper investigates the reliability of computing architectures, including FPGAs, GPUs, and CPUs, in the context of mixed-precision operations, which is crucial for deep learning applications that can tolerate precision variability.",
      "suggestion": "Cite this paper to discuss the impact of mixed-precision computing on hardware reliability in deep learning systems.",
      "rating": 7
    },
    "sections": null
  },
  "cheng_reliability_2023": {
    "base": {
      "article_type": "article",
      "title": "eliability Exploration of System-on-Chip With Multi-Bit-Width Accelerator for Multi-Precision Deep Neural Network",
      "authors": [
        "Cheng, Quan",
        "Huang, Mingqiang",
        "Man, Changhai",
        "Shen, Ao",
        "Dai, Liuyao",
        "Yu, Hao",
        "Hashimoto, Masanori"
      ],
      "abstract": "Deep neural networks ({DNNs}) in safety-critical applications demand high reliability even when running on edge-computing devices. Recent works on System-on-Chip ({SoC}) design with state-of-the-art ({SOTA}) hardware artificial intelligence ({AI}) accelerators and corresponding multi-bit-width ({MBW}) convolutional neural network ({CNN}) generation strategies show that {MBW} {CNNs} can effectively explore the trade-off between network accuracy and hardware efficiency. However, reliability has not been considered in such trade-off analysis, even though highly quantized {CNNs} may elevate the impact of bit flips in the hardware. Also, the reliability of the microcontroller and its interface operating with the {AI} accelerator are not studied. This work evaluates the reliability of {DNN} computation in an {SoC} that includes a processor, {SOTA} {AI} accelerator, and {NN} models highly optimized for computation efficiency using a neural architecture search ({NAS}) method. Focusing on neutron-induced soft error, which is the primary source of bit-flip errors in a terrestrial environment, we perform fault injection and neutron beam experiments. For these experiments, we prototype the {SoC} on a flash-based {FPGA} platform, in which the configuration memory is robust to neutron irradiation. Then, we analyze the experimental data and identify vulnerable components in the system. Furthermore, we evaluate how the {SoC} running different {NAS}-optimized {MBW} {LeNet}5 networks impact the performance, radiation sensitivity, failure rate of {MBW} accelerator, and crash rate of the system on the {FPGAs}. Our results show that instruction and data tightly coupled memory (I/{DTCM}) are the most vulnerable parts and the control status registers ({CSRs}) in our accelerator are the second most vulnerable component. Moreover, {MBW} networks have higher susceptibility to critical errors than single-precision networks, low-precision data are more likely to affect the classification results, and the high bits are more sensitive to faults.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TCSI.2023.3300899"
    },
    "suggestion": {
      "relation": "It evaluates the reliability of DNN computations on SoCs with AI accelerators and MBW CNNs, focusing on fault tolerance against neutron-induced soft errors, directly relevant to hardware fault tolerance in deep learning.",
      "suggestion": "Use this paper to highlight the importance of considering hardware reliability and fault tolerance in the design of SoC for deep learning applications.",
      "rating": 8
    },
    "sections": null
  },
  "tosun_reliability-centric_2005": {
    "base": {
      "article_type": "inproceedings",
      "title": "eliability-Centric Hardware/Software Co-Desig",
      "authors": [
        "Tosun, S.",
        "Mansouri, N.",
        "Arvas, E.",
        "Kandemir, M.",
        "Xie, Y.",
        "Hung, W-L."
      ],
      "abstract": "This paper proposes a reliability-centric hardware/ software co-design framework. This framework operates with a component library that provides multiple alternates for a given task, each of which is potentially different from the others in terms of reliability, performance, and area metrics. The paper also presents an experimental evaluation of the proposed co-design framework using several example designs and a comparison to a conventional co-design method that does not consider reliability. Our experimental evaluation demonstrates that the proposed framework can be used to study the tradeoffs between area, performance, and reliability, and that it is important to include reliability as a first class parameter in optimization.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/ISQED.2005.104"
    },
    "suggestion": {
      "relation": "This paper presents a framework for hardware/software co-design with a focus on reliability, which indirectly supports the development of fault-tolerant deep learning systems by optimizing for reliability.",
      "suggestion": "Reference this paper for a broader perspective on the importance of reliability in hardware/software co-design, which can be applied to deep learning systems.",
      "rating": 5
    },
    "sections": null
  },
  "antunes_tambara_reliabilityperformance_2018": {
    "base": {
      "article_type": "article",
      "title": "eliability\u2013Performance Analysis of Hardware and Software Co-Designs in {SRAM}-Based {APSoCs",
      "authors": [
        "Antunes Tambara, Lucas",
        "Kastensmidt, Fernanda Lima",
        "Rech, Paolo",
        "Lins, Filipe",
        "Medina, Nilberto H.",
        "Added, Nemitala",
        "Aguiar, Vitor A. P.",
        "Silveira, Marcilei A. G."
      ],
      "abstract": "All programmable system-on-chip ({APSoC}) devices provide higher system performance and programmable \ufb02exibility at lower costs compared to standalone \ufb01eld-programmable gate array devices and processors. Unfortunately, it has been demonstrated that the high complexity and density of {APSoCs} increase the system\u2019s susceptibility to radiation-induced errors. This paper investigates the effects of soft errors on {APSoCs} at design level through reliability and performance analyses. We explore 28 different hardware and software co-designs varying the workload distribution between hardware and software. We also propose a reliability analysis \ufb02ow based on fault injection ({FI}) to estimate the reliability trend of hardware-only and software-only designs and hardware\u2013software co-designs. Results obtained from both radiation experiments and {FI} campaigns reveal that performance and reliability can be improved up to 117\u00d7 by of\ufb02oading the workload of an {APSoC}-based system to its programmable logic core. We also show that the proposed \ufb02ow is a precise method to estimate the reliability trend of system designs on {APSoCs} before radiation experiments.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TNS.2018.2844250"
    },
    "suggestion": {
      "relation": "Investigates the impact of soft errors on the reliability and performance of APSoCs, which are increasingly used for deep learning applications, emphasizing the need for fault tolerance in such systems.",
      "suggestion": "Mention this study to discuss the challenges and solutions for ensuring reliability in APSoC-based deep learning systems.",
      "rating": 6
    },
    "sections": null
  },
  "putra_rescuesnn_2023": {
    "base": {
      "article_type": "article",
      "title": "RescueSNN}: Enabling Reliable Executions on Spiking Neural Network Accelerators under Permanent Fault",
      "authors": [
        "Putra, Rachmad Vidya Wicaksana",
        "Hanif, Muhammad Abdullah",
        "Shafique, Muhammad"
      ],
      "abstract": "To maximize the performance and energy ef\ufb01ciency of Spiking Neural Network ({SNN}) processing on resource-constrained embedded systems, specialized hardware accelerators/chips are employed. However, these {SNN} chips may suffer from permanent faults which can affect the functionality of weight memory and neuron behavior, thereby causing potentially signi\ufb01cant accuracy degradation and system malfunctioning. Such permanent faults may come from manufacturing defects during the fabrication process, and/or from device/transistor damages (e.g., due to wear out) during the run-time operation. However, the impact of permanent faults in {SNN} chips and the respective mitigation techniques have not been thoroughly investigated yet. Toward this, we propose {RescueSNN}, a novel methodology to mitigate permanent faults in the compute engine of {SNN} chips without requiring additional retraining, thereby signi\ufb01cantly cutting down the design time and retraining costs, while maintaining the throughput and quality. The key ideas of our {RescueSNN} methodology are (1) analyzing the characteristics of {SNN} under permanent faults; (2) leveraging this analysis to improve the {SNN} fault-tolerance through effective fault-aware mapping ({FAM}); and (3) devising lightweight hardware enhancements to support {FAM}. Our {FAM} technique leverages the fault map of {SNN} compute engine for (i) minimizing weight corruption when mapping weight bits on the faulty memory cells, and (ii) selectively employing faulty neurons that do not cause signi\ufb01cant accuracy degradation to maintain accuracy and throughput, while considering the {SNN} operations and processing data\ufb02ow. The experimental results show that our {RescueSNN} improves accuracy by up to 80\\% while maintaining the throughput reduction below 25\\% in high fault rate (e.g., 0.5 of the potential fault locations), as compared to running {SNNs} on the faulty chip without mitigation. In this manner, the embedded systems that employ {RescueSNN}-enhanced chips can ef\ufb01ciently ensure reliable executions against permanent faults during their operational lifetime.",
      "publication_year": "",
      "keywords": "Computer Science - Artificial Intelligence, Computer Science - Hardware Architecture, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing",
      "doi": "10.3389/fnins.2023.1159440"
    },
    "suggestion": {
      "relation": "Proposes a methodology to mitigate permanent faults in SNN accelerators, directly addressing the issue of hardware fault tolerance in deep learning by maintaining accuracy and throughput.",
      "suggestion": "Cite this paper to discuss specific strategies for enhancing fault tolerance in hardware accelerators designed for deep learning.",
      "rating": 9
    },
    "sections": null
  },
  "pappalardo_resilience-performance_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "esilience-Performance Tradeoff Analysis of a Deep Neural Network Accelerato",
      "authors": [
        "Pappalardo, Salvatore",
        "Ruospo, Annachiara",
        "O\u2019Connor, Ian",
        "Deveautour, Bastien",
        "Sanchez, Ernesto",
        "Bosio, Alberto"
      ],
      "abstract": "Nowadays, Deep Neural Networks ({DNNs}) are one of the most computationally-intensive algorithms because of the (i) huge amount of data to be transferred from/to the memory, and (ii) the huge amount of matrix multiplications to compute. These issues motivate the design of custom {DNN} hardware accelerators. These accelerators are widely used for low-latency safety-critical applications such as object detection in autonomous cars. Safety-critical applications have to be resilient with respect to hardware faults and Deep Learning ({DL}) accelerators are subjected to hardware faults that can cause functional failures, potentially leading to catastrophic consequences. Although {DNNs} possess a certain level of intrinsic resilience, it varies depending on the hardware on which they are run. The intent of the paper is to assess the resilience of a systolic-array-based {DNN} accelerator in the presence of hardware faults, in order to identify the architectural parameters that may mainly impact the {DNN} resilience.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/DDECS57882.2023.10139704"
    },
    "suggestion": {
      "relation": "Assesses the resilience of a DNN accelerator against hardware faults, focusing on the architectural parameters that influence fault tolerance, which is directly relevant to the literature review topic.",
      "suggestion": "Use this paper to discuss the resilience of DNN accelerators and the factors affecting their fault tolerance.",
      "rating": 8
    },
    "sections": null
  },
  "li_resilient_2021": {
    "base": {
      "article_type": "inproceedings",
      "title": "esilient error-bounded lossy compressor for data transfe",
      "authors": [
        "Li, Sihuan",
        "Di, Sheng",
        "Zhao, Kai",
        "Liang, Xin",
        "Chen, Zizhong",
        "Cappello, Franck"
      ],
      "abstract": "Today\u2019s exa-scale scientific applications or advanced instruments are producing vast volumes of data, which need to be shared/transferred through the network/devices with relatively low bandwidth (e.g., data sharing on {WAN} or transferring from edge devices to supercomputers). Lossy compression is one of the candidate strategies to address the big data issue. However, little work was done to make it resilient against silent errors, which may happen during the stage of compression or data transferring. In this paper, we propose a resilient error-bounded lossy compressor based on the {SZ} compression framework. Specifically, we design a new independentblock-wise model that decomposes the entire dataset into many independent sub-blocks to compress. Then, we design and implement a series of error detection/correction strategies elaboratively for each stage of {SZ}. Our method is arguably the first algorithmbased fault tolerance ({ABFT}) solution for lossy compression. Our proposed solution incurs negligible execution overhead in the faultfree situation. Upon soft errors happening, it ensures decompressed data strictly bounded within user\u2019s requirement with a very limited degradation of compression ratio and low overhead.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1145/3458817.3476195"
    },
    "suggestion": {
      "relation": "Focuses on resilient lossy compression for data transfer, which is tangentially related to fault tolerance in deep learning hardware by ensuring data integrity during compression or transfer.",
      "suggestion": "Reference this paper for discussions on ensuring data integrity, which indirectly supports fault tolerance in deep learning systems.",
      "rating": 4
    },
    "sections": null
  },
  "li_rethinking_2013": {
    "base": {
      "article_type": "inproceedings",
      "title": "ethinking algorithm-based fault tolerance with a cooperative software-hardware approac",
      "authors": [
        "Li, Dong",
        "Chen, Zizhong",
        "Wu, Panruo",
        "Vetter, Jeffrey S."
      ],
      "abstract": "Algorithm-based fault tolerance ({ABFT}) is a highly e\ufb03cient resilience solution for many widely-used scienti\ufb01c computing kernels. However, in the context of the resilience ecosystem, {ABFT} is completely opaque to any underlying hardware resilience mechanisms. As a result, some data structures are over-protected by {ABFT} and hardware, which leads to redundant costs in terms of performance and energy. In this paper, we rethink {ABFT} using an integrated view including both software and hardware with the goal of improving performance and energy e\ufb03ciency of {ABFT}-enabled applications. In particular, we study how to coordinate {ABFT} and error-correcting code ({ECC}) for main memory, and investigate the impact of this coordination on performance, energy, and resilience for {ABFT}-enabled applications. Scaling tests and analysis indicate that our approach saves up to 25\\% for system energy (and up to 40\\% for dynamic memory energy) with up to 18\\% performance improvement over traditional approaches of {ABFT} with {ECC}.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1145/2503210.2503226"
    },
    "suggestion": {
      "relation": "Explores a cooperative software-hardware approach to improve the efficiency of ABFT, indirectly relevant to enhancing fault tolerance in deep learning hardware through optimized resilience mechanisms.",
      "suggestion": "Mention this study to highlight innovative approaches to improving hardware fault tolerance via software-hardware cooperation.",
      "rating": 5
    },
    "sections": null
  },
  "jung_root_2022": {
    "base": {
      "article_type": "article",
      "title": "oot cause analysis of soft-error-induced failures from hardware and software perspective",
      "authors": [
        "Jung, Jinhyo",
        "Ko, Yohan",
        "So, Hwisoo",
        "Lee, Kyoungwoo",
        "Shrivastava, Aviral"
      ],
      "abstract": "Because the dangers of soft errors are increasing with continued technology scaling, reliability against soft errors is becoming an important design concern for modern embedded systems. Various schemes have been proposed to protect embedded systems from the threat of soft errors, but they incur considerable overheads in terms of cost and performance. Selective protection techniques seem promising because they can achieve high levels of protection with low overhead. Though these techniques can be applied to any system, the most vulnerable parts must first be identified. We, therefore, present {CFA}, a comprehensive failure analysis framework that can analyze the vulnerability of microarchitectural components and software instructions through intensive fault injection campaigns. With {CFA}, we also explore the vulnerability of ten benchmarks from the {MiBench} benchmark suite. We found that protecting a part of the system heavily affects the reliability of the other parts. Therefore, all combinations of protection methods must be examined to present the most efficient and effective protection guidelines. Throughout the experiments, we observed that protection methods offered by single-perspective analyses are sub-optimal. On the other hand, {CFA} finds the optimal solution in every case, reducing the {AVF} of a system by up to 82\\% with minimal protection.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.sysarc.2022.102652"
    },
    "suggestion": {
      "relation": "This paper focuses on identifying and protecting vulnerable parts of embedded systems from soft errors, which is indirectly related to ensuring fault tolerance in hardware for deep learning by addressing similar concerns in embedded systems.",
      "suggestion": "Cite this paper to discuss the broader context of hardware fault tolerance, including methods for identifying and protecting against soft errors in embedded systems.",
      "rating": 4
    },
    "sections": null
  },
  "xu_safety_2019": {
    "base": {
      "article_type": "inproceedings",
      "title": "afety Design of a Convolutional Neural Network Accelerator with Error Localization and Correctio",
      "authors": [
        "Xu, Zheng",
        "Abraham, Jacob"
      ],
      "abstract": "Recently neural network accelerators have grown into prominence with signi\ufb01cant power and performance ef\ufb01ciency improvements over {CPU} and {GPU}. In this paper, we proposed two safety design techniques include Algorithm Based Atomic Error Checking-1 ({ABAEC}-1) and {ABAEC}-2 for a Weight Stationary ({WS}) Convolutional Neural Network ({CNN}) accelerator focusing on low latency and low overhead error detection and correction with no performance degradation. The proposed design techniques not only detect the errors on-the\ufb02y but also perform error diagnosis to localize the errors to a Processing Element ({PE}) for on-line fault management and recovery. We applied the design techniques on an industry quality {CNN} accelerator and demonstrated that we could achieve the required Diagnostic Coverage ({DC}) goal with minimal area and power overhead for selected con\ufb01gurations. Furthermore, we discussed methods to extend the proposed techniques to other data\ufb02ow architecture.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/ITC44170.2019.9000149"
    },
    "suggestion": {
      "relation": "The paper presents safety design techniques for CNN accelerators, directly addressing error detection, localization, and correction in deep learning hardware, which is highly relevant to the review's focus on fault tolerance.",
      "suggestion": "Use this paper to highlight specific techniques for achieving fault tolerance in deep learning accelerators, emphasizing error localization and correction.",
      "rating": 9
    },
    "sections": null
  },
  "libano_selective_2019": {
    "base": {
      "article_type": "article",
      "title": "elective Hardening for Neural Networks in {FPGAs",
      "authors": [
        "Libano, F.",
        "Wilson, B.",
        "Anderson, J.",
        "Wirthlin, M. J.",
        "Cazzaniga, C.",
        "Frost, C.",
        "Rech, P."
      ],
      "abstract": "Neural networks are becoming an attractive solution for automatizing vehicles in the automotive, military, and aerospace markets. Thanks to their low-cost, low-power consumption, and \ufb02exibility, \ufb01eld-programmable gate arrays ({FPGAs}) are among the promising devices to implement neural networks. Unfortunately, {FPGAs} are also known to be susceptible to radiation-induced errors. In this paper, we evaluate the effects of radiation-induced errors in the output correctness of two neural networks [Iris Flower arti\ufb01cial neural network ({ANN}) and Modi\ufb01ed National Institute of Standards and Technology ({MNIST}) convolutional neural network ({CNN})] implemented in static random-access memory-based {FPGAs}. In particular, we notice that radiation can induce errors that modify the output of the network with or without affecting the neural network\u2019s functionality. We call the former critical errors and the latter tolerable errors. Through exhaustive fault injection, we identify the portions of Iris Flower {ANN} and {MNIST} {CNN} implementation on {FPGAs} that are more likely, once corrupted, to generate a critical or a tolerable error. Based on this analysis, we propose a selective hardening strategy that triplicates only the most vulnerable layers of the neural network. With neutron radiation testing, our selective hardening solution was able to mask 40\\% of faults with a marginal 8\\% overhead in one of our tested neural networks.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TNS.2018.2884460"
    },
    "suggestion": {
      "relation": "It evaluates the impact of radiation-induced errors on neural networks in FPGAs and proposes a selective hardening strategy, directly contributing to the discussion on hardware fault tolerance in deep learning.",
      "suggestion": "Reference this paper for discussing the effects of radiation on neural network outputs and the strategies for selective hardening in FPGAs.",
      "rating": 8
    },
    "sections": null
  },
  "ruospo_selective_2022": {
    "base": {
      "article_type": "inproceedings",
      "title": "elective Hardening of Critical Neurons in Deep Neural Network",
      "authors": [
        "Ruospo, Annachiara",
        "Gavarini, Gabriele",
        "Bragaglia, Ilaria",
        "Traiola, Marcello",
        "Bosio, Alberto",
        "Sanchez, Ernesto"
      ],
      "abstract": "In the literature, it is argued that Deep Neural Networks ({DNNs}) possess a certain degree of robustness mainly for two reasons: their distributed and parallel architecture, and their redundancy introduced due to over provisioning. Indeed, they are made, as a matter of fact, of more neurons with respect to the minimal number required to perform the computations. It means that they could withstand errors in a bounded number of neurons and continue to function properly. However, it is also known that different neurons in {DNNs} have divergent fault tolerance capabilities. Neurons that contribute the least to the \ufb01nal prediction accuracy are less sensitive to errors. Conversely, the neurons that contribute most are considered critical because errors within them could seriously compromise the correct functionality of the {DNN}. This paper presents a software methodology based on a Triple Modular Redundancy technique, which aims at improving the overall reliability of the {DNN}, by selectively protecting a reduced set of critical neurons. Our \ufb01ndings indicate that the robustness of the {DNNs} can be enhanced, clearly, at the cost of a larger memory footprint and a small increase in the total execution time. The trade-offs as well as the improvements are discussed in the work by exploiting two {DNN} architectures: {ResNet} and {DenseNet} trained and tested on {CIFAR}-10.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/DDECS54261.2022.9770168"
    },
    "suggestion": {
      "relation": "This paper introduces a software methodology for selectively protecting critical neurons in DNNs to enhance fault tolerance, which aligns with the review's theme but from a software perspective complementing hardware fault tolerance.",
      "suggestion": "Cite this paper to discuss complementary software-based approaches to enhancing hardware fault tolerance in deep learning systems.",
      "rating": 6
    },
    "sections": null
  },
  "liu_selective_2022": {
    "base": {
      "article_type": "article",
      "title": "elective Neuron Re-Computation ({SNRC}) for Error-Tolerant Neural Network",
      "authors": [
        "Liu, Shanshan",
        "Reviriego, Pedro",
        "Lombardi, Fabrizio"
      ],
      "abstract": "Arti\ufb01cial Neural networks ({ANNs}) are widely used to solve classi\ufb01cation problems for many machine learning applications. When errors occur in the computational units of an {ANN} implementation due to for example radiation effects, the result of an arithmetic operation can be changed, and therefore, the predicted classi\ufb01cation class may be erroneously affected. This is not acceptable when {ANNs} are used in many safety-critical applications, because the incorrect classi\ufb01cation may result in a system failure. Existing errortolerant techniques usually rely on physically replicating parts of the {ANN} implementation or incurring in a signi\ufb01cant computation overhead. Therefore, ef\ufb01cient protection schemes are needed for {ANNs} that are run on a processor and used in resource-limited platforms. A technique referred to as Selective Neuron Re-Computation ({SNRC}), is proposed in this paper. As per the {ANN} structure and algorithmic properties, {SNRC} can identify the cases in which the errors have no impact on the outcome; therefore, errors only need to be handled by re-computation when the classi\ufb01cation result is detected as unreliable. Compared with existing temporal redundancybased protection schemes, {SNRC} saves more than 60 percent of the re-computation (more than 90 percent in many cases) overhead to achieve complete error protection as assessed over a wide range of datasets. Different activation functions are also evaluated.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TC.2021.3056992"
    },
    "suggestion": {
      "relation": "Proposes a technique for efficient error protection in ANNs by selective neuron re-computation, which indirectly supports hardware fault tolerance by reducing the need for extensive hardware protection.",
      "suggestion": "Mention this paper as an example of how algorithmic strategies can reduce the computational overhead of hardware fault tolerance in deep learning.",
      "rating": 5
    },
    "sections": null
  },
  "sun_selective_2020": {
    "base": {
      "article_type": "inproceedings",
      "title": "elective Protection for Sparse Iterative Solvers to Reduce the Resilience Overhea",
      "authors": [
        "Sun, Hongyang",
        "Gainaru, Ana",
        "Shantharam, Manu",
        "Raghavan, Padma"
      ],
      "abstract": "The increasing scale and complexity of today\u2019s highperformance computing ({HPC}) systems demand a renewed focus on enhancing the resilience of long-running scienti\ufb01c applications in the presence of faults. Many of these applications are iterative in nature as they operate on sparse matrices that concern the simulation of partial differential equations ({PDEs}) which numerically capture the physical properties on discretized spatial domains. While these applications currently bene\ufb01t from many application-agnostic resilience techniques at the system level, such as checkpointing and replication, there is signi\ufb01cant overhead in deploying these techniques. In this paper, we seek to develop application-aware resilience techniques that leverage an iterative application\u2019s intrinsic resiliency to faults and selectively protect certain elements, thereby reducing the resilience overhead. Speci\ufb01cally, we investigate the impact of soft errors on the widely used Preconditioned Conjugate Gradient ({PCG}) method, whose reliability depends heavily on the error propagation through the sparse matrix-vector multiplication ({SpMV}) operation. By characterizing the performance of {PCG} in correlation with a numerical property of the underlying sparse matrix, we propose a selective protection scheme that protects only certain critical elements of the operation based on an analytical model. An experimental evaluation using 20 sparse matrices from the {SuiteSparse} Matrix Collection shows that our proposed scheme is able to reduce the resilience overhead by as much as 70.2\\% and an average of 32.6\\% compared to the baseline techniques with full-protection or zero-protection.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/SBAC-PAD49847.2020.00029"
    },
    "suggestion": {
      "relation": "Focuses on application-aware resilience techniques for iterative applications, which is tangentially related to deep learning fault tolerance by addressing similar concepts in a different application domain.",
      "suggestion": "Reference this paper to discuss the broader concept of application-aware resilience techniques, potentially applicable to deep learning.",
      "rating": 3
    },
    "sections": null
  },
  "al-allaf_simulation-based_2023": {
    "base": {
      "article_type": "article",
      "title": "imulation-based fault-tolerant multiprocessors syste",
      "authors": [
        "Al-Allaf, Ahmad F.",
        "Farej, Ziyad Khalaf"
      ],
      "abstract": "System reliability is an important issue in designing modern multiprocessor systems. This paper proposes a fault-tolerant, scalable, multiprocessor system architecture that adopts a pipeline scheme. To verify the performance of the proposed system, the {SimEvent}/Stateflow tool of the {MATLAB} program was used to simulate the system. The proposed system uses twelve processors (P), connected in a linear array, to build a ten-stage system with two backup processors ({BP}). However, the system can be expanded by adding more processors to increase pipeline stages and performance, and more backup processors to increase system reliability. The system can automatically reorganize itself in the event of a failure of one or two processors and execution continues without interruption. Each processor communicates with its neighboring processors through input/output (I/O) ports which are used as bypass links between the processors. In the event of a processor failure, the function of the faulty processor is assigned to the next processor that is free from faults. The fast Fourier transform ({FFT}) algorithm is implemented on the simulated circuit to evaluate the performance of the proposed system. The results showed that the system can continue to execute even if one or two processors fail without a noticeable decrease in performance.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.12928/telkomnika.v21i2.24253"
    },
    "suggestion": {
      "relation": "Discusses a fault-tolerant multiprocessor system architecture, which is indirectly related to the review's topic by addressing fault tolerance at the system architecture level rather than specifically for deep learning.",
      "suggestion": "Cite this paper to provide context on fault-tolerant system architectures that could be adapted for deep learning hardware.",
      "rating": 4
    },
    "sections": null
  },
  "maleki_simulation-based_2021": {
    "base": {
      "article_type": "inproceedings",
      "title": "imulation-based Fault Injection in Advanced Driver Assistance Systems Modelled in {SUMO",
      "authors": [
        "Maleki, Mehdi",
        "Sangchoolie, Behrooz"
      ],
      "abstract": "Embedded electronic systems used in vehicles are becoming more exposed and thus vulnerable to different types of faults and cybersecurity attacks. Examples of these systems are advanced driver assistance systems ({ADAS}). Failures in these systems could have severe consequences. Therefore, these systems should be thoroughly evaluated during different stages of product development. An effective way of evaluating these systems is through the injection of faults and monitoring their impacts on these systems. Fault injection can be conducted either through the field tests or simulation-based tests. While conducting field tests could be costly and sometimes lifethreatening [1], [2], simulation-based tests provide a wide range of advantages, such as adaptation of tests to a variety of traffic scenarios and avoiding the life-threatening situations. In this paper, we present {SUFI}, a simulation-based fault injector that is capable of injecting faults into {ADAS} features (e.g., car-following and lane-changing) modelled in {SUMO} (simulation of urban mobility). Using {SUFI}, we model different faults and measure their impacts on the target system. The results of the fault injection experiments show the effectiveness of {SUFI} in revealing weaknesses of {ADAS} features modelled in {SUMO} when targeted by faults and attacks.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/DSN-S52858.2021.00036"
    },
    "suggestion": {
      "relation": "This paper's focus on simulation-based fault injection for evaluating ADAS systems is tangentially related, offering insights into fault injection methodologies that could be applied to test deep learning systems' fault tolerance.",
      "suggestion": "Use this paper to discuss fault injection as a method for evaluating the fault tolerance of deep learning systems, despite its primary focus on ADAS.",
      "rating": 3
    },
    "sections": null
  },
  "rajappa_smart_nodate": {
    "base": {
      "article_type": "article",
      "title": "SMART}: Selective {MAC} zero-optimzation for neural network reliability under radiatio",
      "authors": [
        "Rajappa, Anuj Justus",
        "Reiter, Philippe",
        "Sartori, Tarso Kraemer Sarzi",
        "Laurini, Luiz Henrique",
        "Fourati, Hassen",
        "Mercelis, Siegfried",
        "Hellinckx, Peter",
        "Bastos, Rodrigo Possamai"
      ],
      "abstract": "Neural networks running on low-power edge devices can help in achieving ubiquitous computing with limited infrastructure. When such edge devices are deployed in conventional and extreme environments without the necessary shields, they must be fault tolerant for reliable operation. As a pilot study, we focus on embedding fault tolerance into neural networks by proposing a novel selective multiply-accumulate zero-optimization technique based on whether the value of an input provided to a neuron of a neural network is zero. If the value is zero, then the corresponding multiply-accumulate operation is bypassed. We subjected the implementation of our optimization technique to radiation test campaigns using \u223c14 {MeV} neutrons, and found the proposed optimization technique to improve the fault tolerance of the tested neural network by a factor of 1.78 times.",
      "publication_year": "",
      "keywords": "",
      "doi": ""
    },
    "suggestion": {
      "relation": "This paper introduces a fault tolerance technique for neural networks on edge devices, focusing on a selective optimization method to improve reliability under radiation, which is directly relevant to hardware error fault tolerance in deep learning.",
      "suggestion": "Cite this paper as an example of innovative fault tolerance techniques specifically designed for neural networks in radiation-prone environments.",
      "rating": 8
    },
    "sections": null
  },
  "yang_smart_2023": {
    "base": {
      "article_type": "article",
      "title": "mart Traffic Navigation System for Fault-Tolerant Edge Computing of Internet of Vehicle in Intelligent Transportation Gatewa",
      "authors": [
        "Yang, Shuangming",
        "Tan, Jiangtong",
        "Lei, Tao",
        "Linares-Barranco, Bernabe"
      ],
      "abstract": "To investigate the diversi\ufb01ed technologies in Internet of Vehicles ({IoVs}) under intelligent edge computing, braininspired computing techniques are proposed in this study, which is a promising biologically inspired method by using brain cognition mechanism for various applications. A neuromorphic approach in a scalable and fault-tolerant framework is presented, targeting to realize the navigation function for the edge computing in {IoV} applications. A novel fault-tolerant address event representation approach is proposed for the spike information routing, which makes the presented model both scalable and fault-tolerant. Experimental results reveal that the proposed approaches can enhance the communication distance, the load balancing and the maximum throughput of the neuromorphic system accordingly. Based on the proposed neuromorphic model, the effects of the dopamine level are investigated. Besides, the results show that the proposed work can realize the accurate obstacle avoidance for the edge {IoV} computing, and the performance of the proposed network is superior to the network without the proposed scalable and faulttolerant design. Therefore, the proposed {IoV} model provides an experimental basis for the improvement of the {IoV} system.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TITS.2022.3232231"
    },
    "suggestion": {
      "relation": "Although focused on neuromorphic computing for Internet of Vehicles, this paper presents fault-tolerant design principles that could be extrapolated to deep learning hardware fault tolerance.",
      "suggestion": "Reference this paper for its fault-tolerant design approach in neuromorphic computing, drawing parallels to deep learning hardware.",
      "rating": 5
    },
    "sections": null
  },
  "savalam_soft_2023": {
    "base": {
      "article_type": "article",
      "title": "oft error detection and correction for parallel digital filters using Hamming cod",
      "authors": [
        "Savalam, Chandrasekhar",
        "Alapati, Venkata Nagaratna Tilak"
      ],
      "abstract": "The digital filters play a significant role in signal processing and communication systems. In applications like space and medical, where consistency is very essential, some amount of fault tolerance need to be maintained for their operation. Hamming code is found to be useful in the protection of parallel filters. Every filter is analogous to a bit in conventional Hamming code. In this work, faulttolerant digital Finite Impulse Response ({FIR}) and Infinite Impulse Response ({IIR}) filters, which have the same impulse response that process different signal inputs and different responses that process the same signal inputs are designed using Hamming code to detect and correct singlebit soft errors. The implementation cost in terms of number of Artix-7 xc7a200tffg {FPGA} device resources utilized is found to be reduced as compared to triple modular redundancy method by 40.68\\% and 51.82\\%, respectively, for four and eleven original {FIR} filters, whereas these are 28.17\\% and 28.79\\% for original {IIR} filters having identical impulse response. In case of various impulse responses, the reduction is 36\\%, 50.36\\%, 30.62\\%, and 13.49\\%, respectively. The proposed technique was implemented and mapped using an {FPGA} device called Artix-7 xc7a200tffg, and the results demonstrated that it was effective in terms of resource utilization and implementation costs.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1007/s13198-023-01876-6"
    },
    "suggestion": {
      "relation": "This paper discusses the use of Hamming code for fault detection and correction in digital filters, which is indirectly related to ensuring computational accuracy in hardware used for deep learning.",
      "suggestion": "Mention this paper as an example of traditional fault tolerance methods that could inspire similar strategies in deep learning hardware.",
      "rating": 4
    },
    "sections": null
  },
  "topcu_soft_2023": {
    "base": {
      "article_type": "article",
      "title": "oft error vulnerability prediction of {GPGPU} application",
      "authors": [
        "Top\u00e7u, Burak",
        "\u00d6z, I\u015f\u0131l"
      ],
      "abstract": "As graphics processing units ({GPUs}) evolve to offer high performance for generalpurpose computations in addition to inherently fault-tolerant graphics applications, soft error reliability becomes a significant concern. Fault injection provides a method of evaluating the soft error vulnerability of target programs. Since performing fault injection experiments for complex {GPU} hardware structures takes impractical times, the prediction-based techniques to evaluate the soft error vulnerability of general-purpose {GPU} ({GPGPU}) programs based on metrics from different domains get crucial for both {HPC} developers and {GPU} vendors. In this work, we propose machine learning ({ML})-based prediction frameworks for the soft error vulnerability evaluation of {GPGPU} programs. We consider program characteristics, hardware usage and performance metrics collected from the simulation and the profiling tools. While we utilize regression models to predict the masked fault rates, we build classification models to specify the vulnerability level of the {GPGPU} programs based on their silent data corruption ({SDC}) and crash rates. Our prediction models achieve maximum prediction accuracy rates of 95.9, 88.46, and 85.7\\% for masked fault rates, {SDCs}, and crashes, respectively.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1007/s11227-022-04933-2"
    },
    "suggestion": {
      "relation": "It explores soft error vulnerability prediction in GPGPU applications, including deep learning, using machine learning models, which is pertinent to understanding and mitigating hardware errors in deep learning computations.",
      "suggestion": "Utilize this paper to discuss the importance of predicting hardware error vulnerabilities in deep learning applications.",
      "rating": 7
    },
    "sections": null
  },
  "ibrahim_soft_2020": {
    "base": {
      "article_type": "article",
      "title": "oft errors in {DNN} accelerators: A comprehensive revie",
      "authors": [
        "Ibrahim, Younis",
        "Wang, Haibin",
        "Liu, Junyang",
        "Wei, Jinghe",
        "Chen, Li",
        "Rech, Paolo",
        "Adam, Khalid",
        "Guo, Gang"
      ],
      "abstract": "Deep learning tasks cover a broad range of domains and an even more extensive range of applications, from entertainment to extremely safety-critical fields. Thus, Deep Neural Network ({DNN}) algorithms are implemented on different systems, from small embedded devices to data centers. {DNN} accelerators have proven to be a key to efficiency, as they are even more efficient than {CPUs}. Therefore, they have become the major executing hardware for {DNN} algorithms. However, these accelerators are susceptible to several types of faults. Soft errors pose a particular threat because the high-level parallelism in these accelerators can propagate a single failure to mul\u00ad tiple errors in the next levels until the model predictions\u2019 output is affected. This article presents a compre\u00ad hensive review of the reliability of the {DNN} accelerators. The study begins by reviewing the widely assumed claim that {DNNs} are inherently tolerant to faults. Then, the available {DNN} accelerators are systematically classified into several categories. Each is individually analyzed; and the commonly used accelerators are compared in an attempt to answer the question, which accelerator is more reliable against transient faults? The concluding part of this review highlights the gray areas of the {DNNs} and predicts future research directions that will enhance its applicability. This study is expected to benefit researchers in the areas of deep learning, {DNN} accelerators, and reliability of this efficient paradigm.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.microrel.2020.113969"
    },
    "suggestion": {
      "relation": "Offers a comprehensive review of DNN accelerators' susceptibility to faults, including soft errors, and their impact on reliability, aligning closely with the topic of hardware fault tolerance in deep learning.",
      "suggestion": "Cite this review to provide a broad overview of the challenges and considerations in making DNN accelerators fault-tolerant.",
      "rating": 9
    },
    "sections": null
  },
  "ibrahim_soft_2020-1": {
    "base": {
      "article_type": "article",
      "title": "oft Error Resilience of Deep Residual Networks for Object Recognitio",
      "authors": [
        "Ibrahim, Younis",
        "Wang, Haibin",
        "Bai, Man",
        "Liu, Zhi",
        "Wang, Jianan",
        "Yang, Zhiming",
        "Chen, Zhengming"
      ],
      "abstract": "Convolutional Neural Networks ({CNNs}) have truly gained attention in object recognition and object classi\ufb01cation in particular. When being implemented on Graphics Processing Units ({GPUs}), deeper networks are more accurate than shallow ones. Residual Networks ({ResNets}) are one of the deepest {CNN} architectures used in various \ufb01elds including safety-critical ones. {GPUs} have proven to be the major accelerator for {CNN} models. However, modern {GPUs} are prone to radiation-induced soft errors, which is a serious issue in safety-compliant systems. In this work, we analyze and propose an approach to address the reliability of {ResNet} on {GPUs}. We \ufb01rstly analyze three popular {ResNet} models, explicitly, {ResNet}-50, {ResNet}-101, and {ResNet}-152 through {NVIDIA}\u2019s fault injector, {SASSIFI}. We perform an indepth analysis of the model from the perspective of layer and kernel vulnerability. Then, we experimentally show the vulnerability of {ResNet} models and identify the most vulnerable portions. Finally, we validate our solution, which is a selective-hardening technique, through hardening the worth-hardening kernels to avoid unnecessary overheads. Our strategy is demonstrated to mask up to 93.38\\% of the injected errors with performance overhead less than 5.35\\%. Furthermore, the percentage of the errors causing misclassi\ufb01cations can be reduced from 4.2\\% to 0.104\\%, thereby signi\ufb01cantly improving the model\u2019s reliability.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/ACCESS.2020.2968129"
    },
    "suggestion": {
      "relation": "Analyzes the resilience of deep residual networks to soft errors on GPUs, offering insights into fault tolerance strategies for deep learning models, which is highly relevant to the literature review topic.",
      "suggestion": "Reference this paper for its detailed analysis of fault tolerance in specific deep learning architectures (ResNets) and its proposed solutions.",
      "rating": 8
    },
    "sections": null
  },
  "putra_softsnn_2022": {
    "base": {
      "article_type": "inproceedings",
      "title": "SoftSNN}: Low-Cost Fault Tolerance for Spiking Neural Network Accelerators under Soft Error",
      "authors": [
        "Putra, Rachmad Vidya Wicaksana",
        "Hanif, Muhammad Abdullah",
        "Shafique, Muhammad"
      ],
      "abstract": "Specialized hardware accelerators have been designed and employed to maximize the performance efficiency of Spiking Neural Networks ({SNNs}). However, such accelerators are vulnerable to transient faults (i.e., soft errors), which occur due to high-energy particle strikes, and manifest as bit flips at the hardware layer. These errors can change the weight values and neuron operations in the compute engine of {SNN} accelerators, thereby leading to incorrect outputs and accuracy degradation. However, the impact of soft errors in the compute engine and the respective mitigation techniques have not been thoroughly studied yet for {SNNs}. A potential solution is employing redundant executions (re-execution) for ensuring correct outputs, but it leads to huge latency and energy overheads. Toward this, we propose {SoftSNN}, a novel methodology to mitigate soft errors in the weight registers (synapses) and neurons of {SNN} accelerators without re-execution, thereby maintaining the accuracy with low latency and energy overheads. Our {SoftSNN} methodology employs the following key steps: (1) analyzing the {SNN} characteristics under soft errors to identify faulty weights and neuron operations, which are required for recognizing faulty {SNN} behavior; (2) a Bound-and-Protect technique that leverages this analysis to improve the {SNN} fault tolerance by bounding the weight values and protecting the neurons from faulty operations; and (3) devising lightweight hardware enhancements for the neural hardware accelerator to efficiently support the proposed technique. The experimental results show that, for a 900-neuron network with even a high fault rate, our {SoftSNN} maintains the accuracy degradation below 3\\%, while reducing latency and energy by up to 3x and 2.3x respectively, as compared to the re-execution technique.",
      "publication_year": "",
      "keywords": "Computer Science - Hardware Architecture, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing",
      "doi": "10.1145/3489517.3530657"
    },
    "suggestion": {
      "relation": "Proposes a methodology for mitigating soft errors in SNN accelerators without re-execution, directly addressing hardware fault tolerance in a specific type of neural network accelerator.",
      "suggestion": "Use this paper to discuss fault tolerance techniques in spiking neural network accelerators as a case study in the broader context of deep learning hardware.",
      "rating": 7
    },
    "sections": null
  },
  "reghenzani_software_2023": {
    "base": {
      "article_type": "article",
      "title": "oftware Fault Tolerance in Real-Time Systems: Identifying the Future Research Question",
      "authors": [
        "Reghenzani, Federico",
        "Guo, Zhishan",
        "Fornaciari, William"
      ],
      "abstract": "Tolerating hardware faults in modern architectures is becoming a prominent problem due to the miniaturization of the hardware components, their increasing complexity, and the necessity to reduce costs. Software-Implemented Hardware Fault Tolerance approaches have been developed to improve system dependability regarding hardware faults without resorting to custom hardware solutions. However, these come at the expense of making the satisfaction of the timing constraints of the applications/activities harder from a scheduling standpoint. This article surveys the current state-of-the-art of fault tolerance approaches when used in the context of real-time systems, identifying the main challenges and the cross-links between these two topics. We propose a joint scheduling-failure analysis model that highlights the formal interactions among software fault tolerance mechanisms and timing properties. This model allows us to present and discuss many open research questions with the final aim to spur future research activities.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1145/3589950"
    },
    "suggestion": {
      "relation": "Discusses software-implemented hardware fault tolerance in real-time systems, which is tangentially related to the topic but focuses more on software approaches rather than hardware design for deep learning.",
      "suggestion": "Mention this paper to highlight the importance of software approaches in complementing hardware fault tolerance strategies in deep learning systems.",
      "rating": 3
    },
    "sections": null
  },
  "liu_fault-tolerant_2022": {
    "base": {
      "article_type": "misc",
      "title": "ault-Tolerant Deep Learning: A Hierarchical Perspectiv",
      "authors": [
        "Liu, Cheng",
        "Gao, Zhen",
        "Liu, Siting",
        "Ning, Xuefei",
        "Li, Huawei",
        "Li, Xiaowei"
      ],
      "abstract": "With the rapid advancements of deep learning in the past decade, it can be foreseen that deep learning will be continuously deployed in more and more safety-critical applications such as autonomous driving and robotics. In this context, reliability turns out to be critical to the deployment of deep learning in these applications and gradually becomes a \ufb01rstclass citizen among the major design metrics like performance and energy ef\ufb01ciency. Nevertheless, the back-box deep learning models combined with the diverse underlying hardware faults make resilient deep learning extremely challenging. In this special session, we conduct a comprehensive survey of fault-tolerant deep learning design approaches with a hierarchical perspective and investigate these approaches from model layer, architecture layer, circuit layer, and cross layer respectively.",
      "publication_year": "",
      "keywords": "B.2.3, B.8.1, Computer Science - Artificial Intelligence, Computer Science - Hardware Architecture, Computer Science - Machine Learning",
      "doi": ""
    },
    "suggestion": {
      "relation": "This paper provides a comprehensive overview of fault-tolerant design approaches for deep learning from a hierarchical perspective, addressing model, architecture, circuit, and cross-layer resilience against hardware faults.",
      "suggestion": "Cite this paper to discuss the importance of hierarchical fault-tolerant designs in ensuring the reliability of deep learning applications in safety-critical systems.",
      "rating": 9
    },
    "sections": null
  },
  "kundu_special_2021": {
    "base": {
      "article_type": "misc",
      "title": "pecial Session: Reliability Analysis for {ML}/{AI} Hardwar",
      "authors": [
        "Kundu, Shamik",
        "Basu, Kanad",
        "Sadi, Mehdi",
        "Titirsha, Twisha",
        "Song, Shihao",
        "Das, Anup",
        "Guin, Ujjwal"
      ],
      "abstract": "Arti\ufb01cial intelligence ({AI}) and Machine Learning ({ML}) are becoming pervasive in today\u2019s applications, such as autonomous vehicles, healthcare, aerospace, cybersecurity, and many critical applications. Ensuring the reliability and robustness of the underlying {AI}/{ML} hardware becomes our paramount importance. In this paper, we explore and evaluate the reliability of different {AI}/{ML} hardware. The \ufb01rst section outlines the reliability issues in a commercial systolic array-based {ML} accelerator in the presence of faults engendering from devicelevel non-idealities in the {DRAM}. Next, we quanti\ufb01ed the impact of circuit-level faults in the {MSB} and {LSB} logic cones of the Multiply and Accumulate ({MAC}) block of the {AI} accelerator on the {AI}/{ML} accuracy. Finally, we present two key reliability issues \u2013 circuit aging and endurance in emerging neuromorphic hardware platforms and present our system-level approach to mitigate them.",
      "publication_year": "",
      "keywords": "Computer Science - Hardware Architecture",
      "doi": ""
    },
    "suggestion": {
      "relation": "It evaluates the reliability of AI/ML hardware, including the impact of circuit-level faults on AI accelerator accuracy, which is directly related to fault tolerance in deep learning hardware.",
      "suggestion": "Use this paper to highlight specific hardware reliability challenges and fault tolerance strategies in AI/ML accelerators.",
      "rating": 8
    },
    "sections": null
  },
  "asgari_khoshouyeh_structural_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "tructural Coding: A Low-Cost Scheme to Protect {CNNs} from Large-Granularity Memory Fault",
      "authors": [
        "Asgari Khoshouyeh, Ali",
        "Geissler, Florian",
        "Qutub, Syed",
        "Paulitsch, Michael",
        "Nair, Prashant",
        "Pattabiraman, Karthik"
      ],
      "abstract": "The advent of High-Performance Computing has led to the adoption of Convolutional Neural Networks ({CNNs}) in safety-critical applications such as autonomous vehicles. However, {CNNs} are vulnerable to {DRAM} errors corrupting their parameters, thereby degrading their accuracy. Existing techniques for protecting {CNNs} from {DRAM} errors are either expensive or fail to protect from largegranularity, multi-bit errors, which occur commonly in {DRAMs}. We propose a software-implemented coding scheme, Structural Coding ({SC}) for protecting {CNNs} from large-granularity memory errors. {SC} achieves three orders of magnitude reduction in Silent Data Corruption ({SDC}) rates of {CNNs} compared to no protection. Its average error correction coverage is also significantly higher than other software techniques to protect {CNNs} from faults in the memory. Further, its average performance, memory, and energy overheads are respectively 3\\%, 15.71\\%, and 4.38\\%. These overheads are much lower than other software protection techniques.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1145/3581784.3607084"
    },
    "suggestion": {
      "relation": "This paper introduces a software-implemented coding scheme to protect CNNs from large-granularity memory faults, offering a solution to hardware-induced errors in deep learning models.",
      "suggestion": "Reference this paper for discussions on software-based approaches to enhancing hardware fault tolerance in CNNs.",
      "rating": 7
    },
    "sections": null
  },
  "ko_survey_2022": {
    "base": {
      "article_type": "article",
      "title": "urvey of Software-Implemented Soft Error Protectio",
      "authors": [
        "Ko, Yohan"
      ],
      "abstract": "As soft errors are important design concerns in embedded systems, several schemes have been presented to protect embedded systems against them. Embedded systems can be protected by hardware redundancy; however, hardware-based protections cannot provide \ufb02exible protection due to hardware-only protection modi\ufb01cations. Further, they incur signi\ufb01cant overheads in terms of area, performance, and power consumption. Therefore, hardware redundancy techniques are not appropriate for resource-constrained embedded systems. On the other hand, software-based protection techniques can be an attractive alternative to protect embedded systems, especially speci\ufb01c-purpose architectures. This manuscript categorizes and compares software-based redundancy techniques for general-purpose and speci\ufb01c-purpose processors, such as {VLIW} (Very Long Instruction Word) and {CGRA} (Coarse-Grained Recon\ufb01gurable Architectures).",
      "publication_year": "",
      "keywords": "",
      "doi": "10.3390/electronics11030456"
    },
    "suggestion": {
      "relation": "Although focused on software-implemented error protection, this paper provides insights into the limitations of hardware redundancy in embedded systems, relevant to the broader context of fault tolerance in deep learning hardware.",
      "suggestion": "Mention this paper when comparing hardware and software fault tolerance solutions for deep learning systems.",
      "rating": 5
    },
    "sections": null
  },
  "burlyaev_system_2014": {
    "base": {
      "article_type": "article",
      "title": "ystem fault-tolerance analysis of {COTS}-based satellite on-board computer",
      "authors": [
        "Burlyaev, Dmitry",
        "Van Leuken, Rene"
      ],
      "abstract": "Fault-tolerance analysis reveals possible system behavior under the in\ufb02uence of faults. Such analysis is essential for satellites where faults might be caused by space radiation and autonomous recovery is needed. In this paper we present a statistical simulation approach for fault-tolerance analysis of satellite On-Board Computers ({OBCs}) that are based on Commercial Off-The-Shelf ({COTS}) components. Since the logic level of {COTS} electronics is unknown to satellite designers, a new higher-level fault-tolerance analysis is required. We propose such technique that relies on {OBC} modeling and fault modeling, based on the modeling principle of Single-Event Upsets ({SEUs}). For the \ufb01rst time we can compare the ef\ufb01ciency of fault-tolerance techniques implemented in software and Field-Programmable Gate Array ({FPGA}). In addition, our approach enables to analyze system fault-tolerance at early development stages. In a case study the approach is applied to an {OBC} with a Microsemi {SmartFusion} {SoC}, that executes a satellite attitude control algorithm. The gained statistical simulation results enabled 50\\% reduction in the hardware overhead of the implemented memory scrubbing technique without loss in fault-tolerance. Our method revealed critical fault-tolerance drawbacks of the initial system design that could have lead to satellite mission failure.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.mejo.2014.01.007"
    },
    "suggestion": {
      "relation": "The paper presents a fault-tolerance analysis method for satellite on-board computers using COTS components, which, while not directly related to deep learning, addresses hardware fault tolerance in critical applications.",
      "suggestion": "Cite this paper as an example of fault tolerance analysis in non-deep learning but safety-critical hardware systems.",
      "rating": 3
    },
    "sections": null
  },
  "gao_systematic_2023": {
    "base": {
      "article_type": "article",
      "title": "ystematic Reliability Evaluation of {FPGA} Implemented {CNN} Accelerator",
      "authors": [
        "Gao, Zhen",
        "Gao, Shihui",
        "Yao, Yi",
        "Liu, Qiang",
        "Zeng, Shulin",
        "Ge, Guangjun",
        "Wang, Yu",
        "Ullah, Anees",
        "Reviriego, Pedro"
      ],
      "abstract": "Convolutional neural networks ({CNN}) have become essential for many scienti\ufb01c and industrial applications, such as image classi\ufb01cation and pattern detection. Among the devices that can implement neural networks, {SRAM} based {FPGAs} are a popular option due to their excellent parallel computing capability and good \ufb02exibility. However, {SRAM}-{FPGAs} are susceptible to radiation effects, which limits its application on safety critical applications. In this paper, the reliability of an accelerator based on the advanced Instruction-Set Architecture is evaluated based on hardware fault injection experiments. Each main module of the accelerator is evaluated separately, and the impact of parallelism and model features on the accelerator reliability is also examined. The experimental results reveal some important conclusions in terms of general hardware reliability and also of the particular model reliability. First, over 99\\% of {SEUs} on the computation modules will cause accuracy loss, and the reliability improves for higher parallelism. Second, a large portion of {SEUs} on the data mover and the instruction scheduler will cause system corruptions due to abnormal interactions with the {ARM} or other modules. Third, nonlinear activation and pooling layers are effective in reducing the effect of {SEUs} on computation modules, so models that use these layers tend to be more robust. The results provide a deep understanding of the impact of errors on {CNNs} implemented on {ISA} based {FPGA} accelerators (e.g., the Xilinx {DPU}).",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TDMR.2023.3235767"
    },
    "suggestion": {
      "relation": "It assesses the reliability of FPGA-implemented CNN accelerators against radiation effects, offering insights into hardware fault tolerance in deep learning applications.",
      "suggestion": "Reference this paper to discuss the challenges and solutions in ensuring the reliability of FPGA-based deep learning accelerators.",
      "rating": 8
    },
    "sections": null
  },
  "wei_tc-sepm_2023": {
    "base": {
      "article_type": "article",
      "title": "TC}-{SEPM}: Characterizing soft error resilience of {CNNs} on Tensor Cores from program and microarchitecture perspective",
      "authors": [
        "Wei, Xiaohui",
        "Zhou, Changbao",
        "Yue, Hengshan",
        "Zhou, Joey Tianyi"
      ],
      "abstract": "As an architectural {CNN} accelerator integrated into {NVIDIA}\u2019s {GPUs}, existing research mainly focuses on improving the performance of Tensor Cores. However, the highly integrated Tensor Cores are vulnerable to transient faults (i.e., soft errors), causing catastrophic consequences in safety-critical applications like automatic driving. Thus, it is imperative to estimate the reliability of {CNNs} on Tensor Cores. However, obtaining a statistically significant resilience profile of {CNNs} on Tensor Cores with the existing fault injection ({FI})-based reliability estimation methods is expensive. To this end, we build {TC}-{SEPM} to predict the error resilience of {CNNs} on Tensor Cores instead of {FI} methods. To ensure the accuracy of {TC}-{SEPM}, we first investigate resilience-related features from program and microarchitecture perspectives. Then, leveraging these heuristic features, we train machine learning models to learn the hidden relationship between error resilience and the investigated features, enabling us to predict the impact of soft errors in Tensor Cores on {CNN} output. Experimental results show that {TC}-{SEPM} achieves high accuracy for individual soft error resiliency prediction and overall program resilience estimation while its overhead is only 1/27 of {FI} methods. Additionally, {TCSEPM} can provide valuable insights for programmers or architects to design more robust {CNN} models on Tensor Cores.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.sysarc.2023.103024"
    },
    "suggestion": {
      "relation": "This paper focuses on characterizing the soft error resilience of CNNs on Tensor Cores, addressing the reliability of deep learning models on specific GPU architectures.",
      "suggestion": "Use this paper to discuss the impact of soft errors on deep learning models and the importance of fault tolerance in GPU-accelerated deep learning.",
      "rating": 7
    },
    "sections": null
  },
  "krishnan_tenet_nodate": {
    "base": {
      "article_type": "article",
      "title": "TENET}: Memory Safe and Fault Tolerant Persistent Transactional Memor",
      "authors": [
        "Krishnan, R Madhava",
        "Zhou, Diyu",
        "Kim, Wook-Hee",
        "Kannan, Sudarsun",
        "Kashyap, Sanidhya",
        "Min, Changwoo"
      ],
      "abstract": "Byte-addressable non-volatile memory ({NVM}) allows programs to directly access storage using memory interface without going through the expensive conventional storage stack. However, direct access to {NVM} makes the {NVM} data vulnerable to software bugs and hardware errors. This issue is critical because, unlike {DRAM}, corrupted data can persist forever, even after the system restart. Albeit the plethora of research on {NVM} programs and systems, there is little focus on protecting {NVM} data from software bugs and hardware errors.",
      "publication_year": "",
      "keywords": "",
      "doi": ""
    },
    "suggestion": {
      "relation": "While it discusses protecting non-volatile memory data from errors, its focus on transactional memory safety and fault tolerance is tangentially related to deep learning hardware fault tolerance.",
      "suggestion": "Mention this paper in the context of data integrity and fault tolerance in memory systems supporting deep learning applications.",
      "rating": 4
    },
    "sections": null
  },
  "panek_fault-tolerant_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "he Fault-tolerant Single-{FPGA} Systems with a Self-repair Reconfiguration Controlle",
      "authors": [
        "P\u00e1nek, Richard",
        "Lojda, Jakub"
      ],
      "abstract": "Fault tolerance in electronic systems is essential in harsh environments such as space. However, {FPGAs} that can be used to accelerate various computations are prone to configuration memory faults that determine their function. Repairing these faults is essential to increase system resilience. For this purpose, the partial dynamic reconfiguration controller is necessary. We force the controller to be on the same {FPGA} with a payload circuit to design a comprehensive system inside one {FPGA}. We create and thoroughly test a new reconfiguration controller to increase the system\u2019s resiliency with the ability to repair itself during its own operation. For this purpose, the {FPGA} controller is in coarse-grained triple modular redundancy to be able to recover despite the failure of any of its modules. The proposed controller has been tested to increase the resilience of circuits from a set of benchmark circuits. The entire system with the controller was evaluated on an actual {FPGA}, where faults were injected directly into the configuration memory of this {FPGA}. Reliability parameters are measured by a platform designed for this purpose, partly directly on the tested {FPGA}. As we can see from the results, the mean time to failure has been increased by up to 69\\% compared to a system equipped with only triple modular redundancy with a reasonable amount of hardware resources. The competitive solution brings only a 42\\% improvement in resilience with similar parameters.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/LASCAS56464.2023.10108372"
    },
    "suggestion": {
      "relation": "This paper focuses on enhancing fault tolerance in FPGA systems through a self-repair reconfiguration controller, directly addressing hardware fault tolerance in deep learning accelerators.",
      "suggestion": "Cite this paper to discuss innovative approaches in FPGA-based fault tolerance for deep learning applications.",
      "rating": 7
    },
    "sections": null
  },
  "geissler_towards_2021": {
    "base": {
      "article_type": "misc",
      "title": "owards a Safety Case for Hardware Fault Tolerance in Convolutional Neural Networks Using Activation Range Supervisio",
      "authors": [
        "Geissler, Florian",
        "Qutub, Syed",
        "Roychowdhury, Sayanta",
        "Asgari, Ali",
        "Peng, Yang",
        "Dhamasia, Akash",
        "Graefe, Ralf",
        "Pattabiraman, Karthik",
        "Paulitsch, Michael"
      ],
      "abstract": "Convolutional neural networks ({CNNs}) have become an established part of numerous safetycritical computer vision applications, including human robot interactions and automated driving. Real-world implementations will need to guarantee their robustness against hardware soft errors corrupting the underlying platform memory. Based on the previously observed ef\ufb01cacy of activation clipping techniques, we build a prototypical safety case for classi\ufb01er {CNNs} by demonstrating that range supervision represents a highly reliable fault detector and mitigator with respect to relevant bit \ufb02ips, adopting an eight-exponent \ufb02oating point data representation. We further explore novel, non-uniform range restriction methods that effectively suppress the probability of silent data corruptions and uncorrectable errors. As a safety-relevant end-to-end use case, we showcase the bene\ufb01t of our approach in a vehicle classi\ufb01cation scenario, using {ResNet}50 and the traf\ufb01c camera data set {MIOVision}. The quantitative evidence provided in this work can be leveraged to inspire further and possibly more complex {CNN} safety arguments.",
      "publication_year": "",
      "keywords": "Computer Science - Machine Learning",
      "doi": ""
    },
    "suggestion": {
      "relation": "The paper explores hardware fault tolerance in CNNs through activation range supervision, a method relevant for ensuring deep learning model reliability against hardware errors.",
      "suggestion": "Use this paper to illustrate fault detection and mitigation techniques in CNNs, emphasizing hardware error resilience.",
      "rating": 8
    },
    "sections": null
  },
  "lu_towards_2024": {
    "base": {
      "article_type": "article",
      "title": "owards Critical Flip-Flop Identification for Soft-Error Tolerance with Graph Neural Network",
      "authors": [
        "Lu, Li",
        "Chen, Junchao",
        "Ulbricht, Markus",
        "Krstic, Milos"
      ],
      "abstract": "Nanometer circuits are becoming increasingly susceptible to soft errors. Selective hardening is a less expensive technique to improve the reliability of circuits because it hardens the critical components instead of hardening an entire circuit. One challenge of selective hardening is efficiently and effectively identifying the critical parts in circuits. Simulationbased fault injection is commonly used but extremely timeconsuming, especially for complex circuits. This paper proposes an approach based on Graph Neural Networks ({GNNs}) to identify critical flip-flops in circuits. {GNNs} can take advantage of the circuit\u2019s structural features and the features of individual flipflops. To convert the features into abstract data that can be fed into {GNNs}, we provide a feature extraction method that uses a graph model to represent the relevant features of the circuit. The method converts the target circuit into a graph representing its architecture. The graph also contains features of individual flip-flops extracted from the circuit\u2019s netlist and the Value Change Dump ({VCD}) waveforms of the test used for fault simulation. Additionally, we extract edge features in the graph to utilize the information on combinational gates on the path between flip-flops. Datasets generated based on two opensource {RISC}-V cores are used to validate the proposed approach. We compare the performance of different {GNNs} on them and discuss the contribution of edge features to their performance. Our experiments show that the prediction accuracy increases significantly with edge features. {GraphSAGE} and {SAGE}-{GCN} with edge features perform best among the selected {GNNs}. The highest accuracy we achieved on Ibex and {RI}5CY is 97.75\\% and 98.67\\%, respectively. We also provide a method to accelerate the process of critical flip-flop identification.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TCAD.2023.3331968"
    },
    "suggestion": {
      "relation": "It introduces a method using Graph Neural Networks for identifying critical components in circuits for selective hardening, indirectly contributing to hardware fault tolerance in deep learning systems.",
      "suggestion": "Reference this paper for discussions on selective hardening techniques as a means to enhance hardware fault tolerance.",
      "rating": 6
    },
    "sections": null
  },
  "sanic_towards_2022": {
    "base": {
      "article_type": "inproceedings",
      "title": "owards Reliable {AI} Applications via Algorithm-Based Fault Tolerance on {NVDLA",
      "authors": [
        "Sanic, Mustafa Tarik",
        "Guo, Cong",
        "Leng, Jingwen",
        "Guo, Minyi",
        "Ma, Weiyin"
      ],
      "abstract": "With the development of deep neural networks ({DNNs}), more complex accelerators have been designed for more sophisticated networks. Naturally, the complexity of accelerators makes them vulnerable to transient errors. Also, some {DNN} accelerators are widely used the safety-critical systems, such as autonomous vehicles. Therefore, the susceptibility to transient errors makes research on mitigation techniques more significant, and errors of accelerators should be limited to none. Some researchers proposed the modular redundancy method, which offers a highly reliable way but also considerably increases overhead. In this regard, algorithm-based solutions offer cheaper solutions. However, their implementation is primarily observed in software-based error injections.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/MSN57253.2022.00120"
    },
    "suggestion": {
      "relation": "Discusses algorithm-based fault tolerance in DNN accelerators, highlighting the importance of addressing transient errors in hardware for reliable AI applications.",
      "suggestion": "Mention this paper when covering algorithm-based solutions to hardware fault tolerance in the context of deep learning accelerators.",
      "rating": 7
    },
    "sections": null
  },
  "wu_transom_2023": {
    "base": {
      "article_type": "misc",
      "title": "TRANSOM}: An Efficient Fault-Tolerant System for Training {LLMs",
      "authors": [
        "Wu, Baodong",
        "Xia, Lei",
        "Li, Qingping",
        "Li, Kangyu",
        "Chen, Xu",
        "Guo, Yongqiang",
        "Xiang, Tieyao",
        "Chen, Yuheng",
        "Li, Shigang"
      ],
      "abstract": "Large language models ({LLMs}) with hundreds of billions or trillions of parameters, represented by {chatGPT}, have achieved profound impact on various fields. However, training {LLMs} with super-large-scale parameters requires large highperformance {GPU} clusters and long training periods lasting for months. Due to the inevitable hardware and software failures in large-scale clusters, maintaining uninterrupted and long-duration training is extremely challenging. As a result, A substantial amount of training time is devoted to task checkpoint saving and loading, task rescheduling and restart, and task manual anomaly checks, which greatly harms the overall training efficiency.",
      "publication_year": "",
      "keywords": "Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing",
      "doi": ""
    },
    "suggestion": {
      "relation": "This paper addresses fault tolerance in the context of training large language models, focusing on system-level solutions rather than hardware-specific fault tolerance.",
      "suggestion": "Reference it for a broader perspective on fault tolerance challenges and solutions in training large-scale deep learning models.",
      "rating": 4
    },
    "sections": null
  },
  "he_understanding_2023": {
    "base": {
      "article_type": "inproceedings",
      "title": "nderstanding and Mitigating Hardware Failures in Deep Learning Training System",
      "authors": [
        "He, Yi",
        "Hutton, Mike",
        "Chan, Steven",
        "De Gruijl, Robert",
        "Govindaraju, Rama",
        "Patil, Nishant",
        "Li, Yanjing"
      ],
      "abstract": "Deep neural network ({DNN}) training workloads are increasingly susceptible to hardware failures in datacenters. For example, Google experienced \u201cmysterious, difficult to identify problems\" in their {TPU} training systems due to hardware failures [7]. Although these particular problems were subsequently corrected through significant efforts, they have raised the urgency of addressing the growing challenges emerging from hardware failures impacting many {DNN} training workloads.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1145/3579371.3589105"
    },
    "suggestion": {
      "relation": "Focuses on the susceptibility of DNN training workloads to hardware failures, providing insights into the challenges and solutions for hardware fault tolerance in deep learning systems.",
      "suggestion": "Cite this paper to discuss the impact of hardware failures on DNN training and the importance of fault tolerance.",
      "rating": 7
    },
    "sections": null
  },
  "li_understanding_2017": {
    "base": {
      "article_type": "inproceedings",
      "title": "nderstanding error propagation in deep learning neural network ({DNN}) accelerators and application",
      "authors": [
        "Li, Guanpeng",
        "Hari, Siva Kumar Sastry",
        "Sullivan, Michael",
        "Tsai, Timothy",
        "Pattabiraman, Karthik",
        "Emer, Joel",
        "Keckler, Stephen W."
      ],
      "abstract": "Deep learning neural networks ({DNNs}) have been successful in solving a wide range of machine learning problems. Specialized hardware accelerators have been proposed to accelerate the execution of {DNN} algorithms for high-performance and energy efficiency. Recently, they have been deployed in datacenters (potentially for business-critical or industrial applications) and safety-critical systems such as self-driving cars. Soft errors caused by high-energy particles have been increasing in hardware systems, and these can lead to catastrophic failures in {DNN} systems. Traditional methods for building resilient systems, e.g., Triple Modular Redundancy ({TMR}), are agnostic of the {DNN} algorithm and the {DNN} accelerator\u2019s architecture. Hence, these traditional resilience approaches incur high overheads, which makes them challenging to deploy. In this paper, we experimentally evaluate the resilience characteristics of {DNN} systems (i.e., {DNN} software running on specialized accelerators). We find that the error resilience of a {DNN} system depends on the data types, values, data reuses, and types of layers in the design. Based on our observations, we propose two efficient protection techniques for {DNN} systems.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1145/3126908.3126964"
    },
    "suggestion": {
      "relation": "Examines the resilience characteristics of DNN systems against soft errors in hardware accelerators, offering insights into efficient protection techniques tailored for DNN architectures.",
      "suggestion": "Use this paper to discuss the specific challenges and solutions for ensuring fault tolerance in DNN accelerators.",
      "rating": 8
    },
    "sections": null
  },
  "cherezova_understanding_2023": {
    "base": {
      "article_type": "article",
      "title": "nderstanding fault-tolerance vulnerabilities in advanced {SoC} {FPGAs} for critical application",
      "authors": [
        "Cherezova, Natalia",
        "Shibin, Konstantin",
        "Jenihhin, Maksim",
        "Jutman, Artur"
      ],
      "abstract": "The emergence of heterogeneous {FPGA}-based {SoCs} and their growing complexity fueled by the introduction of various accelerators bring the reliability aspect of these systems to the front. The concern is particularly important for critical applications, such as safety-critical autonomous vehicles, real-time systems, space missions, health, and security cyber\u2013physical systems. This paper presents an analysis of the most common types of errors caused by faults in different components and identifies the most critical and vulnerable components based on the literature survey. The study looks into vendor-provided and user-space reliability solutions. The paper highlights the existing fault-tolerance gaps in the modern {SoC} {FPGAs} and outlines a vision for future solutions.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.microrel.2023.115010"
    },
    "suggestion": {
      "relation": "Analyzes fault-tolerance vulnerabilities in SoC FPGAs, relevant for understanding the reliability challenges in hardware accelerators used for deep learning in critical applications.",
      "suggestion": "Reference this paper to highlight the reliability and fault tolerance challenges in modern FPGA-based systems for deep learning.",
      "rating": 7
    },
    "sections": null
  },
  "liu_using_2022": {
    "base": {
      "article_type": "article",
      "title": "sing checksum to improve the reliability of embedded convolutional neural network",
      "authors": [
        "Liu, Zhi",
        "Deng, Zhen",
        "Yang, Xinni"
      ],
      "abstract": "Convolutional Neural Networks ({CNNs}) have been used in resource-constrained edge devices, including safetycritical applications. However, recent studies demonstrate that soft errors may result in {CNN} prediction failures. Many approaches are proposed at the end of layers to detect computation errors. Still, these approaches incur severe overheads or cannot detect errors in memory, e.g., traditional Double Modular Redundancy ({DMR}) can protect data integrity in memory, but it incurs significant overheads on time and memory. Some existing {DNNspecific} approaches are lightweight to protect {DNNs} from transient faults but cannot provide high fault coverage to detect permanent faults in memory. Thus, we propose two low-cost software approaches, channel-based weight checksum and layer-based output checksum, which protect the integrity of weights and layer outputs in memory. We perform a large number of fault injections to evaluate the performance of the proposed ap\u00ad proaches and compare them with other approaches. The results show that combining the proposed approaches can provide approximately 100 \\% fault coverage for weights and layer outputs in memory. The combination increases total time and memory overheads by 4 \\% and 17 \\%, respectively, for {DNNs} that run sequentially. In addition, we observe that the layer-based output checksum can improve the error resilience of register files because most {SDCs} caused by register file errors are induced by the control flow error and the pointer error. Finally, we present a case study on {MobileNet} to show that our approaches can be generalized to other {DNN} libraries that run parallel by dividing the single-layer execution into N parts. When N is 2, our approaches in\u00ad crease total time and memory overheads by 7.4 \\% and 31 \\%, respectively.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.microrel.2022.114666"
    },
    "suggestion": {
      "relation": "This paper introduces software approaches for enhancing the fault tolerance of CNNs in memory, specifically targeting the integrity of weights and layer outputs, which is directly relevant to hardware error fault tolerance in deep learning.",
      "suggestion": "Cite this paper to discuss innovative software-level solutions for improving hardware fault tolerance in deep learning systems.",
      "rating": 8
    },
    "sections": null
  },
  "xue_winograd_2022": {
    "base": {
      "article_type": "misc",
      "title": "inograd Convolution: A Perspective from Fault Toleranc",
      "authors": [
        "Xue, Xinghua",
        "Huang, Haitong",
        "Liu, Cheng",
        "Wang, Ying",
        "Luo, Tao",
        "Zhang, Lei"
      ],
      "abstract": "Winograd convolution is originally proposed to reduce the computing overhead by converting multiplication in neural network ({NN}) with addition via linear transformation. Other than the computing efficiency, we observe its great potential in improving {NN} fault tolerance and evaluate its fault tolerance comprehensively for the first time. Then, we explore the use of fault tolerance of winograd convolution for either fault-tolerant or energy-efficient {NN} processing. According to our experiments, winograd convolution can be utilized to reduce fault-tolerant design overhead by 27.49\\% or energy consumption by 7.19\\% without any accuracy loss compared to that without being aware of the fault tolerance.",
      "publication_year": "",
      "keywords": "Computer Science - Hardware Architecture, Computer Science - Machine Learning, Computer Science - Performance",
      "doi": ""
    },
    "suggestion": {
      "relation": "The paper explores the fault tolerance potential of Winograd convolution in neural networks, offering a novel perspective on reducing fault-tolerant design overhead and energy consumption without accuracy loss.",
      "suggestion": "Reference this paper to illustrate the fault tolerance benefits of Winograd convolution in deep learning architectures.",
      "rating": 7
    },
    "sections": null
  },
  "vafaei_x-rel_2023": {
    "base": {
      "article_type": "article",
      "title": "-Rel: Energy-Efficient and Low-Overhead Approximate Reliability Framework for Error-Tolerant Applications Deployed in Critical System",
      "authors": [
        "Vafaei, Jafar",
        "Akbari, Omid",
        "Shafique, Muhammad",
        "Hochberger, Christian"
      ],
      "abstract": "Triple modular redundancy ({TMR}) is one of the most common techniques in fault-tolerant systems, in which the output is determined by a majority voter. However, the design diversity of replicated modules and/or soft errors that are more likely to happen in the nanoscale era may affect the majority voting scheme. Besides, the significant overheads of the {TMR} scheme may limit its usage in energy consumption and area-constrained critical systems. However, for most inherently error-resilient applications such as image processing and vision deployed in critical systems (such as autonomous vehicles and robotics), achieving a given level of reliability has more priority than precise results. Therefore, these applications can benefit from the approximate computing paradigm to achieve higher energy efficiency and a lower area. This article proposes an energy-efficient approximate reliability (X-Rel) framework to overcome the aforementioned challenges of the {TMR} systems and get the full potential of approximate computing without sacrificing the desired reliability constraint and output quality. The X-Rel framework relies on relaxing the precision of the voter based on a systematical error bounding method that leverages user-defined quality and reliability constraints. Afterward, the size of the achieved voter is used to approximate the {TMR} modules such that the overall area and energy consumption are minimized. The effectiveness of employing the proposed X-Rel technique in a {TMR} structure, for different quality constraints as well as with various reliability bounds, is evaluated in a 15-nm {FinFET} technology. The results of the X-Rel voter show delay, area, and energy consumption reductions of up to 86\\%, 87\\%, and 98\\%, respectively, when compared to those of the state-of-the-art approximate {TMR} voters. Also, the effectiveness of the proposed X-Rel-based {TMR} structure is assessed in four benchmark applications from different domains. For these benchmarks, the results show 1.59\u00d7, 2.35\u00d7, and 3.39\u00d7 energydelay-area-product ({EDAP}) reduction for less than 1\\%, 5\\%, and 10\\% output quality degradations, respectively. Finally, an image processing application is benchmarked to evaluate the X-Rel framework efficacy in the presence of errors, where the results show up to a 4.78\u00d7 higher output image quality in comparison with the typical {TMR} voters.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/TVLSI.2023.3272226"
    },
    "suggestion": {
      "relation": "This article proposes an energy-efficient framework for enhancing the reliability of error-tolerant applications in critical systems, leveraging approximate computing, which indirectly supports fault tolerance in deep learning through efficient redundancy.",
      "suggestion": "Use this paper to discuss the broader implications of approximate computing on fault tolerance in critical deep learning applications.",
      "rating": 5
    },
    "sections": null
  },
  "burel_zero-overhead_2021": {
    "base": {
      "article_type": "inproceedings",
      "title": "ero-Overhead Protection for {CNN} Weight",
      "authors": [
        "Burel, Stephane",
        "Evans, Adrian",
        "Anghel, Lorena"
      ],
      "abstract": "The numerical format used for representing weights and activations plays a key role in the computational ef\ufb01ciency and robustness of {CNNs}. Recently, a 16-bit \ufb02oating point format called Brain-Float 16 (bf16) has been proposed and implemented in hardware accelerators. However, the robustness of accelerators implemented with this format has not yet been studied. In this paper, we perform a comparison of the robustness of state-of-the art {CNNs} implemented with 8-bit integer, Brain-Float 16 and 32bit \ufb02oating point formats. We also introduce an error detection and masking technique, called opportunistic parity ({OP}), which can detect and mask errors in the weights with zero storage overhead. With this technique, the robustness of \ufb02oating point weights to bit-\ufb02ips can be improved by up to three orders of magnitude.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/DFT52944.2021.9568363"
    },
    "suggestion": {
      "relation": "It introduces a zero-overhead error detection and masking technique for CNN weights, directly addressing the challenge of hardware fault tolerance in deep learning without additional storage costs.",
      "suggestion": "Cite this paper for examples of practical, low-overhead techniques for enhancing the robustness of deep learning models against hardware errors.",
      "rating": 8
    },
    "sections": null
  },
  "demirkiran_blueprint_2023": {
    "base": {
      "article_type": "misc",
      "title": " Blueprint for Precise and Fault-Tolerant Analog Neural Network",
      "authors": [
        "Demirkiran, Cansu",
        "Nair, Lakshmi",
        "Bunandar, Darius",
        "Joshi, Ajay"
      ],
      "abstract": "Analog computing has reemerged as a promising avenue for accelerating deep neural networks ({DNNs}) due to its potential to overcome the energy efficiency and scalability challenges posed by traditional digital architectures. However, achieving high precision and {DNN} accuracy using such technologies is challenging, as highprecision data converters are costly and impractical. In this paper, we address this challenge by using the residue number system ({RNS}). {RNS} allows composing highprecision operations from multiple low-precision operations, thereby eliminating the information loss caused by the limited precision of the data converters. Our study demonstrates that analog accelerators utilizing the {RNS}-based approach can achieve \u226599\\% of {FP}32 accuracy for state-of-the-art {DNN} inference using data converters with only 6-bit precision whereas a conventional analog core requires more than 8-bit precision to achieve the same accuracy in the same {DNNs}. The reduced precision requirements imply that using {RNS} can reduce the energy consumption of analog accelerators by several orders of magnitude while maintaining the same throughput and precision. Our study extends this approach to {DNN} training, where we can efficiently train {DNNs} using 7-bit integer arithmetic while achieving accuracy comparable to {FP}32 precision. Lastly, we present a faulttolerant dataflow using redundant {RNS} error-correcting codes to protect the computation against noise and errors inherent within an analog accelerator.",
      "publication_year": "",
      "keywords": "Computer Science - Artificial Intelligence, Computer Science - Emerging Technologies, Computer Science - Hardware Architecture",
      "doi": ""
    },
    "suggestion": {
      "relation": "The paper addresses precision and fault tolerance in analog neural networks using the residue number system, offering insights into overcoming hardware limitations in deep learning accelerators.",
      "suggestion": "Reference this study to discuss fault tolerance strategies in the emerging field of analog deep learning accelerators.",
      "rating": 6
    },
    "sections": null
  },
  "liu_cross-layer_2021": {
    "base": {
      "article_type": "article",
      "title": " cross-layer fault propagation analysis method for edge intelligence systems deployed with {DNNs",
      "authors": [
        "Liu, Ting",
        "Fu, Yuzhuo",
        "Xu, Xiaotong",
        "Yan, Wei"
      ],
      "abstract": "To evaluate the impact of soft errors on convolutional neural networks ({CNNs}) deployed in edged computation systems, we propose a data-driven assessment strategy to characterize the propagation flow across hardware and software abstraction layers of the system in an interpretable way. Single-bit-flip injections in underlying hardware architecture are performed on virtual embedded system with a {CNN}-based image classifier deployed on it. We depict the local activation and global dependencies caused by soft errors across the system in form of a directed acyclic graph by using generative adversarial networks and Bayesian networks as data modeling methods. The cross-layer fault propagation paths and component sensitivities show that the deep neural networks like {CNNs} can effectively prevent the faults that may cause critical failures from propagating to the system output via the channel sparsity and regular pooling mechanism in the network pipelines.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.sysarc.2021.102057"
    },
    "suggestion": {
      "relation": "This work evaluates the impact of soft errors on CNNs in edge systems, providing a data-driven approach to understanding fault propagation, which is crucial for developing fault-tolerant deep learning systems.",
      "suggestion": "Use this paper to highlight the importance of understanding fault propagation in the design of fault-tolerant deep learning systems.",
      "rating": 7
    },
    "sections": null
  },
  "nabavi_fault-tolerant_2023": {
    "base": {
      "article_type": "article",
      "title": " fault-tolerant resource locking protocol for multiprocessor real-time system",
      "authors": [
        "Nabavi, Seyede Sahebeh",
        "Farbeh, Hamed"
      ],
      "abstract": "This paper presents the first fault-tolerant resource locking protocol for multiprocessor real-time systems. Resource locking protocols control access to shared resources in real-time systems. Most of the previous studies in resource locking protocols have focused on decreasing blocking time and resource conflicts. The use of such protocols in multiprocessor safety-critical real-time systems necessitates fault tolerance in the design of these systems. However, in research focused on multiprocessor real-time systems, fault tolerance has received low attention in resource allocation studies. In addition, many solutions proposed to improve the reliability in realtime scheduling ignored resource models in their task model. This paper proposes a novel fault-tolerant resource locking protocol, called {FTPIA}-{NPCS}, which considers transient faults in shared resources. {FTPIA}-{NPCS} proposes a checkpointing-based fault-tolerant mechanism to recover from errors caused by transient faults. In this mechanism, a checkpoint is taken from the tasks, before each resource allocation. In addition to providing fault tolerance, {FTPIA}-{NPCS} solves the problem of priority inversion in {NPCS} protocol for periodic tasks with hard deadlines. The simulation results demonstrate that {FTPIA}-{NPCS} tolerates at least one transient fault in resources for each execution of the periodic tasks with 15\\% blocking time overhead in comparison to its non-fault-tolerant configuration, called {PIA}-{NPCS}. Furthermore, the blocking time of higher-priority tasks is minimized in com\u00ad parison to {PIA}-{NPCS}. An analytical evaluation of the protocol supports the simulation results.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.mejo.2023.105809"
    },
    "suggestion": {
      "relation": "Although focused on real-time systems, this paper's introduction of a fault-tolerant resource locking protocol indirectly relates to ensuring reliability in multiprocessor systems that could be used for deep learning.",
      "suggestion": "Mention this paper to discuss fault tolerance in the broader context of multiprocessor systems, with potential implications for deep learning.",
      "rating": 3
    },
    "sections": null
  },
  "tsounis_methodology_2023": {
    "base": {
      "article_type": "article",
      "title": " Methodology for Fault-tolerant Pareto-optimal Approximate Designs of {FPGA}-based Accelerator",
      "authors": [
        "Tsounis, Ioannis",
        "Agiakatsikas, Dimitris",
        "Psarakis, Mihalis"
      ],
      "abstract": "Approximate Computing Techniques ({ACTs}) take advantage of resilience computing applications to trade off among output precision, area, power, and performance. {ACTs} can lead to significant gains at affordable costs when efficiently implemented on Field Programmable Gate Array\u2013 ({FPGA}) based accelerators. Although several novel {ACTs} works have been proposed for {FPGA} accelerators, their applicability to high-assurance systems has not been explored as much. {ACTs} are becoming necessary in many critical Edge computing systems, such as self-driving cars and Earth observation satellites, to increase computational efficiency. However, an important question comes to mind when targeting critical systems: Does {ACT} optimization negatively affect the reliability of the system and how can one find optimal design architectures that blend classic mitigation techniques like Triple Modular Redundancy with approximation- and precise-based arithmetic hardware units to achieve the best possible computational efficiency without compromising dependability? This work aims to solve this research problem by introducing a Design Space Exploration ({DSE}) methodology that employs {ACTs} in arithmetic units of the design and identifies Pareto-optimal microarchitectures that balance all relevant gains of {ACTs}, such as area, speed, power, failure rate, and precision, by inserting the correct amount of approximation in the design. In a nutshell, our {DSE} methodology has formulated the {DSE} with a Multi-Objective Optimization Problem ({MOP}). Each Pareto-optimal solution of our tool finds which arithmetic units of the design to implement with precise and approximate circuits and which units to selectively triplicate to remove single points of failure that compromise system reliability below acceptable thresholds. We also suggest another formulation of the {DSE} into a Single-Objective constraint Optimization Problem ({ScOP}) producing a single optimal point, and that the user may demand, as a less time-consuming alternative to the {MOP} if a complete Pareto-front is not needed. Our methodology generates fault-tolerant versions of the Pareto-optimal approximate designs (or simple optimized approximate designs if the {ScOP} choice is picked) by selectively applying mitigation techniques in a way that the overheads of redundant resources for fault-tolerance do not negate the gains of approximation in comparison to the fault-tolerant versions of the precise design. We evaluate our method on two {FPGA}-based accelerators: a {JPEG} encoder and an H.264/Advanced Video Coding decoder. Our experimental results show significant gains in area, frequency, and power consumption without compromising output quality and system reliability compared to classic solutions that replicate all or a part of the resources of the precise design to increase dependability metrics.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1145/3568021"
    },
    "suggestion": {
      "relation": "It presents a methodology for achieving fault-tolerant, Pareto-optimal approximate designs in FPGA-based accelerators, directly relevant to designing efficient and reliable deep learning hardware.",
      "suggestion": "Cite this paper for discussions on optimizing FPGA-based accelerators for fault tolerance in deep learning applications.",
      "rating": 7
    },
    "sections": null
  },
  "jafarzadeh_novel_2023": {
    "base": {
      "article_type": "article",
      "title": " novel buffering fault\u2010tolerance approach for network on chip ({NoC}",
      "authors": [
        "Jafarzadeh, Nima",
        "Jalili, Ahmad",
        "Alzubi, Jafar A.",
        "Rezaee, Khosro",
        "Liu, Yang",
        "Gheisari, Mehdi",
        "Sadeghi Bigham, Bahram",
        "Javadpour, Amir"
      ],
      "abstract": "Network\u2010on\u2010Chip ({NoC}) is a key component in chip multiprocessors ({CMPs}) as it supports communication between many cores. {NoC} is a network\u2010based communication subsystem on an integrated circuit, most typically between modules in a system on a chip ({SoC}). Designing a reliable {NoC} against failures that can prevent failure using some measures or preventing error or system failure while failure happens and proper performance became a significant concern. For a reliable design against failures, first, the system should be analysed to discover the critical points. Hence, in this research, it is tried first to investigate the scale of fault tolerance effect on the mechanism in the router on the network by injecting simulated errors, and then these errors are prevented. As the major novelty, the authors implemented a router on a synchronised network and calculated the network buffering fault tolerance by injecting error in the buffer. Specifically, a new method for improving fault tolerance is proposed, which uses the existing resources efficiently. So, it does not impose any overhead on hardware and improves the error tolerance scale. The authors also evaluate it from different perspectives to show its superior performance.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1049/cds2.12127"
    },
    "suggestion": {
      "relation": "This paper focuses on enhancing the fault tolerance of Network-on-Chip (NoC), a crucial component in chip multiprocessors, which indirectly supports deep learning computations by improving communication reliability.",
      "suggestion": "Cite this paper to discuss the importance of NoC reliability in supporting deep learning applications.",
      "rating": 5
    },
    "sections": null
  },
  "zhang_scalable_2023": {
    "base": {
      "article_type": "misc",
      "title": " Scalable, Fast and Programmable Neural Decoder for Fault-Tolerant Quantum Computation Using Surface Code",
      "authors": [
        "Zhang, Mengyu",
        "Ren, Xiangyu",
        "Xi, Guanglei",
        "Zhang, Zhenxing",
        "Yu, Qiaonian",
        "Liu, Fuming",
        "Zhang, Hualiang",
        "Zhang, Shengyu",
        "Zheng, Yi-Cong"
      ],
      "abstract": "Quantum error-correcting codes ({QECCs}) can eliminate the negative effects of quantum noise, the major obstacle to the execution of quantum algorithms. However, realizing practical quantum error correction ({QEC}) requires resolving many challenges to implement a high-performance real-time decoding system. Many decoding algorithms have been proposed and optimized in the past few decades, of which neural network ({NNs}) based solutions have drawn an increasing amount of attention due to their effectiveness and high efficiency. Unfortunately, previous works on neural decoders are still at an early stage and have only relatively simple architectures, which makes them unsuitable for practical fault-tolerant quantum error correction ({FTQEC}).",
      "publication_year": "",
      "keywords": "Computer Science - Hardware Architecture, Quantum Physics",
      "doi": ""
    },
    "suggestion": {
      "relation": "Although focused on quantum computing, this paper's exploration of neural network-based solutions for fault tolerance presents an interesting parallel to hardware fault tolerance in deep learning.",
      "suggestion": "Reference this paper for a broader perspective on fault tolerance techniques, including those outside traditional deep learning hardware.",
      "rating": 3
    },
    "sections": null
  },
  "wang_survey_2023": {
    "base": {
      "article_type": "article",
      "title": " Survey of Reliability Issues Related to Approximate Circuit",
      "authors": [
        "Wang, Zhen",
        "Xu, Rong-Chen",
        "Chen, Jia-Cheng",
        "Xiao, Jie"
      ],
      "abstract": "As one of the most promising paradigms of integrated circuit design, the approximate circuit has aroused widespread concern in the scientific community. It takes advantage of the inherent error tolerance of some applications and relaxes the accuracy for reductions in area and power consumption. This paper aims to provide a comprehensive survey of reliability issues related to approximate circuits, which covers three concerns: error characteristic analysis, reliability and test, and reliable design involving approximate circuits. The error characteristic analysis is used to compare the outputs of the approximate circuit with those of its precise counterpart, which can help to find the most appropriate approximate design for a specific application in the large design space. With the approximate design getting close to physical realization, manufacturing defects and operational faults are inevitable; therefore, the reliability prediction and vulnerability test become increasingly important. However, the research on approximate circuit reliability and test is insufficient and needs more attention. Furtherly, although there is some existing work combining the approximate design with fault tolerant techniques, the reliability-enhancement approaches for approximate circuits are lacking.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1007/s11390-023-2554-x"
    },
    "suggestion": {
      "relation": "This paper surveys reliability issues in approximate circuits, relevant for deep learning as it touches on error tolerance and fault-tolerant design in hardware that could be used for deep learning computations.",
      "suggestion": "Use this paper to discuss the intersection of approximate computing and deep learning hardware fault tolerance.",
      "rating": 6
    },
    "sections": null
  },
  "smith_survey_nodate": {
    "base": {
      "article_type": "article",
      "title": " Survey of Software Fault Tolerance Technique",
      "authors": [
        "Smith, Jonathan M"
      ],
      "abstract": "This report examines the state of the field of software fault tolerance. Terminology, techniques for building reliable systems, and fault tolerance are discussed. While a scientific consensus on the measurement of software reliability has not been reached, software systems are sufficiently pervasive that \u2018\u2018software components\u2019\u2019 of larger systems must be reliable, since dependence is placed on them. Fault tolerant systems utilize redundant components to mitigate the effects of component failures, and thus create a system which is more reliable than a single component. This idea can be applied to software systems as well. Several techniques for designing fault tolerant software systems are discussed and assessed qualitatively, where \u2018\u2018software fault\u2019\u2019 refers to what is more commonly known as a bug. The assumptions, relative merits, available experimental results, and implementation experience are discussed for each technique. This leads us to some conclusions about the state of the field.",
      "publication_year": "",
      "keywords": "",
      "doi": ""
    },
    "suggestion": {
      "relation": "Focuses on software fault tolerance techniques, which, while related to reliability, diverge from the hardware-centric approach required for deep learning computational error avoidance.",
      "suggestion": "Mention this paper to contrast software and hardware fault tolerance strategies, emphasizing the review's focus on hardware.",
      "rating": 2
    },
    "sections": null
  },
  "eslami_survey_2020": {
    "base": {
      "article_type": "article",
      "title": " survey on fault injection methods of digital integrated circuit",
      "authors": [
        "Eslami, Mohammad",
        "Ghavami, Behnam",
        "Raji, Mohsen",
        "Mahani, Ali"
      ],
      "abstract": "One of the most popular methods for reliability assessment of digital circuits is Fault Injection ({FI}) in which the behavior of the circuit is simulated in presence of faults. In this paper, we present a survey of {FI} techniques as well as classifying these techniques considering different aspects and criteria to bring out their similarities and differences. The goal of this paper is to help the researchers and reliable circuit designers in gaining insights into the state-of-art in {FI} techniques and motivate them to further improve these techniques for more ef\ufb01cient reliability evaluation of digital circuit designs of tomorrow.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.vlsi.2019.11.006"
    },
    "suggestion": {
      "relation": "Discusses fault injection methods for assessing digital circuit reliability, directly applicable to evaluating and designing fault-tolerant hardware for deep learning.",
      "suggestion": "Cite this paper to cover fault injection as a method for testing and improving hardware fault tolerance in deep learning systems.",
      "rating": 7
    },
    "sections": null
  },
  "syed_survey_2023": {
    "base": {
      "article_type": "article",
      "title": " Survey on Fault-Tolerant Methodologies for Deep Neural Network",
      "authors": [
        "Syed, Rizwan",
        "Ulbricht, Markus",
        "Piotrowski, Krzysztof",
        "Krstic, Milos"
      ],
      "abstract": "Asignificant rise in Artificial Intelligence ({AI}) has impacted many applications around us, so much so that {AI} has now been increasingly used in safety-critical applications. {AI} at the edge is the reality, which means performing the data computation closer to the source of the data, as opposed to performing it on the cloud. Safety-critical applications have strict reliability requirements; therefore, it is essential that {AI} models running on the edge (i.e., hardware) must fulfill the required safety standards. In the vast field of {AI}, Deep Neural Networks ({DNNs}) are the focal point of this survey as it has continued to produce extraordinary outcomes in various applications .i.e medical, automotive, aerospace, defense, etc. Traditional reliability techniques for {DNNs} implementation are not always practical, as they fail to exploit the unique characteristics of the {DNNs}. Furthermore, it is also essential to understand the targeted edge hardware because the impact of the faults can be different in {ASICs} and {FPGAs}. Therefore, in this survey, first, we have examined the impact of the fault in {ASICs} and {FPGAs}, and then we seek to provide a glimpse of the recent progress made towards the fault-tolerant {DNNs}. We have discussed several factors that can impact the reliability of the {DNNs}. Further, we have extended this discussion to shed light on many state-of-the-art fault mitigation techniques for {DNNs}.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.14313/PAR_248/89"
    },
    "suggestion": {
      "relation": "Directly addresses fault-tolerant methodologies for Deep Neural Networks (DNNs), focusing on hardware aspects crucial for the literature review's topic on avoiding computational errors in deep learning.",
      "suggestion": "Highly recommend citing this paper as it directly correlates with the review's aim of exploring hardware fault tolerance in deep learning.",
      "rating": 9
    },
    "sections": null
  },
  "mittal_survey_2020": {
    "base": {
      "article_type": "article",
      "title": " survey on modeling and improving reliability of {DNN} algorithms and accelerator",
      "authors": [
        "Mittal, Sparsh"
      ],
      "abstract": "As {DNNs} become increasingly common in mission-critical applications, ensuring their reliable operation has become crucial. Conventional resilience techniques fail to account for the unique characteristics of {DNN} algorithms/accelerators, and hence, they are infeasible or ineffective. In this paper, we present a survey of techniques for studying and optimizing the reliability of {DNN} accelerators and architectures. The reliability issues we cover include soft/hard errors arising due to process variation, voltage scaling, timing errors, {DRAM} errors due to refresh rate scaling and thermal effects, etc. We organize the research projects on several categories to bring out their key attributes. This paper underscores the importance of designing for reliability as the first principle, and not merely retrofit for it.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1016/j.sysarc.2019.101689"
    },
    "suggestion": {
      "relation": "Surveys techniques for optimizing the reliability of DNN accelerators and architectures, which is central to the literature review's focus on hardware error fault tolerance in deep learning.",
      "suggestion": "This paper should be a primary citation for discussions on DNN accelerator and architecture reliability.",
      "rating": 9
    },
    "sections": null
  },
  "ahmadilivani_systematic_2023": {
    "base": {
      "article_type": "misc",
      "title": " Systematic Literature Review on Hardware Reliability Assessment Methods for Deep Neural Network",
      "authors": [
        "Ahmadilivani, Mohammad Hasan",
        "Taheri, Mahdi",
        "Raik, Jaan",
        "Daneshtalab, Masoud",
        "Jenihhin, Maksim"
      ],
      "abstract": "Artificial Intelligence ({AI}) and, in particular, Machine Learning ({ML}) have emerged to be utilized in various applications due to their capability to learn how to solve complex problems. Over the last decade, rapid advances in {ML} have presented Deep Neural Networks ({DNNs}) consisting of a large number of neurons and layers. {DNN} Hardware Accelerators ({DHAs}) are leveraged to deploy {DNNs} in the target applications. Safety-critical applications, where hardware faults/errors would result in catastrophic consequences, also benefit from {DHAs}. Therefore, the reliability of {DNNs} is an essential subject of research. In recent years, several studies have been published accordingly to assess the reliability of {DNNs}. In this regard, various reliability assessment methods have been proposed on a variety of platforms and applications. Hence, there is a need to summarize the state of the art to identify the gaps in the study of the reliability of {DNNs}. In this work, we conduct a Systematic Literature Review ({SLR}) on the reliability assessment methods of {DNNs} to collect relevant research works as much as possible, present a categorization of them, and address the open challenges. Through this {SLR}, three kinds of methods for reliability assessment of {DNNs} are identified including Fault Injection ({FI}), Analytical, and Hybrid methods. Since the majority of works assess the {DNN} reliability by {FI}, we characterize different approaches and platforms of the {FI} method comprehensively. Moreover, Analytical and Hybrid methods are propounded. Thus, different reliability assessment methods for {DNNs} have been elaborated on their conducted {DNN} platforms and reliability evaluation metrics. Finally, we highlight the advantages and disadvantages of the identified methods and address the open challenges in the research area.",
      "publication_year": "",
      "keywords": "Computer Science - Artificial Intelligence, Computer Science - Hardware Architecture, Computer Science - Machine Learning",
      "doi": ""
    },
    "suggestion": {
      "relation": "Conducts a systematic review on hardware reliability assessment methods for DNNs, identifying key methods and challenges, which aligns closely with the literature review's topic.",
      "suggestion": "Cite this paper to provide a comprehensive overview of current reliability assessment methods for DNN hardware.",
      "rating": 8
    },
    "sections": null
  },
  "lari_co-design_2015": {
    "base": {
      "article_type": "inproceedings",
      "title": " co-design approach for fault-tolerant loop execution on Coarse-Grained Reconfigurable Array",
      "authors": [
        "Lari, Vahid",
        "Tanase, Alexandru",
        "Teich, Jurgen",
        "Witterauf, Michael",
        "Khosravi, Faramarz",
        "Hannig, Frank",
        "Meyer, Brett H."
      ],
      "abstract": "We present a co-design approach to establish redundancy schemes such as Dual Modular Redundancy ({DMR}) and Triple Modular Redundancy ({TMR}) to a whole region of a processor array for a class of Coarse-Grained Recon\ufb01gurable Arrays ({CGRAs}). The approach is applied to applications with mixed-criticality properties and experiencing varying Soft Error Rates ({SERs}) due to environmental reasons, e. g., changing altitude. The core idea is to adapt the degree of fault protection for loop programs executing in parallel on a {CGRA} to the level of reliability required as well as {SER} pro\ufb01les. This is realized through claiming neighbor regions of processing elements for the execution of replicated loop nests. First, at the source code level, a compiler transformation is proposed that realizes these replication schemes in two steps: (1) replicate given parallel loop program two or three times for {DMR} or {TMR}, respectively, and (2) add appropriate error handling functions (voting or comparison) in order to detect respectively correct any single errors. Then, using the opportunities of hardware/software co-design, we propose optimized implementations of the error handling functions in software as well as in hardware. Finally, experimental results are given for the analysis of reliability gains for each proposed scheme of array replication in dependence of different {SERs}.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/AHS.2015.7231157"
    },
    "suggestion": {
      "relation": "This paper explores redundancy schemes for fault tolerance in Coarse-Grained Reconfigurable Arrays (CGRAs), focusing on adapting fault protection levels for loop programs based on Soft Error Rates (SERs), which is directly related to hardware fault tolerance in computing systems.",
      "suggestion": "Cite this paper to discuss redundancy schemes like DMR and TMR in the context of hardware fault tolerance for deep learning, emphasizing the adaptability to varying SERs.",
      "rating": 7
    },
    "sections": null
  },
  "cui_diagonal_2020": {
    "base": {
      "article_type": "inproceedings",
      "title": " Diagonal Checksum Algorithm-Based Fault Tolerance for Parallel Matrix Multiplicatio",
      "authors": [
        "Cui, Ke",
        "Koibuchi, Michihiro"
      ],
      "abstract": "Algorithm-based fault tolerance ({ABFT}) has been proposed to complete a program on an unreliable computer that would generate bit \ufb02ips. A famous {ABFT} technique is to insert checksums into a parallel matrix multiplication, including {LUdecomposition}. It can correct a single incorrect element caused by a bit \ufb02ip in computation and communication. To correct two incorrect elements, we extend the above {ABFT}, named diagonal checksum, to a parallel matrix multiplier in this study. Simulation results show that the diagonal checksum {ABFT} method has a similar latency overhead to the famous {ABFT} on a parallel computer using an unreliable interconnection network. They also show that the diagonal checksum {ABFT} method works well even when an interconnection network has 10\u22127 of bit error rate.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1109/CANDARW51189.2020.00050"
    },
    "suggestion": {
      "relation": "The study introduces an Algorithm-based Fault Tolerance (ABFT) method for parallel matrix multiplication, aiming to correct computational errors due to bit flips, which aligns with the theme of hardware error fault tolerance in deep learning computations.",
      "suggestion": "Reference this paper to illustrate how ABFT, particularly the diagonal checksum method, can be applied to ensure fault tolerance in deep learning hardware.",
      "rating": 6
    },
    "sections": null
  },
  "_double_nodate": {
    "base": {
      "article_type": "article",
      "title": "ouble fault tolerant array code with low compilation complexit",
      "authors": [
        "\u89e3, \u5ce5"
      ],
      "abstract": "\u7ea0\u5220\u7801\u6280\u672f\u662f\u72ec\u7acb\u78c1\u76d8\u5197\u4f59\u9635\u5217-6\uff08{RAID}-6\uff09\u7684\u53cc\u5bb9\u9519\u80fd\u529b\u7684\u5e95\u5c42\u5b9e\u73b0\u6280\u672f\uff0c\u5b83\u7684\u6027\u80fd\u662f\u5de6\u53f3 {RAID}-6 \u6027 \u80fd\u7684\u91cd\u8981\u56e0\u7d20\u3002\u9488\u5bf9 {RAID}-6 \u4e2d\u5e38\u7528\u9635\u5217\u7ea0\u5220\u7801\u7684 I/O \u4e0d\u5e73\u8861\u548c\u6570\u636e\u6062\u590d\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f02\u6216\uff08{XOR}\uff09\u7684 \u6df7\u5408\u9635\u5217\u7801\u2014\u2014J \u7801\uff08J-code\uff09\u3002J-code \u91c7\u7528\u65b0\u7684\u6821\u9a8c\u751f\u6210\u89c4\u5219\uff0c\u9996\u5148\uff0c\u5229\u7528\u539f\u59cb\u6570\u636e\u6784\u9020\u7684\u4e8c\u7ef4\u9635\u5217\u8ba1\u7b97\u51fa\u5bf9\u89d2\u6821\u9a8c\u4f4d \u5e76\u6784\u9020\u65b0\u7684\u9635\u5217\uff1b\u7136\u540e\uff0c\u5229\u7528\u65b0\u9635\u5217\u4e2d\u6570\u636e\u5757\u4e4b\u95f4\u7684\u4f4d\u7f6e\u5173\u7cfb\u8ba1\u7b97\u5f97\u5230\u53cd\u5bf9\u89d2\u6821\u9a8c\u4f4d\u3002\u6b64\u5916\uff0cJ-code \u5c06\u539f\u59cb\u6570\u636e\u4e0e\u90e8 \u5206\u6821\u9a8c\u4f4d\u5b58\u50a8\u4e8e\u540c\u4e00\u78c1\u76d8\uff0c\u80fd\u51cf\u5c11\u7f16\u8bd1\u7801\u8fc7\u7a0b\u4e2d\u7684\u5f02\u6216\uff08{XOR}\uff09\u64cd\u4f5c\u6b21\u6570\u548c\u5355\u76d8\u6062\u590d\u8fc7\u7a0b\u4e2d\u8bfb\u53d6\u6570\u636e\u5757\u7684\u4e2a\u6570\uff0c\u4ece\u800c \u964d\u4f4e\u7f16\u8bd1\u7801\u590d\u6742\u5ea6\u548c\u5355\u76d8\u6545\u969c\u4fee\u590d\u7684 I/O \u6210\u672c\uff0c\u7f13\u89e3\u78c1\u76d8\u70ed\u70b9\u96c6\u4e2d\u73b0\u8c61\u3002\u4eff\u771f\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u8f83\u4e8e{DP}\uff08{RowDiagonal} Parity\uff09\u3001{EaR}\uff08Endurance-aware {RAID}-6\uff09\u7b49\u9635\u5217\u7801\uff0cJ-code \u7684\u7f16\u7801\u65f6\u95f4\u51cf\u5c11\u4e86 0. 30\\%{\\textasciitilde}28. 70\\%\uff0c\u5355\u78c1\u76d8\u6545\u969c\u548c\u53cc \u78c1\u76d8\u6545\u969c\u7684\u4fee\u590d\u7528\u65f6\u5206\u522b\u51cf\u5c11\u4e86 2. 23\\%{\\textasciitilde}31. 62\\% \u548c 0. 39\\%{\\textasciitilde}36. 00\\%\u3002",
      "publication_year": "",
      "keywords": "",
      "doi": ""
    },
    "suggestion": {
      "relation": "Although focusing on RAID-6 array codes for error correction and fault tolerance, this paper's relevance to deep learning hardware fault tolerance is indirect, as it deals with data storage rather than computational error correction.",
      "suggestion": "Mention this study as an example of fault tolerance in data storage systems, noting the difference in application compared to deep learning computational hardware.",
      "rating": 3
    },
    "sections": null
  },
  "nadeem_fault_2023": {
    "base": {
      "article_type": "article",
      "title": "ault tolerance designs of interconnection network",
      "authors": [
        "Nadeem, Muhammad Faisal",
        "Imran, Muhammad",
        "Afzal Siddiqui, Hafiz Muhammad",
        "Azeem, Muhammad"
      ],
      "abstract": "Multiprocessor interconnection networks are mainly required to link a significant number of stable processor memory sets, every one of which is known as a processing vertex. Due to the cost-effectiveness, the design and utilization of multiprocessor interconnection networks have drawn attention lately. An interconnection network is extensively used in a parallel multiprocessor system to connect the processors and memory modules. In multiprocessor networks, vertices represent processors, and each processor of which is subject to complete failure. Fault tolerance is a process that ensures the working of a system to its total capacity, even if one or more of its components fail. In fault-tolerant design, backup components are used in interconnection networks to automatically take the place of the failed component, guaranteeing that the system\u2019s working continues. This paper proposed the fault-tolerant design in the form of Pk j and Ck j graphs, where n processing components are linked, and i of these n components creating a chain of maximum size, are used to perform the task, they can tolerate the failure of components, maintaining steady operation. In particular, we present the fault-tolerant designs of pyramid, {OTIS}, biswapped, and mesh-derived networks.",
      "publication_year": "",
      "keywords": "",
      "doi": "10.1007/s12083-023-01462-4"
    },
    "suggestion": {
      "relation": "This paper discusses fault tolerance designs in multiprocessor interconnection networks, which is crucial for maintaining the reliability of parallel processing systems used in deep learning.",
      "suggestion": "Use this paper to discuss the importance of fault-tolerant interconnection network designs in supporting the robustness of deep learning hardware systems.",
      "rating": 5
    },
    "sections": null
  }
}