@article{Zahid2020,
abstract = {Deep neural networks (DNNs) are state-of-The-Art algorithms for multiple applications, spanning from image classification to speech recognition. While providing excellent accuracy, they often have enormous compute and memory requirements. As a result of this, quantized neural networks (QNNs) are increasingly being adopted and deployed especially on embedded devices, thanks to their high accuracy, but also since they have significantly lower compute and memory requirements compared to their floating point equivalents. QNN deployment is also being evaluated for safety-critical applications, such as automotive, avionics, medical or industrial. These systems require functional safety, guaranteeing failure-free behaviour even in the presence of hardware faults. In general fault tolerance can be achieved by adding redundancy to the system, which further exacerbates the overall computational demands and makes it difficult to meet the power and performance requirements. In order to decrease the hardware cost for achieving functional safety, it is vital to explore domain-specific solutions which can exploit the inherent features of DNNs. In this work we present a novel methodology called fault-Aware training (FAT), which includes error modeling during neural network (NN) training, to make QNNs resilient to specific fault models on the device. Our experiments show that by injecting faults in the convolutional layers during training, highly accurate convolutional neural networks (CNNs) can be trained which exhibits much better error tolerance compared to the original. Furthermore, we show that redundant systems which are built from QNNs trained with FAT achieve higher worse-case accuracy at lower hardware cost. This has been validated for numerous classification tasks including CIFAR10, GTSRB, SVHN and ImageNet.},
archivePrefix = {arXiv},
arxivId = {2011.05873},
author = {Zahid, Ussama and Gambardella, Giulio and Fraser, Nicholas J. and Blott, Michaela and Vissers, Kees},
doi = {10.1109/ITC44778.2020.9325249},
eprint = {2011.05873},
file = {::},
isbn = {9781728191133},
issn = {10893539},
journal = {Proceedings - International Test Conference},
keywords = {FPGA,Functional Safety,Neural Networks,Quantized Neural Networks,Safety},
title = {{FAT: Training Neural Networks for Reliable Inference under Hardware Faults}},
volume = {2020-Novem},
year = {2020}
}
@article{Liu2019b,
abstract = {Approximate computing is a promising paradigm to deal with large computing workloads in fault-tolerant applications, providing opportunities to improve hardware efficiency of Deep Neural Networks (DNNs). However, it is still difficult to apply highly approximate arithmetics (e.g., multipliers) to DNNs due to the effect of error accumulation and the convergence problem in re-training phase. To tackle this limitation, we propose a hardware-software co-design algorithm, namely Incremental Network Approximation (INA). By addressing the convergence problem, INA promotes fault tolerance of DNNs, and yields more tradeoffs between accuracy and implementation cost. Experiments show that the approximate inference models re-trained by INA could achieve up to 80% hardware reduction in various hardware design level, while the classification accuracy degradation is less than 2%. Moreover, the experiments also exhibit the generality of INA algorithm for applying to various approximate multiplier design.},
author = {Liu, Zheyu and Jia, Kaige and Liu, Weiqiang and Wei, Qi and Qiao, Fei and Yang, Huazhong},
doi = {10.1109/ICCAD45719.2019.8942054},
isbn = {9781728123509},
issn = {10923152},
journal = {IEEE/ACM International Conference on Computer-Aided Design, Digest of Technical Papers, ICCAD},
title = {{INA: Incremental network approximation algorithm for limited precision deep neural networks}},
volume = {2019-Novem},
year = {2019}
}
@article{Chiu1994,
abstract = {This paper addresses methods ofimproving the fault tolerance of feedforward neural nets. The ?rst method is to coerce weights to have low magnitudes during the backpropagation training process, since fault tolerance is degraded by the use ofhigh magnitude weights; at the same time, additional hidden nodes are added dynami- cally to the network to ensure that desired performance can be obtained. The second method is to add arti?- cial faults to various components (nodes and links) of a network during training. The third method is to re- peatedly remove nodes that do not signi?cantly a?ect the network output, and then add new nodes that share the load of the more critical nodes in the network. Ex- perimental results have shown that these methods can obtain better robustness than backpropagation training, and compare favorably with other approaches [1, 15].},
author = {Chiu, Ching-tai and Mehrotra, Kishan and Mohan, Chilukuri K and Ranka, Sanjay},
file = {::},
journal = {Proceedings of IEEE 24th International Symposium on Fault-Tolerant Computing},
pages = {360----369},
title = {{Training Techniques to Obtain Fault Tolerant Neural Networks}},
year = {1994}
}
@article{Deng2015,
abstract = {Recently, neural network (NN) accelerators are gaining popularity as part of future heterogeneous multi-core architectures due to their broad application scope and excellent energy efficiency. Additionally, since neural networks can be retrained, they are inherently resillient to errors and noises. Prior work has utilized the error tolerance feature to design approximate neural network circuits or tolerate logical faults. However, besides high-level faults or noises, timing errors induced by delay faults, process variations, aging, etc. are dominating the reliability of NN accelerator under nanoscale manufacturing process. In this paper, we leverage the error resiliency of neural network to mitigate timing errors in NN accelerators. Specifically, when timing errors significantly affect the output results, we propose to retrain the accelerators to update their weights, thus circumventing critical timing errors. Experimental results show that timing errors in NN accelerators can be well tamed for different applications.},
author = {Deng, Jiacnao and Rang, Yuntan and Du, Zidong and Wang, Ymg and Li, Huawei and Temam, Olivier and Ienne, Paolo and Novo, David and Li, Xiaowei and Chen, Yunji and Wu, Chengyong},
doi = {10.7873/date.2015.0849},
file = {::},
isbn = {9783981537048},
issn = {15301591},
journal = {Proceedings -Design, Automation and Test in Europe, DATE},
keywords = {error tolerance,machine learning,neural networks,overclocking,timing errors},
number = {2011},
pages = {593--596},
title = {{Retraining-based timing error mitigation for hardware neural networks}},
volume = {2015-April},
year = {2015}
}
@article{Schorn2018c,
abstract = {Deep neural networks have become a ubiquitous tool for mastering complex classification tasks. Current research focuses on the development of power-efficient and fast neural network hardware accelerators for mobile and embedded devices. However, when used in safety-critical applications, for example autonomously operating vehicles, the reliability of such accelerators becomes a further optimization criterion which can stand in contrast to power-efficiency and latency. Furthermore, ensuring hardware reliability becomes increasingly challenging for shrinking structure widths and rising power densities in the nanometer semiconductor technology era. One solution to this challenge is the exploitation of fault tolerant parts in deep neural networks. In this paper we propose a new method for predicting the error resilience of neurons in deep neural networks and show that this method significantly improves upon existing methods in terms of accuracy as well as interpretability. We evaluate prediction accuracy by simulating hardware faults in networks trained on the CIFAR-10 and ILSVRC image classification benchmarks and protecting neurons according to the resilience estimations. In addition, we demonstrate how our resilience prediction can be used for a flexible trade-off between reliability and efficiency in neural network hardware accelerators.},
author = {Schorn, Christoph and Guntoro, Andre and Ascheid, Gerd},
doi = {10.23919/DATE.2018.8342151},
file = {::},
isbn = {9783981926316},
journal = {Proceedings of the 2018 Design, Automation and Test in Europe Conference and Exhibition, DATE 2018},
pages = {979--984},
publisher = {EDAA},
title = {{Accurate neuron resilience prediction for a flexible reliability management in neural network accelerators}},
volume = {2018-Janua},
year = {2018}
}
@inproceedings{Li2019,
abstract = {The emerging memristor technology is considered a promising solution to the edge-oriented deep learning and neuromorphic processor chips because it enables power-efficient Computing-in-Memory (CiM) and normally-off architecture simultaneously. However, as the analog nature and the immature nano-scale fabrication technology, the memristive cells suffer from manufacturing defects, process variations and aging-induced variations, which may incur system and function failures in applications. How to detect and rescue from the permanent and soft faults poses a significant challenge to the edge ReRAM-based deep learning or neuromorphic chips. In this work, we propose an edge-cloud collaborative framework, RRAMedy, to achieve in-situ fault detection and network remedy for memristor-based neural accelerators. In this framework, we present Adversarial Example Testing, a lifetime on-device fault detection technique, which can accurately detect defected cells and memristor soft faults with high probability and at a low cost. Furthermore, the model accuracy can be restored by the proposed edge-cloud collaborative fault-masking retraining and model updating mechanism with a minimized edge-cloud communication overhead. The experimental results show that RRAMedy can effectively detect the memristor permanent and soft faults, protecting the neural accelerator from accuracy and performance degradation in its life cycle.},
author = {Li, Wen and Wang, Ying and Li, Huawei and Li, Xiaowei},
booktitle = {Proceedings - 2019 IEEE International Conference on Computer Design, ICCD 2019},
doi = {10.1109/ICCD46524.2019.00020},
isbn = {9781538666487},
keywords = {Fault Detection,Fault Masking,Hard Fault,Memristor,Reliability,Soft Fault},
pages = {91--99},
title = {{RRAMedy: Protecting ReRAM-based neural network from permanent and soft faults during its lifetime}},
year = {2019}
}
@article{Gambardella2019a,
abstract = {Neural Networks are currently one of the most widely deployed machine learning algorithms. In particular, Convolutional Neural Networks (CNNs), are gaining popularity and are evaluated for deployment in safety critical applications such as self driving vehicles. Modern CNNs feature enormous memory bandwidth and high computational needs, challenging existing hardware platforms to meet throughput, latency and power requirements. Functional safety and error tolerance need to be considered as additional requirement in safety critical systems. In general, fault tolerant operation can be achieved by adding redundancy to the system, which is further exacerbating the computational demands. Furthermore, the question arises whether pruning and quantization methods for performance scaling turn out to be counterproductive with regards to fail safety requirements. In this work we present a methodology to evaluate the impact of permanent faults affecting Quantized Neural Networks (QNNs) and how to effectively decrease their effects in hardware accelerators. We use FPGA-based hardware accelerated error injection, in order to enable the fast evaluation. A detailed analysis is presented showing that QNNs containing convolutional layers are by far not as robust to faults as commonly believed and can lead to accuracy drops of up to 10%. To circumvent that, we propose two different methods to increase their robustness: 1) selective channel replication which adds significantly less redundancy than used by the common triple modular redundancy and 2) a fault-aware scheduling of processing elements for folded implementations.},
author = {Gambardella, Giulio and Kappauf, Johannes and Blott, Michaela and Doehring, Christoph and Kumm, Martin and Zipf, Peter and Vissers, Kees},
file = {::},
isbn = {9781538683989},
issn = {23318422},
journal = {arXiv},
keywords = {Automotive,FPGA,Neural networks,Quantized neural networks,Safety},
publisher = {IEEE},
title = {{Efficient Error-Tolerant Quantized Neural Network Accelerators}},
year = {2019}
}
@article{Mittal2020,
abstract = {As DNNs become increasingly common in mission-critical applications, ensuring their reliable operation has become crucial. Conventional resilience techniques fail to account for the unique characteristics of DNN algorithms/accelerators, and hence, they are infeasible or ineffective. In this paper, we present a survey of techniques for studying and optimizing the reliability of DNN accelerators and architectures. The reliability issues we cover include soft/hard errors arising due to process variation, voltage scaling, timing errors, DRAM errors due to refresh rate scaling and thermal effects, etc. We organize the research projects on several categories to bring out their key attributes. This paper underscores the importance of designing for reliability as the first principle, and not merely retrofit for it.},
author = {Mittal, Sparsh},
doi = {10.1016/j.sysarc.2019.101689},
file = {::},
issn = {13837621},
journal = {Journal of Systems Architecture},
keywords = {Deep learning,Deep neural networks,Fault-injection,Permanent fault,Review,Transient fault},
number = {August 2019},
pages = {101689},
publisher = {Elsevier B.V.},
title = {{A survey on modeling and improving reliability of DNN algorithms and accelerators}},
url = {https://doi.org/10.1016/j.sysarc.2019.101689},
volume = {104},
year = {2020}
}
@article{Mahdiani2012,
abstract = {Reliability should be identified as the most important challenge in future nano-scale very large scale integration (VLSI) implementation technologies for the development of complex integrated systems. Normally, fault tolerance (FT) in a conventional system is achieved by increasing its redundancy, which also implies higher implementation costs and lower performance that sometimes makes it even infeasible. In contrast to custom approaches, a new class of applications is categorized in this paper, which is inherently capable of absorbing some degrees of vulnerability and providing FT based on their natural properties. Neural networks are good indicators of imprecision-tolerant applications. We have also proposed a new class of FT techniques called relaxed fault-tolerant (RFT) techniques which are developed for VLSI implementation of imprecision-tolerant applications. The main advantage of RFT techniques with respect to traditional FT solutions is that they exploit inherent FT of different applications to reduce their implementation costs while improving their performance. To show the applicability as well as the efficiency of the RFT method, the experimental results for implementation of a face-recognition computationally intensive neural network and its corresponding RFT realization are presented in this paper. The results demonstrate promising higher performance of artificial neural network VLSI solutions for complex applications in faulty nano-scale implementation environments. {\textcopyright} 2012 IEEE.},
author = {Mahdiani, Hamid Reza and Fakhraie, Sied Mehdi and Lucas, Caro},
doi = {10.1109/TNNLS.2012.2199517},
file = {::},
issn = {2162237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Artificial neural networks,digital hardware implementation,face recognition hardware,fault tolerant techniques,soft error,very large scale integration (VLSI)},
number = {8},
pages = {1215--1228},
pmid = {24807519},
publisher = {IEEE},
title = {{Relaxed fault-tolerant hardware implementation of neural networks in the presence of multiple transient errors}},
volume = {23},
year = {2012}
}
@inproceedings{Xu2019,
abstract = {Recently neural network accelerators have grown into prominence with significant power and performance efficiency improvements over CPU and GPU. In this paper, we proposed two safety design techniques include Algorithm Based Atomic Error Checking-1 (ABAEC-1) and ABAEC-2 for a Weight Stationary (WS) Convolutional Neural Network (CNN) accelerator focusing on low latency and low overhead error detection and correction with no performance degradation. The proposed design techniques not only detect the errors on-the-fly but also perform error diagnosis to localize the errors to a Processing Element (PE) for on-line fault management and recovery. We applied the design techniques on an industry quality CNN accelerator and demonstrated that we could achieve the required Diagnostic Coverage (DC) goal with minimal area and power overhead for selected configurations. Furthermore, we discussed methods to extend the proposed techniques to other dataflow architecture.},
author = {Xu, Zheng and Abraham, Jacob},
booktitle = {Proceedings - International Test Conference},
doi = {10.1109/ITC44170.2019.9000149},
file = {::},
isbn = {9781728148236},
issn = {10893539},
pages = {1--10},
title = {{Safety design of a convolutional neural network accelerator with error localization and correction}},
volume = {2019-Novem},
year = {2019}
}
@article{Jere2020,
abstract = {Convolutional Neural Networks (CNNs) have made significant progress on several computer vision benchmarks, but are fraught with numerous non-human biases such as vulnerability to adversarial samples. Their lack of explain-ability makes identification and rectification of these biases difficult, and understanding their generalization behavior remains an open problem. In this work we explore the relationship between the generalization behavior of CNNs and the Singular Value Decomposition (SVD) of images. We show that naturally trained and adversarially robust CNNs exploit highly different features for the same dataset. We demonstrate that these features can be disentangled by SVD for ImageNet and CIFAR-10 trained networks. Finally, we propose Rank Integrated Gradients (RIG), the first rank-based feature attribution method to understand the dependence of CNNs on image rank.},
archivePrefix = {arXiv},
arxivId = {2012.03516},
author = {Jere, Malhar and Kumar, Maghav and Koushanfar, Farinaz},
eprint = {2012.03516},
file = {::},
issn = {23318422},
journal = {arXiv},
title = {{A singular value perspective on model robustness}},
year = {2020}
}
@article{He2020,
abstract = {We present a resilience analysis framework, called FIdelity, to accurately and quickly analyze the behavior of hardware errors in deep learning accelerators. Our framework enables resilience analysis starting from the very beginning of the design process to ensure that the reliability requirements are met, so that these accelerators can be safely deployed for a wide range of applications, including safety-critical applications such as self-driving cars. Existing resilience analysis techniques suffer from the following limitations: 1. general-purpose hardware techniques can achieve accurate results, but they require access to RTL to perform timeconsuming RTL simulations, which is not feasible for early design exploration; 2. general-purpose software techniques can produce results quickly, but they are highly inaccurate; 3. techniques targeting deep learning accelerators only focus on memory errors. Our FIdelity framework overcomes these limitations. FIdelity only requires a minimal amount of high-level design information that can be obtained from architectural descriptions/block diagrams, or estimated and varied for sensitivity analysis. By leveraging unique architectural properties of deep learning accelerators, we are able to systematically model a major class of hardware errors - transient errors in logic components - in software with high fidelity. Therefore, FIdelity is both quick and accurate, and does not require access to RTL. We thoroughly validate our FIdelity framework using Nvidia's open-source accelerator called NVDLA, which shows that the results are highly accurate - out of 60K fault injection experiments, the software fault models derived using FIdelity closely match the behaviors observed from RTL simulations. Using the validated FIdelity framework, we perform a large-scale resilience study on NVDLA, which consists of 46M fault injection experiments running various representative deep neural network applications. We report the key findings and architectural insights, which can be used to guide the design of future accelerators.},
author = {He, Yi and Balaprakash, Prasanna and Li, Yanjing},
doi = {10.1109/MICRO50266.2020.00033},
file = {:C\:/Users/fffas/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/He, Balaprakash, Li - 2020 - Fidelity Efficient resilience analysis framework for deep learning accelerators.pdf:pdf},
isbn = {9781728173832},
issn = {10724451},
journal = {Proceedings of the Annual International Symposium on Microarchitecture, MICRO},
pages = {270--281},
title = {{Fidelity: Efficient resilience analysis framework for deep learning accelerators}},
volume = {2020-Octob},
year = {2020}
}
@article{Pandey2020,
abstract = {The emergence of hardware accelerators has brought about several orders of magnitude improvement in the speed of the deep neural-network (DNN) inference. Among such DNN accelerators, the Google tensor processing unit (TPU) has transpired to be the best-in-class, offering more than 15\times speedup over the contemporary GPUs. However, the rapid growth in several DNN workloads conspires to escalate the energy consumptions of the TPU-based data-centers. In order to restrict the energy consumption of TPUs, we propose GreenTPU - a low-power near-threshold (NTC) TPU design paradigm. To ensure a high inference accuracy at a low-voltage operation, GreenTPU identifies the patterns in the error-causing activation sequences in the systolic array, and prevents further timing errors from similar patterns by intermittently boosting the operating voltage of the specific multiplier-and-accumulator units in the TPU. Compared to a cutting-edge timing error mitigation technique for TPUs, GreenTPU enables 2\times to 3\times higher performance (TOPS) in an NTC TPU, with a minimal loss in the prediction accuracy.},
author = {Pandey, Pramesh and Basu, Prabal and Chakraborty, Koushik and Roy, Sanghamitra},
doi = {10.1109/TVLSI.2020.2985057},
file = {::},
isbn = {9781450367257},
issn = {15579999},
journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
keywords = {Deep neural-network (DNN),near-threshold computing (NTC),neural network,predictive,tensor processing unit (TPU),timing error resilience},
number = {7},
pages = {1557--1566},
publisher = {ACM},
title = {{GreenTPU: Predictive Design Paradigm for Improving Timing Error Resilience of a Near-Threshold Tensor Processing Unit}},
volume = {28},
year = {2020}
}
@article{Temam2012,
abstract = {Due to the evolution of technology constraints, especially energy constraints which may lead to heterogeneous multi-cores, and the increasing number of defects, the design of defect-tolerant accelerators for heterogeneous multi-cores may become a major micro-architecture research issue. Most custom circuits are highly defect sensitive, a single transistor can wreck such circuits. On the contrary, artificial neural networks (ANNs) are inherently error tolerant algorithms. And the emergence of high-performance applications implementing recognition and mining tasks, for which competitive ANN-based algorithms exist, drastically expands the potential application scope of a hardware ANN accelerator. However, while the error tolerance of ANN algorithms is well documented, there are few in-depth attempts at demonstrating that an actual hardware ANN would be tolerant to faulty transistors. Most fault models are abstract and cannot demonstrate that the error tolerance of ANN algorithms can be translated into the defect tolerance of hardware ANN accelerators. In this article, we introduce a hardware ANN geared towards defect tolerance and energy efficiency, by spatially expanding the ANN. In order to precisely assess the defect tolerance capability of this hardware ANN, we introduce defects at the level of transistors, and then assess the impact of such defects on the hardware ANN functional behavior. We empirically show that the conceptual error tolerance of neural networks does translate into the defect tolerance of hardware neural networks, paving the way for their introduction in heterogeneous multi-cores as intrinsically defect-tolerant and energy-efficient accelerators. {\textcopyright} 2012 IEEE.},
author = {Temam, Olivier},
doi = {10.1109/ISCA.2012.6237031},
file = {::},
isbn = {9781467304757},
issn = {10636897},
journal = {Proceedings - International Symposium on Computer Architecture},
number = {c},
pages = {356--367},
publisher = {IEEE},
title = {{A defect-tolerant accelerator for emerging high-performance applications}},
volume = {00},
year = {2012}
}
@article{Nunez-Yanez2019,
abstract = {This research presents the extension and application of a voltage and frequency scaling framework called Elongate to a high-performance and reconfigurable binarized neural network. The neural network is created in the FPGA reconfigurable fabric and coupled to a multiprocessor host that controls the operational point to obtain energy proportionality. Elongate instruments a design netlist by inserting timing detectors to enable the exploitation of the operating margins of a device reliably. The elongated neural network is re-targeted to devices with different nominal operating voltages and fabricated with 28 nm (i.e., Zynq) and 16nm (i.e., Zynq Ultrascale) feature sizes showing the portability of the framework to advanced process nodes. New hardware and software components are created to support the 16nm fabric microarchitecture and a comparison in terms of power, energy and performance with the older 28 nm process is performed. The results show that Elongate can obtain new performance and energy points that are up to 86 percent better than nominal at the same level of classification accuracy. Trade-offs between energy and performance are also possible with a large dynamic range of valid working points available. The results also indicate that the built-in neural network robustness allows operation beyond the first point of error while maintaining the classification accuracy largely unaffected.},
author = {Nunez-Yanez, Jose},
doi = {10.1109/TC.2018.2879333},
file = {::},
issn = {15579956},
journal = {IEEE Transactions on Computers},
keywords = {DVFS,FPGA,convolutional neural network,energy efficiency},
number = {5},
pages = {676--687},
publisher = {IEEE},
title = {{Energy proportional neural network inference with adaptive voltage and frequency scaling}},
volume = {68},
year = {2019}
}
@article{Wu2019,
abstract = {As an emerging paradigm for energy-efficiency design, approximate computing can reduce power consumption through simplification of logic circuits. Although calculation errors are caused by approximate computing, their impacts on the final results can be negligible in some error resilient applications, such as Convolutional Neural Networks (CNNs). Therefore, approximate computing has been applied to CNNs to reduce the high demand for computing resources and energy. Compared with the traditional method such as reducing data precision, this paper investigates the effect of approximate computing on the accuracy and power consumption of CNNs. To optimize the approximate computing technology applied to CNNs, we propose a method for quantifying the error resilience of each neuron by theoretical analysis and observe that error resilience varies widely across different neurons. On the basic of quantitative error resilience, dynamic adaptation of approximate bit-width and the corresponding configurable adder are proposed to fully exploit the error resilience of CNNs. Experimental results show that the proposed method further improves the performance of power consumption while maintaining high accuracy. By adopting the optimal approximate bit-width for each layer found by our proposed algorithm, dynamic adaptation of approximate bit-width reduces power consumption by more than 30% and causes less than 1% loss of the accuracy for LeNet-5.},
author = {Wu, Chengjun and Shan, Weiwei and Xu, Jiaming},
doi = {10.1109/NANOARCH47378.2019.181283},
isbn = {9781728155203},
journal = {NANOARCH 2019 - 15th IEEE/ACM International Symposium on Nanoscale Architectures, Proceedings},
keywords = {approximate computing,configurable adder,convolutional neural network,dynamic adaptation of approximate bit-width,error resilience},
title = {{Dynamic Adaptation of Approximate Bit-width for CNNs based on Quantitative Error Resilience}},
year = {2019}
}
@article{Torres-Huitzil2017,
abstract = {Beyond energy, the growing number of defects in physical substrates is becoming another major constraint that affects the design of computing devices and systems. As the underlying semiconductor technologies are getting less and less reliable, the probability that some components of computing devices fail also increases, preventing designers from realizing the full potential benefits of on-chip exascale integration derived from near atomic scale feature dimensions. As the quest for performance confronts permanent and transient faults, device variation, and thermal issues, major breakthroughs in computing efficiency are expected to benefit from unconventional and new models of computation, such as brain-inspired computing. The challenge is then to find not only high-performance and energy-efficient, but also fault-tolerant computing solutions. Neural computing principles remain elusive, yet as source of a promising fault-tolerant computing paradigm. In the quest to fault tolerance can be translated into scalable and reliable computing systems, hardware design itself and/or to use circuits even with faults has further motivated research on neural networks, which are potentially capable of absorbing some degrees of vulnerability based on their natural properties. This paper presents a survey on fault tolerance in neural networks manly focusing on well-established passive techniques to exploit and improve, by design, such potential but limited intrinsic property in neural models, particularly for feedforward neural networks. First, fundamental concepts and background on fault tolerance are introduced. Then, we review fault types, models, and measures used to evaluate performance and provide a taxonomy of the main techniques to enhance the intrinsic properties of some neural models, based on the principles and mechanisms that they exploit to achieve fault tolerance passively. For completeness, we briefly review some representative works on active fault tolerance in neural networks. We present some key challenges that remain to be overcome and conclude with an outlook for this field.},
author = {Torres-Huitzil, Cesar and Girau, Bernard},
doi = {10.1109/ACCESS.2017.2742698},
file = {::},
issn = {21693536},
journal = {IEEE Access},
keywords = {Fault tolerance,fault masking,fault models,neural networks,redundancy,taxonomy},
pages = {17322--17341},
title = {{Fault and Error Tolerance in Neural Networks: A Review}},
volume = {5},
year = {2017}
}
@article{Ozen2019,
abstract = {The widespread usage of deep neural networks in autonomous driving necessitates a consideration of the safety arguments against hardware-level faults. This study confirms the possible catastrophic impact of hardware-level faults on DNN accuracy; the consequent need for low-cost fault tolerance methods can be met through a rigorous exploration of the mathematical properties of the associated computations. We propose Sanity-Check, which makes use of the linearity property and employs spatial and temporal checksums to protect fully-connected and convolutional layers in deep neural networks. Sanity-Check can be purely implemented on software and deployed on different execution platforms with no additional modification. We also propose Sanity-Check hardware which integrates seamlessly with modern DNN accelerators and neutralizes the small performance overhead in pure software implementations. Sanity-Check delivers perfect error-caused misprediction coverage in our experiments, which makes it a promising candidate for boosting the reliability of safety-critical deep neural network applications.},
author = {Ozen, Elbruz and Orailoglu, Alex},
doi = {10.1109/ATS47505.2019.000-8},
file = {::},
isbn = {9781728126951},
issn = {10817735},
journal = {Proceedings of the Asian Test Symposium},
keywords = {autonomous driving,deep neural networks,fault tolerance,reliability},
pages = {7--12},
publisher = {IEEE},
title = {{Sanity-Check: Boosting the Reliability of Safety-Critical Deep Neural Network Applications}},
volume = {2019-Decem},
year = {2019}
}
@misc{Shafique2020a,
abstract = {Currently, machine learning (ML) techniques are at the heart of smart cyber-physical systems (CPS) and Internet-of-Things (IoT). This article discusses various challenges and probable solutions for security attacks on these ML-inspired hardware and software techniques. - Partha Pratim Pande, Washington State University.},
archivePrefix = {arXiv},
arxivId = {2101.02559},
author = {Shafique, Muhammad and Naseer, Mahum and Theocharides, Theocharis and Kyrkou, Christos and Mutlu, Onur and Orosa, Lois and Choi, Jungwook},
booktitle = {IEEE Design and Test},
doi = {10.1109/MDAT.2020.2971217},
eprint = {2101.02559},
file = {:C\:/Users/fffas/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shafique et al. - 2020 - Robust machine learning systems Challenges, current trends, perspectives, and the road ahead.pdf:pdf;::},
issn = {21682364},
number = {2},
pages = {30--57},
title = {{Robust machine learning systems: Challenges, current trends, perspectives, and the road ahead}},
volume = {37},
year = {2020}
}
@article{Leung2017,
abstract = {Many existing results on fault-tolerant algorithms focus on the single fault source situation, where a trained network is affected by one kind of weight failure. In fact, a trained network may be affected by multiple kinds of weight failure. This paper first studies how the open weight fault and the multiplicative weight noise degrade the performance of radial basis function (RBF) networks. Afterward, we define the objective function for training fault-tolerant RBF networks. Based on the objective function, we then develop two learning algorithms, one batch mode and one online mode. Besides, the convergent conditions of our online algorithm are investigated. Finally, we develop a formula to estimate the test set error of faulty networks trained from our approach. This formula helps us to optimize some tuning parameters, such as RBF width.},
author = {Leung, Chi Sing and Wan, Wai Yan and Feng, Ruibin},
doi = {10.1109/TNNLS.2016.2536172},
file = {::},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Fault tolerance,prediction error,radial basis function (RBF),regularization},
number = {6},
pages = {1360--1372},
pmid = {28113823},
publisher = {IEEE},
title = {{A Regularizer Approach for RBF Networks Under the Concurrent Weight Failure Situation}},
volume = {28},
year = {2017}
}
@misc{Hari2020,
abstract = {—The ability of Convolutional Neural Networks (CNNs) to accurately process real-time telemetry has boosted their use in safety-critical and high-performance computing systems. As such systems require high levels of resilience to errors, CNNs must execute correctly in the presence of hardware faults. Full duplication provides the needed assurance but incurs a prohibitive 100% overhead. Algorithmic techniques are known to offer low-cost solutions, but the practical feasibility and performance of such techniques have never been studied for CNN deployment platforms (e.g., TensorFlow or TensorRT on GPUs). In this paper, we focus on algorithmically verifying Convolutions, which are the most resource-demanding operations in CNNs. We use checksums to verify convolutions, adding a small amount of redundancy, far less than full-duplication. We first identify the challenges that arise in employing Algorithm-Based Error Detection (ABED) for Convolutions in optimized inference platforms that fuse multiple network layers and use reduced-precision operations, and demonstrate how to overcome them. We propose and evaluate variations of ABED techniques that offer implementation complexity, runtime overhead, and coverage trade-offs. Results show that ABED can detect all transient hardware errors that might otherwise corrupt output and does so while incurring low runtime overheads (6-23%), offering at least 1.6× throughput to workloads compared to full duplication.},
archivePrefix = {arXiv},
arxivId = {2006.04984},
author = {Hari, Siva Kumar Sastry and Sullivan, Michael B. and Tsai, Timothy and Keckler, Stephen W.},
booktitle = {arXiv},
eprint = {2006.04984},
file = {::},
issn = {23318422},
pages = {1--12},
title = {{Making Convolutions Resilient via Algorithm-Based Error Detection Techniques}},
year = {2020}
}
@article{Li2020,
abstract = {Convolution neural networks (CNNs) have been widely used in many applications. Field-Programmable Gate Array (FPGA) based accelerator is an ideal solution for CNNs in embedded systems. However, the single event upset (SEU) effect in FPGA device may have a significant influence on the performance of CNNs. In this paper, we analyze the sensibility of CNNs to SEU and present a fault-tolerant design for CNN accelerators. First, we find that SEU in processing elements (PEs) has the worst effects on CNNs since it produces proportional errors and will not get refreshed. Furthermore, it is indicated that the large positive perturbation contributes almost all of the performance loss. Based on such observations, we propose an error detecting scheme to locate incorrect PEs and give an error masking method to achieve fault-tolerance. Experiments demonstrate that the proposed method achieves similar fault-tolerant performance with the triple modular redundancy (TMR) scheme while the overhead is much lower than it.},
author = {Li, Wenshuo and Ge, Guangjun and Guo, Kaiyuan and Chen, Xiaoming and Wei, Qi and Gao, Zhen and Wang, Yu and Yang, Huazhong},
doi = {10.1109/AICAS48895.2020.9073925},
file = {::},
isbn = {9781728149226},
journal = {Proceedings - 2020 IEEE International Conference on Artificial Intelligence Circuits and Systems, AICAS 2020},
pages = {1--5},
title = {{Soft Error Mitigation for Deep Convolution Neural Network on FPGA Accelerators}},
year = {2020}
}
@article{Chaudhuri2020,
abstract = {The ubiquitous application of deep neural networks (DNNs) has led to a rise in demand for artificial intelligence (AI) accelerators. This paper studies the problem of classifying structural faults in such an accelerator based on their functional criticality. We analyze the impact of stuck-At faults in the processing elements (PEs) of a 128 \times 128 systolic array designed to perform classification on the MNIST dataset using both 32-bit and 16-bit data paths. We present a two-Tier machine-learning (ML) based method to assess the functional criticality of these faults. We address the problem of minimizing misclassification by utilizing generative adversarial networks (GANs). The two-Tier ML/GAN-based criticality assessment method leads to less than 1% test escapes during functional criticality evaluation.},
author = {Chaudhuri, Arjun and Talukdar, Jonti and Su, Fei and Chakrabarty, Krishnendu},
doi = {10.1109/ITC44778.2020.9325272},
file = {::},
isbn = {9781728191133},
issn = {10893539},
journal = {Proceedings - International Test Conference},
pages = {1--5},
title = {{Functional Criticality Classification of Structural Faults in AI Accelerators}},
volume = {2020-Novem},
year = {2020}
}
@article{Mandal2019,
abstract = {Efficient low complexity error correcting code (ECC) is considered as an effective technique for mitigation of multi-bit upset (MBU) in the configuration memory (CM) of static random access memory (SRAM) based Field Programmable Gate Array (FPGA) devices. Traditional multi-bit ECCs have large overhead and complex decoding circuit to correct adjacent multi-bit error. In this work, we propose a simple multi-bit ECC which uses Secure Hash Algorithm for error detection and parity based two dimensional Erasure Product Code for error correction. Present error mitigation techniques perform error correction in the CM without considering the criticality or the execution period of the tasks allocated in different portion of CM. In most of the cases, error correction is not done in the right instant, which sometimes either suspends normal system operation or wastes hardware resources for less critical tasks. In this paper, we advocate for a dynamic priority-based hardware scheduling algorithm which chooses the tasks for error correction based on their area, execution period and criticality. The proposed method has been validated in terms of overhead due to redundant bits, error correction time and system reliability.},
archivePrefix = {arXiv},
arxivId = {1810.09661},
author = {Mandal, Swagata and Sarkar, Sreetama and Ming, Wong Ming and Chattopadhyay, Anupam and Chakrabarti, Amlan},
doi = {10.1109/VLSID.2019.00063},
eprint = {1810.09661},
file = {::},
isbn = {9781728104096},
journal = {Proceedings - 32nd International Conference on VLSI Design, VLSID 2019 - Held concurrently with 18th International Conference on Embedded Systems, ES 2019},
pages = {257--262},
publisher = {IEEE},
title = {{Criticality aware soft error mitigation in the configuration memory of SRAM based FPGA}},
year = {2019}
}
@inproceedings{Gerasimou2020,
abstract = {Deep Learning (DL) systems are key enablers for engineering intelligent applications. Nevertheless, using DL systems in safety- A nd security-critical applications requires to provide testing evidence for their dependable operation. We introduce DeepImportance, a systematic testing methodology accompanied by an Importance-Driven (IDC) test adequacy criterion for DL systems. Applying IDC enables to establish a layer-wise functional understanding of the importance of DL system components and use this information to assess the semantic diversity of a test set. Our empirical evaluation on several DL systems and across multiple DL datasets demonstrates the usefulness and effectiveness of DeepImportance.},
archivePrefix = {arXiv},
arxivId = {2002.03433},
author = {Gerasimou, Simos and Eniser, Hasan Ferit and Sen, Alper and Cakan, Alper},
booktitle = {Proceedings - 2020 ACM/IEEE 42nd International Conference on Software Engineering: Companion, ICSE-Companion 2020},
doi = {10.1145/3377812.3390793},
eprint = {2002.03433},
file = {::},
isbn = {9781450371223},
issn = {02705257},
keywords = {Deep Neural Networks,Software Testing},
pages = {322--323},
title = {{Importance-Driven Deep Learning System Testing}},
year = {2020}
}
@article{Itsuji2020,
abstract = {The reliability of GPUs is becoming a major concern due to the increased probability of failures and the high vulnerability of GPUs compared to conventional CPUs in terms of tasks per failure. While there are extensive countermeasures against failures in GPU data units, there are fewer countermeasures for failures in GPU control logics. Currently, software-based techniques, such as inserting signature codes for detecting GPU control-logic failures by comparing the expected signature value with the current signature value, are being utilized. However, in the conventional software-based techniques, application calculations, signature calculations, and signature comparison calculations are executed in sequence, which degrades the application throughputs. We have developed a software-based technique that concurrently detects GPU control-logic failures in a running application while largely maintaining its throughput. Experimental results show that when our technique concurrently executed application calculations, signature calculations, and signature comparison calculations for a matrix multiplication application, the application throughput remains 78% of the original one, whereas 62% is reported in literature. We also developed fault injection simulators specialized for injecting GPU-specific control-logic faults into GPU intermediate codes and found that 100% of GPU-specific failures could be detected both during and after application execution. The proposed approach can be utilized for a wide variety of safety-And reliability-critical applications.},
annote = {学习下这个signature-based 策略如何实现故障检测，是否可以应用到DLA加速器设计中去？类似的基于SHA3编码的故障检测技术，是否可以用于这样的场景？},
author = {Itsuji, Hiroaki and Uezono, Takumi and Toba, Tadanobu and Ito, Kojiro and Hashimoto, Masanori},
doi = {10.1109/ITC44778.2020.9325216},
file = {::},
isbn = {9781728191133},
issn = {10893539},
journal = {Proceedings - International Test Conference},
keywords = {GPU,error detection,fault injection,reliability,soft error},
pages = {1--5},
title = {{Concurrent Detection of Failures in GPU Control Logic for Reliable Parallel Computing}},
volume = {2020-Novem},
year = {2020}
}
@article{Tambe2020,
abstract = {Conventional hardware-friendly quantization methods, such as fixed-point or integer, tend to perform poorly at very low precision as their shrunken dynamic ranges cannot adequately capture the wide data distributions commonly seen in sequence transduction models. We present an algorithm-hardware co-design centered around a novel floating-point inspired number format, AdaptivFloat, that dynamically maximizes and optimally clips its available dynamic range, at a layer granularity, in order to create faithful encodings of neural network parameters. AdaptivFloat consistently produces higher inference accuracies compared to block floating-point, uniform, IEEE-like float or posit encodings at low bit precision ( =8-bit) across a diverse set of state-of-the-art neural networks, exhibiting narrow to wide weight distribution. Notably, at 4-bit weight precision, only a 2.1 degradation in BLEU score is observed on the AdaptivFloat-quantized Transformer network compared to total accuracy loss when encoded in the above-mentioned prominent datatypes. Furthermore, experimental results on a deep neural network (DNN) processing element (PE), exploiting AdaptivFloat logic in its computational datapath, demonstrate per-operation energy and area that is 0.9× and 1.14×, width, respectively that of an equivalent bit NVDLA-like integer-based PE.},
author = {Tambe, Thierry and Yang, En Yu and Wan, Zishen and Deng, Yuntian and {Janapa Reddi}, Vijay and Rush, Alexander and Brooks, David and Wei, Gu Yeon},
doi = {10.1109/DAC18072.2020.9218516},
file = {::},
isbn = {9781450367257},
issn = {0738100X},
journal = {Proceedings - Design Automation Conference},
title = {{Algorithm-hardware co-design of adaptive floating-point encodings for resilient deep learning inference}},
volume = {2020-July},
year = {2020}
}
@article{Libano2019a,
abstract = {Neural networks are becoming an attractive solution for automatizing vehicles in the automotive, military, and aerospace markets. Thanks to their low-cost, low-power consumption, and flexibility, field-programmable gate arrays (FPGAs) are among the promising devices to implement neural networks. Unfortunately, FPGAs are also known to be susceptible to radiation-induced errors. In this paper, we evaluate the effects of radiation-induced errors in the output correctness of two neural networks [Iris Flower artificial neural network (ANN) and Modified National Institute of Standards and Technology (MNIST) convolutional neural network (CNN)] implemented in static random-access memory-based FPGAs. In particular, we notice that radiation can induce errors that modify the output of the network with or without affecting the neural network's functionality. We call the former critical errors and the latter tolerable errors. Through exhaustive fault injection, we identify the portions of Iris Flower ANN and MNIST CNN implementation on FPGAs that are more likely, once corrupted, to generate a critical or a tolerable error. Based on this analysis, we propose a selective hardening strategy that triplicates only the most vulnerable layers of the neural network. With neutron radiation testing, our selective hardening solution was able to mask 40% of faults with a marginal 8% overhead in one of our tested neural networks.},
author = {Libano, F. and Wilson, B. and Anderson, J. and Wirthlin, M. J. and Cazzaniga, C. and Frost, C. and Rech, P.},
doi = {10.1109/TNS.2018.2884460},
file = {::},
issn = {00189499},
journal = {IEEE Transactions on Nuclear Science},
keywords = {Field-programmable gate array (FPGA),hardening,neural networks,reliability},
number = {1},
pages = {216--222},
publisher = {IEEE},
title = {{Selective hardening for neural networks in FPGAs}},
volume = {66},
year = {2019}
}
@inproceedings{Zhang2019a,
abstract = {Machine learning, in particular deep learning, is being used in almost all the aspects of life to facilitate humans, specifically in mobile and Internet of Things (IoT)-based applications. Due to its state-of-the-art performance, deep learning is also being employed in safety-critical applications, for instance, autonomous vehicles. Reliability and security are two of the key required characteristics for these applications because of the impact they can have on human's life. Towards this, in this paper, we highlight the current progress, challenges and research opportunities in the domain of robust systems for machine learning-based applications.},
author = {Zhang, Jeff Jun and Liu, Kang and Khalid, Faiq and Hanif, Muhammad Abdullah and Rehman, Semeen and Theocharides, Theocharis and Artussi, Alessandro and Shafique, Muhammad and Garg, Siddharth},
booktitle = {Proceedings - Design Automation Conference},
doi = {10.1145/3316781.3323472},
file = {::},
isbn = {9781450367257},
issn = {0738100X},
keywords = {Adversarial Attacks,Deep Learning,Machine Learning,Permanent Faults,Reliability,Robustness,Security,Timing Errors},
title = {{INVITED: Building robust machine learning systems: Current progress, research challenges, and opportunities}},
year = {2019}
}
@article{Schorn2019a,
abstract = {Deep neural networks usually possess a high overall resilience against errors in their intermediate computations. However, it has been shown that error resilience is generally not homogeneous within a neural network and some neurons might be very sensitive to faults. Even a single bit-flip fault in one of these critical neuron outputs can result in a large degradation of the final network output accuracy, which cannot be tolerated in some safety-critical applications. While critical neuron computations can be protected using error correction techniques, a resilience optimization of the neural network itself is more desirable, since it can reduce the required effort for error correction and fault protection in hardware. In this paper, we develop a novel resilience optimization method for deep neural networks, which builds upon a previously proposed resilience estimation technique. The optimization involves only few steps and can be applied to pre-trained networks. In our experiments, we significantly reduce the worst-case failure rates after a bit-flip fault for deep neural networks trained on the MNIST, CIFAR-10 and ILSVRC classification benchmarks.},
author = {Schorn, Christoph and Guntoro, Andre and Ascheid, Gerd},
doi = {10.23919/DATE.2019.8714885},
file = {::},
isbn = {9783981926323},
journal = {Proceedings of the 2019 Design, Automation and Test in Europe Conference and Exhibition, DATE 2019},
keywords = {Deep neural networks,fault injection,fault tolerance,memory faults,resilience optimization},
pages = {1507--1512},
publisher = {EDAA},
title = {{An Efficient Bit-Flip Resilience Optimization Method for Deep Neural Networks}},
year = {2019}
}
@inproceedings{Li2020a,
abstract = {With the fast evolvement of deep-learning specific embedded computing systems, applications powered by deep learning are moving from the cloud to the edge. When deploying NNs onto the edge devices under complex environments, there are various types of possible faults: Soft errors caused by atmospheric neutrons and radioactive impurities, voltage instability, aging, temperature variations, and malicious attackers. Thus the safety risk of deploying neural networks at edge computing devices in safety-critic applications is now drawing much attention. In this paper, we implement the random bit-flip, Gaussian, and Salt-and-Pepper fault models and establish a multi-objective fault-tolerant neural architecture search framework. On top of the NAS framework, we propose Fault-Tolerant Neural Architecture Search (FT-NAS) to automatically discover convolutional neural network (CNN) architectures that are reliable to various faults in nowadays edge devices. Then we incorporate fault-tolerant training (FTT) in the search process to achieve better results, which we called FTT-NAS. Experiments show that the discovered architecture FT-NAS-Net and FTT-NAS-Net outperform other hand-designed baseline architectures (58.1%/86.6% VS. 10.0%/52.2%), with comparable FLOPs and less parameters. What is more, the architectures trained under a single fault model can also defend against other faults. By inspecting the discovered architecture, we find that there are redundant connections learned to protect the sensitive paths. This insight can guide future fault-tolerant neural architecture design, and we verify it by a modification on ResNet-20-ResNet-M.},
archivePrefix = {arXiv},
arxivId = {2003.10375},
author = {Li, Wenshuo and Ning, Xuefei and Ge, Guangjun and Chen, Xiaoming and Wang, Yu and Yang, Huazhong},
booktitle = {Proceedings of the Asia and South Pacific Design Automation Conference, ASP-DAC},
doi = {10.1109/ASP-DAC47756.2020.9045324},
eprint = {2003.10375},
isbn = {9781728141237},
pages = {211--216},
title = {{FTT-NAS: Discovering Fault-Tolerant Neural Architecture}},
volume = {2020-Janua},
year = {2020}
}
@article{Clemente2016,
abstract = {This letter presents an FPGA implementation of a fault-tolerant Hopfield Neural Network (HNN). The robustness of this circuit against Single Event Upsets (SEUs) and Single Event Transients (SETs) has been evaluated. Results show the fault tolerance of the proposed design, compared to a previous non-fault-tolerant implementation and a solution based on triple modular redundancy (TMR) of a standard HNN design.},
author = {Clemente, Juan Antonio and Mansour, Wassim and Ayoubi, Rafic and Serrano, Felipe and Mecha, Hortensia and Ziade, Haissam and {El Falou}, Wassim and Velazco, Raoul},
doi = {10.1016/j.neucom.2015.06.038},
file = {::},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Artificial Neural Network (ANN),FPGA,Fault tolerance,Hopfield Neural Network (HNN),Single Event Transient (SET),Single Event Upset (SEU)},
pages = {1606--1609},
title = {{Hardware implementation of a fault-tolerant Hopfield Neural Network on FPGAs}},
volume = {171},
year = {2016}
}
@article{Song2020a,
abstract = {Quantization is an effective technique for Deep Neural Network (DNN) inference acceleration. However, conventional quantization techniques are either applied at network or layer level that may fail to exploit fine-grained quantization for further speedup, or only applied on kernel weights without paying attention to the feature map dynamics that may lead to lower NN accuracy. In this paper, we propose a dynamic region-based quantization, namely DRQ, which can change the precision of a DNN model dynamically based on the sensitive regions in the feature map to achieve greater acceleration while reserving better NN accuracy. We propose an algorithm to identify the sensitive regions and an architecture that utilizes a variable-speed mixed-precision convolution array to enable the algorithm with better performance and energy efficiency. Our experiments on a wide variety of networks show that compared to a coarse-grained quantization accelerator like 'Eyeriss', DRQ can achieve 92% performance gain and 72% energy reduction with less then 1% accuracy loss. Compared to the state-of-the-art mixed-precision quantization accelerator 'OLAccel', DRQ can also achieve 21% performance gain and 33% energy reduction with 3% prediction accuracy improvement which is quite impressive for inference.},
author = {Song, Zhuoran and Fu, Bangqi and Wu, Feiyang and Jiang, Zhaoming and Jiang, Li and Jing, Naifeng and Liang, Xiaoyao},
doi = {10.1109/ISCA45697.2020.00086},
isbn = {9781728146614},
issn = {10636897},
journal = {Proceedings - International Symposium on Computer Architecture},
pages = {1010--1021},
title = {{DRQ: Dynamic Region-based Quantization for Deep Neural Network Acceleration}},
volume = {2020-May},
year = {2020}
}
@article{Karim2018a,
abstract = {Spiking Astrocyte-neuron Networks (SANNs) model the adaptive/repair feature of the human brain. They integrate astrocyte cells with spiking neurons to facilitate a distributed and fine-grained self-repair capability at the synapse level. SANNs are more complex with the addition of astrocyte cells and require longer simulation times, as they are dynamic over much longer time-scales than traditional neural networks. Therefore, dedicated FPGA accelerators offer reductions in simulation times. To support the acceleration of SANNs, the capability of fault injection to synapses and monitoring significant levels of neuron and astrocyte data for off-chip transmission to PC-based analysis, are required. This paper presents an FPGA-based monitoring platform (FMP) for injecting faults and capturing and analyzing data acquired from the SANN FPGA accelerator, Astrobyte. The FMP uses custom logic and a NIOS II based system to control fault injection and data monitoring on the FPGA. Results show accurate accelerated simulations of fault injection scenarios using FMP with speedups up to 65 times greater compared with equivalent Matlab implementations.},
author = {Karim, Shvan and Harkin, Jim and McDaid, Liam and Gardiner, Bryan and Liu, Junxiu and Halliday, David M. and Tyrrell, Andy M. and Timmis, Jon and Millard, Alan G. and Johnson, Anju P.},
doi = {10.1109/ISCAS.2018.8351512},
file = {::},
isbn = {9781538648810},
issn = {02714310},
journal = {Proceedings - IEEE International Symposium on Circuits and Systems},
keywords = {Astrocytes,Data Acquisition,FPGA acceleration,Fault injection,Self repair,Spiking neural network},
title = {{FPGA-based Fault-injection and Data Acquisition of Self-repairing Spiking Neural Network Hardware}},
volume = {2018-May},
year = {2018}
}
@article{Tsai2021,
abstract = {Adversarial robustness has become an emerging challenge for neural network owing to its over-sensitivity to small input perturbations. While being critical, we argue that solving this singular issue alone fails to provide a comprehensive robustness assessment. Even worse, the conclusions drawn from singular robustness may give a false sense of overall model robustness. Specifically, our findings show that adversarially trained models that are robust to input perturbations are still (or even more) vulnerable to weight perturbations when compared to standard models. In this paper, we formalize the notion of non-singular adversarial robustness for neural networks through the lens of joint perturbations to data inputs as well as model weights. To our best knowledge, this study is the first work considering simultaneous input-weight adversarial perturbations. Based on a multi-layer feed-forward neural network model with ReLU activation functions and standard classification loss, we establish error analysis for quantifying the loss sensitivity subject to $\ell_\infty$-norm bounded perturbations on data inputs and model weights. Based on the error analysis, we propose novel regularization functions for robust training and demonstrate improved non-singular robustness against joint input-weight adversarial perturbations.},
archivePrefix = {arXiv},
arxivId = {2102.11935},
author = {Tsai, Yu-Lin and Hsu, Chia-Yi and Yu, Chia-Mu and Chen, Pin-Yu},
eprint = {2102.11935},
file = {::},
title = {{Non-Singular Adversarial Robustness of Neural Networks}},
url = {http://arxiv.org/abs/2102.11935},
year = {2021}
}
@article{Zhang2019c,
abstract = {Editor's note: Systolic array is embracing its renaissance after being accepted by Google TPU as the core computing architecture of machine learning acceleration. In this article, the authors propose two strategies to enhance fault tolerance of systolic array based deep neural network accelerators. - Yiran Chen, Duke University.},
author = {Zhang, Jeff Jun and Basu, Kanad and Garg, Siddharth},
doi = {10.1109/MDAT.2019.2915656},
file = {::},
issn = {21682364},
journal = {IEEE Design and Test},
keywords = {Deep Neural Networks,Fault Toerance,Reliability,Systolic Arrays,Testing},
number = {5},
pages = {44--53},
publisher = {IEEE},
title = {{Fault-Tolerant Systolic Array Based Accelerators for Deep Neural Network Execution}},
volume = {36},
year = {2019}
}
@article{Arechiga2018,
abstract = {Neural Networks have revolutionized computer vision and the field has advanced so rapidly that in less than a decade neural networks are on par with human performance at image classification tasks. Applying neural networks to robotics should drastically increase the capabilities of robots to navigate and interact with the world around them. However, robots are often used to investigate environments too dangerous for humans. Such environments include areas with high levels of radiation such as the Fukushima reactor or Jovian moons. Radiation causes single event upset (SEU) errors such as unexpected bit flips so robots must be robust to those kinds of errors. Before robots can fully utilize the advances of neural networks, the networks must be tested for their robustness against SEUs. SEUs are most likely to cause bit flips within the trained parameters of a neural network. This is because the memory that stores the trained parameters takes up the most surface area of a computer, and thus is more likely to be hit by high energy particles. Previous papers in this field have focused on older networks such as Multi-Layer Perceptrons, but Convolutional Neural Networks are the current state of the art when it comes to object classification tasks. This paper tests several modern neural network architectures for their robustness to bit flips in their weights and examines which aspects of each different architecture lead to greater robustness. The different architectures tested are VGG16, ResNet50, and InceptionV3. The experiments show that all three networks display bimodal distributions under memory errors, meaning that the networks either retain their trained classification accuracy, or drop to very low accuracies. Additionally, it only takes a small number of memory errors compared to the total size of the network for the neural network's performance to significantly degrade. When comparing the three architectures, VGG16 was the least robust against random bit flips in its trained weights while ResNet50 and InceptionV3 had similar levels of robustness against SEU-type memory errors. Some possible reasons for ResNet50 and Inception V3's robustness are their use of batch normalization or their use of shortcut connections.},
author = {Arechiga, Austin P. and Michaels, Alan J.},
doi = {10.1109/HPEC.2018.8547532},
file = {::},
isbn = {9781538659892},
journal = {2018 IEEE High Performance Extreme Computing Conference, HPEC 2018},
keywords = {Machine Learning,Memory Error,Neural Networks,Robust,Single Event Upset},
publisher = {IEEE},
title = {{The Robustness of Modern Deep Learning Architectures against Single Event Upset Errors}},
year = {2018}
}
@article{Salami2018,
abstract = {Machine Learning (ML) is making a strong resurgence in tune with the massive generation of unstructured data which in turn requires massive computational resources. Due to the inherently compute and power-intensive structure of Neural Networks (NNs), hardware accelerators emerge as a promising solution. However, with technology node scaling below 10nm, hardware accelerators become more susceptible to faults, which in turn can impact the NN accuracy. In this paper, we study the resilience aspects of Register-Transfer Level (RTL) model of NN accelerators, in particular, fault characterization and mitigation. By following a High-Level Synthesis (HLS) approach, first, we characterize the vulnerability of various components of RTL NN. We observed that the severity of faults depends on both i) application-level specifications, i.e., NN data (inputs, weights, or intermediate) and NN layers and ii) architectural-level specifications, i.e., data representation model and the parallelism degree of the underlying accelerator. Second, motivated by characterization results, we present a low-overhead fault mitigation technique that can efficiently correct bit flips, by 47.3% better than state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1806.09679},
author = {Salami, Behzad and Unsal, Osman S. and Kestelman, Adrian Cristal},
doi = {10.1109/CAHPC.2018.8645906},
eprint = {1806.09679},
file = {::},
isbn = {9781538677698},
journal = {Proceedings - 2018 30th International Symposium on Computer Architecture and High Performance Computing, SBAC-PAD 2018},
pages = {322--329},
title = {{On the Resilience of RTL NN Accelerators: Fault Characterization and Mitigation}},
url = {http://arxiv.org/abs/1806.09679},
year = {2019}
}
@inproceedings{Liu2019c,
abstract = {New DNN accelerators based on emerging technologies, such as resistive random access memory (ReRAM), are gaining increasing research attention given their potential of "in-situ" data processing. Unfortunately, device-level physical limitations that are unique to these technologies may cause weight disturbance in memory and thus compromising the performance and stability of DNN accelerators. In this work, we propose a novel fault-tolerant neural network architecture to mitigate the weight disturbance problem without involving expensive retraining. Specifically, we propose a novel collaborative logistic classifier to enhance the DNN stability by redesigning the binary classifiers augmented from both traditional error correction output code (ECOC) and modern DNN training algorithm. We also develop an optimized variable-length "decodefree" scheme to further boost the accuracy under fewer number of classifiers. Experimental results on cutting-edge DNN models and complex datasets show that the proposed fault-tolerant neural network architecture can effectively rectify the accuracy degradation against weight disturbance for DNN accelerators with low cost, thus allowing for its deployment in a variety of mainstream DNNs.},
author = {Liu, Tao and Wen, Wujie and Jiang, Lei and Wang, Yanzhi and Yang, Chengmo and Quan, Gang},
booktitle = {Proceedings - Design Automation Conference},
doi = {10.1145/3316781.3317742},
file = {::},
isbn = {9781450367257},
issn = {0738100X},
title = {{A fault-tolerant neural network architecture}},
year = {2019}
}
@article{Qin2017,
abstract = {We study the trade-offs between storage/bandwidth and prediction accuracy of neural networks that are stored in noisy media. Conventionally, it is assumed that all parameters (e.g., weight and biases) of a trained neural network are stored as binary arrays and are error-free. This assumption is based upon the implementation of error correction codes (ECCs) that correct potential bit flips in storage media. However, ECCs add storage overhead and cause bandwidth reduction when loading the trained parameters during the inference. We study the robustness of deep neural networks when bit errors exist but ECCs are turned off for different neural network models and datasets. It is observed that more sophisticated models and datasets are more vulnerable to errors in their trained parameters. We propose a simple detection approach that can universally improve the robustness, which in some cases can be improved by orders of magnitude. We also propose an alternative binary representation of the parameters such that the distortion brought by bit flips is reduced and even theoretically vanishing when the number of bits to represent a parameter increases.},
archivePrefix = {arXiv},
arxivId = {1709.06173},
author = {Qin, Minghai and Sun, Chao and Vucinic, Dejan},
eprint = {1709.06173},
file = {::},
issn = {23318422},
journal = {arXiv},
title = {{Robustness of Neural Networks against Storage Media Errors}},
year = {2017}
}
@article{Song2021,
abstract = {Deep neural networks (DNNs) have gained a strong momentum among various applications. The enormous matrix-multiplication exhibited in the above DNNs is computation and memory intensive. Resistive random-access memory crossbar (RRAM-crossbar) consisting of memristor cells can naturally carry out the matrix-vector multiplication. RRAM-crossbar-based accelerator, therefore, has two orders of magnitude of higher energy-efficiency than conventional accelerators. The imperfect fabrication process of RRAM-crossbars, however, causes various defects and process variations. These fabrication imperfections not only result in significant yield loss but also degrade the accuracy of DNNs executed on the RRAM-crossbars. In this article, we first propose an accelerator-friendly neural-network training method, by leveraging the inherent self-healing capability of the neural network, to prevent the large-weight synapses from being mapped to the imperfect memristors. Next, we propose a dynamic adjustment mechanism to extend the above method for DNNs, such as multilayer perceptrons (MLPs), wherein the imperfect-memristor induced errors can accumulate and magnify through multiple layers. Such off-device training method is a pure software solution, and it is unable to provide enough accuracy for convolutional neural networks (CNNs). Several works propose error-tolerable hardware design by allowing the retraining of CNNs on the RRAM-crossbar. Although this hardware-based on-device training method is effective, the frequent write operation on RRAM-crossbar hurt the endurance of RRAM-crossbars. Consequently, we propose a software and hardware co-design methodology to effectively preserve the classification accuracy of CNN with few on-device training iterations. The experimental results show that the proposed method can guarantee ≤1.1% loss of accuracy for resistance variations in MLP and CNN. Moreover, the proposed method can guarantee ≤1% loss of accuracy even when stuck-at-faults (SAFs) rate = 20%.},
author = {Song, Zhuoran and Sun, Yanan and Chen, Lerong and Li, Tianjian and Jing, Naifeng and Liang, Xiaoyao and Jiang, Li},
doi = {10.1109/TCAD.2020.2989373},
file = {::},
issn = {19374151},
journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
keywords = {Accelerator,deep neural network (DNN),memristor,resistance variations,resistive random-access memory crossbar (RRAM-cros,stuck-at-faults (SAFs)},
number = {1},
pages = {129--142},
title = {{ITT-RNA: Imperfection Tolerable Training for RRAM-Crossbar-Based Deep Neural-Network Accelerator}},
volume = {40},
year = {2021}
}
@article{Schorn2018b,
abstract = {Deep neural networks have become a ubiquitous tool for mastering complex classification tasks. Current research focuses on the development of power-efficient and fast neural network hardware accelerators for mobile and embedded devices. However, when used in safety-critical applications, for example autonomously operating vehicles, the reliability of such accelerators becomes a further optimization criterion which can stand in contrast to power-efficiency and latency. Furthermore, ensuring hardware reliability becomes increasingly challenging for shrinking structure widths and rising power densities in the nanometer semiconductor technology era. One solution to this challenge is the exploitation of fault tolerant parts in deep neural networks. In this paper we propose a new method for predicting the error resilience of neurons in deep neural networks and show that this method significantly improves upon existing methods in terms of accuracy as well as interpretability. We evaluate prediction accuracy by simulating hardware faults in networks trained on the CIFAR-10 and ILSVRC image classification benchmarks and protecting neurons according to the resilience estimations. In addition, we demonstrate how our resilience prediction can be used for a flexible trade-off between reliability and efficiency in neural network hardware accelerators.},
author = {Schorn, Christoph and Guntoro, Andre and Ascheid, Gerd},
doi = {10.23919/DATE.2018.8342151},
file = {::},
isbn = {9783981926316},
journal = {Proceedings of the 2018 Design, Automation and Test in Europe Conference and Exhibition, DATE 2018},
pages = {979--984},
publisher = {EDAA},
title = {{Accurate neuron resilience prediction for a flexible reliability management in neural network accelerators}},
volume = {2018-Janua},
year = {2018}
}
@article{Hoang2019,
abstract = {Deep Neural Networks (DNNs) are widely being adopted for safety-critical applications, e.g., healthcare and autonomous driving. Inherently, they are considered to be highly error-tolerant. However, recent studies have shown that hardware faults that impact the parameters of a DNN (e.g., weights) can have drastic impacts on its classification accuracy. In this paper, we perform a comprehensive error resilience analysis of DNNs subjected to hardware faults (e.g., permanent faults) in the weight memory. The outcome of this analysis is leveraged to propose a novel error mitigation technique which squashes the high-intensity faulty activation values to alleviate their impact. We achieve this by replacing the unbounded activation functions with their clipped versions. We also present a method to systematically define the clipping values of the activation functions that result in increased resilience of the networks against faults. We evaluate our technique on the AlexNet and the VGG-16 DNNs trained for the CIFAR-10 dataset. The experimental results show that our mitigation technique significantly improves the resilience of the DNNs to faults. For example, the proposed technique offers on average 68.92% improvement in the classification accuracy of resilience-optimized VGG-16 model at 1 × 10-5fault rate, when compared to the base network without any fault mitigation.},
author = {Hoang, Le Ha and Hanif, Muhammad Abdullah and Shafique, Muhammad},
file = {::},
isbn = {9783981926347},
issn = {23318422},
journal = {arXiv},
keywords = {DNN,Error Mitigation,Fault-Tolerance,Machine Learning,Reliability,Resilience,System-Level Optimization},
pages = {1241--1246},
title = {{FT-ClipAct: Resilience analysis of deep neural networks and improving their fault tolerance using clipped activation}},
year = {2019}
}
@article{Choi2019a,
abstract = {With inherent algorithmic error resilience of deep neural networks (DNNs), supply voltage scaling could be a promising technique for energy efficient DNN accelerator design. In this paper, we propose novel error resilient techniques to enable aggressive voltage scaling by exploiting different amount of error resilience (sensitivity) with respect to DNN layers, filters, and channels. First, to rapidly evaluate filter/channel-level weight sensitivities of large scale DNNs, first-order Taylor expansion is used, which accurately approximates weight sensitivity from actual error injection simulation. With measured timing error probability of each multiply-accumulate (MAC) units considering process variations, the sensitivity variation among filter weights can be leveraged to design DNN accelerator, such that the computations with more sensitive weights are assigned to more robust MAC units, while those with less sensitive weights are assigned to less robust MAC units. Based on post-synthesis timing simulations, 51% energy savings has been achieved with CIFAR-10 dataset using VGG-9 compared to state-of-the-art timing error recovery technique with the same constraint of 3% accuracy loss.},
author = {Choi, Wonseok and Shin, Dongyeob and Park, Jongsun and Ghosh, Swaroop},
doi = {10.1145/3316781.3317908},
file = {::},
isbn = {9781450367257},
issn = {0738100X},
journal = {Proceedings - Design Automation Conference},
title = {{Sensitivity based error resilient techniques for energy efficient deep neural network accelerators}},
year = {2019}
}
@article{Kim2017,
abstract = {As a result of the increasing demand for deep neural network (DNN)-based services, efforts to develop dedicated hardware accelerators for DNNs are growing rapidly. However, while accelerators with high performance and efficiency on convolutional deep neural networks (Conv-DNNs) have been developed, less progress has been made with regards to fully-connected DNNs (FC-DNNs). In this paper, we propose MATIC (Memory Adaptive Training with In-situ Canaries), a methodol-ogy that enables aggressive voltage scaling of accelerator weight memories to improve the energy-efficiency of DNN accelerators. To enable accurate operation with voltage overscaling, MATIC combines the characteristics of destructive SRAM reads with the error resilience of neural networks in a memory-adaptive training process. Furthermore, PVT-related voltage margins are eliminated using bit-cells from synaptic weights as in-situ canaries to track runtime environmental variation. Demonstrated on a low-power DNN accelerator that we fabricate in 65 nm CMOS, MATIC enables up to 60-80 mV of voltage overscaling (3.3× total energy reduction versus the nominal voltage), or 18.6× application error reduction.},
author = {Kim, Sung and Howe, Patrick and Moreau, Thierry and Alaghi, Armin and Ceze, Luis and Sathe, Visvesh},
file = {::},
isbn = {9783981926309},
issn = {23318422},
journal = {arXiv},
keywords = {'Deep neural networks,Machine learning acceleration.,SRAM,Voltage scaling},
publisher = {EDAA},
title = {{MATIC: Learning around errors for efficient low-voltage neural network accelerators}},
year = {2017}
}
@article{Chu2020,
abstract = {Deep Convolution Neural network (DCNN) pruning is an efficient way to reduce the resource and power consumption in a DCNN accelerator. Exploiting the sparsity in the weight matrices of DCNNs, however, is nontrivial if we deploy these DC-NNs in a crossbar-based Process-In-Memory (PIM) architecture, because of the crossbar structure. Structural pruning-exploiting a coarse-grained sparsity, such as filter/channel-level pruning-can result in a compressed weight matrix that fits the crossbar structure. However, this pruning method inevitably degrades the model accuracy. To solve this problem, in this paper, we propose PIM-PRUNE to exploit the finer-grained sparsity in PIM-architecture, and the resulting compressed weight matrices can significantly reduce the demand of crossbars with negligible accuracy loss.Further, we explore the design space of the crossbar, such as the crossbar size and aspect-ratio, from a new point-of-view of resource-oriented pruning. We find a trade-off existing between the pruning algorithm and the hardware overhead: a PIM with smaller crossbars is more friendly for pruning methods; however, the resulting peripheral circuit cause higher power consumption. Given a specific DCNN, we can suggest a sweet-spot of crossbar design to the optimal overall energy efficiency. Experimental results show that the proposed pruning method applied on Resnet18 can achieve up to 24.85× and 3.56× higher compression rate of occupied crossbars on CifarlO and Imagenet, respectively; while the accuracy loss is negligible, which is 4.56× and 1.99× better than the state-of-art methods.},
author = {Chu, Chaoqun and Wang, Yanzhi and Zhao, Yilong and Ma, Xiaolong and Ye, Shaokai and Hong, Yunyan and Liang, Xiaoyao and Han, Yinhe and Jiang, Li},
doi = {10.1109/DAC18072.2020.9218523},
isbn = {9781450367257},
issn = {0738100X},
journal = {Proceedings - Design Automation Conference},
keywords = {Crossbar,DCNN pruning,PIM},
title = {{PIM-Prune: Fine-Grain DCNN pruning for crossbar-based process-in-memory architecture}},
volume = {2020-July},
year = {2020}
}
@article{Li2017a,
abstract = {Deep learning neural networks (DNNs) have been successful in solving a wide range of machine learning problems. Specialized hardware accelerators have been proposed to accelerate the execution of DNN algorithms for high-performance and energy efficiency. Recently, they have been deployed in datacenters (potentially for business-critical or industrial applications) and safety-critical systems such as self-driving cars. Soft errors caused by high-energy particles have been increasing in hardware systems, and these can lead to catastrophic failures in DNN systems. Traditional methods for building resilient systems, e.g., Triple Modular Redundancy (TMR), are agnostic of the DNN algorithm and the DNN accelerator's architecture. Hence, these traditional resilience approaches incur high overheads, which makes them challenging to deploy. In this paper, we experimentally evaluate the resilience characteristics of DNN systems (i.e., DNN software running on specialized accelerators). We find that the error resilience of a DNN system depends on the data types, values, data reuses, and types of layers in the design. Based on our observations, we propose two efficient protection techniques for DNN systems.},
author = {Li, Guanpeng and Hari, Siva Kumar Sastry and Sullivan, Michael and Tsai, Timothy and Pattabiraman, Karthik and Emer, Joel and Keckler, Stephen W.},
doi = {10.1145/3126908.3126964},
file = {:C\:/Users/fffas/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2017 - Understanding error propagation in Deep Learning Neural Network (DNN) accelerators and applications.pdf:pdf},
isbn = {9781450351140},
journal = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2017},
keywords = {Deep learning,Reliability,Silent data corruption,Soft error},
pages = {1--12},
title = {{Understanding error propagation in Deep Learning Neural Network (DNN) accelerators and applications}},
year = {2017}
}
@article{Bose2021,
abstract = {Embedded systems-on-chip (SoCs) that are targeted to power autonomous vehicles of the future must meet stringent power, real-time performance, reliability, security and safety criteria in order to qualify for deployment in the field. General purpose processor cores, supported by carefully selected accelerators are needed to meet the power-performance requirements. Such heterogeneity at the hardware processing level immediately points to challenges in user- level programmability. In this paper, we present our solution strategy, and we present an updated results summary achieved after the first two phases of DARPA’s DSSoC (Domain-Specific System-on-Chip) program.},
author = {Bose, Pradip and Vega, Augusto and Watson, I B M T J and Brooks, David and Reddi, Vijay Janapa},
file = {::},
keywords = {accelerators,agile design,ease of programmability,efficiency and resilience,intelligent vehicles,system-on-chip},
title = {{Secure and Resilient SoCs for Autonomous Vehicles}},
year = {2021}
}
@article{Yang2018a,
abstract = {Deployment of convolutional neural networks (ConvNets) in always-on Internet of Everything (IoE) edge devices is severely constrained by the high memory energy consumption of hardware ConvNet implementations. Leveraging the error resilience of ConvNets by accepting bit errors at reduced voltages presents a viable option for energy savings, but few implementations utilize this due to the limited quantitative understanding of how bit errors affect performance. This paper demonstrates the efficacy of SRAM voltage scaling in a 9-layer CIFAR-10 binarized ConvNet processor, achieving memory energy savings of 3.12x with minimal accuracy degradation (~99% of nominal). Additionally, we quantify the effect of bit error accumulation in a multi-layer network and show that further energy savings are possible by splitting weight and activation voltages. Finally, we compare the measured error rates for the CIFAR-10 binarized ConvNet against MNIST networks to demonstrate the difference in bit error requirements across varying complexity in network topologies and classification tasks.},
author = {Yang, Lita and Bankman, Daniel and Moons, Bert and Verhelst, Marian and Murmann, Boris},
doi = {10.1109/ISCAS.2018.8351255},
file = {::},
isbn = {9781538648810},
issn = {02714310},
journal = {Proceedings - IEEE International Symposium on Circuits and Systems},
keywords = {BinaryNet,Convolutional neural networks,approximate SRAM,energy-accuracy trade-off,error resiliency},
title = {{Bit Error Tolerance of a CIFAR-10 Binarized Convolutional Neural Network Processor}},
volume = {2018-May},
year = {2018}
}
@article{Wang2021,
abstract = {Deep neural networks have shown outstanding performance on complex tasks. Recently, various researches have been developed to pursue fast and energy-efficient deep learning accelerators. However, devices may suffer from hard defects and hardware variability during its lifetime, which poses severe challenges to deep learning accelerators. To protect edge deep learning accelerators from fault-induced failures, we leverage the adversarial deep learning technique to tailor a lightweight online fault detection method for neural network accelerator chips. The proposed Adversarial Testing scheme (AT) is a function-level testing method outcompeting conventional test in several ways: negligible run-time overhead, super sensitivity to subtle hardware variations, which reduces chip over-kills and also the unnecessary diagnosis operations. The evaluation results show that AT can accurately detect fault occurrence and ensure the normal use of deep learning accelerator during its lifetime.},
author = {Wang, Ying},
file = {::},
journal = {VTS},
title = {{Adversarial Testing : A Novel On-line Testing Method for Deep Learning Processors}},
year = {2021}
}
@inproceedings{Khoshavi2020,
abstract = {We propose SHIELDeNN, an end-to-end inference accelerator frame-work that synergizes the mitigation approach and computational resources to realize a low-overhead error-resilient Neural Network (NN) overlay. We develop a rigorous fault assessment paradigm to delineate a ground-truth fault-skeleton map for revealing the most vulnerable parameters in NN. The error-susceptible parameters and resource constraints are given to a function to find superior design. The error-resiliency magnitude offered by SHIELDeNN can be adjusted based on the given boundaries. SHIELDeNN methodology improves the error-resiliency magnitude of cnvW1A1 by 17.19% and 96.15% for 100 MBUs that target weight and activation layers, respectively.},
author = {Khoshavi, Navid and Roohi, Arman and Broyles, Connor and Sargolzaei, Saman and Bi, Yu and Pan, David Z},
booktitle = {Proceedings - Design Automation Conference},
doi = {10.1109/DAC18072.2020.9218697},
file = {::},
isbn = {9781450367257},
issn = {0738100X},
title = {{SHIELDeNN: Online accelerated framework for fault-tolerant deep neural network architectures}},
volume = {2020-July},
year = {2020}
}
@inproceedings{Koppula2019,
abstract = {The effectiveness of deep neural networks (DNN) in vision, speech, and language processing has prompted a tremendous demand for energy-efficient high-performance DNN inference systems. Due to the increasing memory intensity of most DNN workloads, main memory can dominate the system's energy consumption and stall time. One effective way to reduce the energy consumption and increase the performance of DNN inference systems is by using approximate memory, which operates with reduced supply voltage and reduced access latency parameters that violate standard specifications. Using approximate memory reduces reliability, leading to higher bit error rates. Fortunately, neural networks have an intrinsic capacity to tolerate increased bit errors. This can enableenergy-efficient and high-performance neural network inference using approximate DRAM devices. Based on this observation, we propose EDEN, the first general framework that reduces DNN energy consumption and DNN evaluation latency by using approximate DRAM devices, while strictly meeting a user-specified target DNN accuracy. EDEN relies on two key ideas: 1) retraining the DNN for a target approximate DRAM device to increase the DNN's error tolerance, and 2) efficient mapping of the error tolerance of each individual DNN data type to a corresponding approximate DRAM partition in a way that meets the user-specified DNN accuracy requirements. We evaluate EDEN on multi-core CPUs, GPUs, and DNN accelerators with error models obtained from real approximate DRAM devices. We show that EDEN's DNN retraining technique reliably improves the error resiliency of the DNN by an order of magnitude. For a target accuracy within 1% of the original DNN, our results show that EDEN enables 1) an average DRAM energy reduction of 21%, 37%, 31%, and 32% in CPU, GPU, and two different DNN accelerator architectures, respectively, across a variety of state-ofthe- art networks, and 2) an average (maximum) speedup of 8% (17%) and 2.7% (5.5%) in CPU and GPU architectures, respectively, when evaluating latency-bound neural networks.},
archivePrefix = {arXiv},
arxivId = {1910.05340},
author = {Koppula, Skanda and Orosa, Lois and Yaglikci, A. Giray and Azizi, Roknoddin and Shahroodi, Taha and Kanellopoulos, Konstantinos and Mutlu, Onur},
booktitle = {Proceedings of the Annual International Symposium on Microarchitecture, MICRO},
doi = {10.1145/3352460.3358280},
eprint = {1910.05340},
isbn = {9781450369381},
issn = {10724451},
keywords = {DRAM,Deep neural networks,Energy efficiency,Error tolerance,Machine learning,Memory systems},
pages = {166--181},
title = {{EDEN: Enabling energy-efficient, high-performance deep neural network inference using approximate DRAM}},
year = {2019}
}
@article{Banerjee2019,
abstract = {This paper presents Bayesian Deep Learning based Fault Injection (BDLFI), a novel methodology for fault injection in neural networks (NNs) and more generally differentiable programs. BDLFI uses (1) Bayesian Deep Learning to model the propagation of faults, and (2) Markov Chain Monte Carlo inference to quantify the effect of faults on the outputs of a NN. We demonstrate BDLFI on two representative networks and present our results that challenge pre-existing results in the field.},
author = {Banerjee, Subho S. and Cyriac, James and Jha, Saurabh and Kalbarczyk, Zbigniew T. and Iyer, Ravishankar K.},
doi = {10.1109/DSN-S.2019.00018},
file = {:C\:/Users/fffas/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Banerjee et al. - 2019 - Towards a Bayesian Approach for Assessing Fault Tolerance of Deep Neural Networks(2).pdf:pdf},
isbn = {9781728130286},
journal = {Proceedings - 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume, DSN-S 2019},
keywords = {Fault Injection,Neural Networks},
pages = {25--26},
publisher = {IEEE},
title = {{Towards a Bayesian Approach for Assessing Fault Tolerance of Deep Neural Networks}},
year = {2019}
}
@misc{Santhanam2019,
abstract = {Recent progress in artificial intelligence (AI) using deep learning techniques has triggered its wide-scale use across a broad range of applications. These systems can already perform tasks such as natural language processing of voice and text, visual recognition, question-answering, recom-mendations and decision support. However, at the current level of maturity, the use of an AI component in mission-critical or safety-critical applications can have unexpected consequences. Consequently, serious concerns about relia-bility, repeatability, trust, and maintainability of AI applica-tions remain. As AI becomes pervasive despite its short-comings, more systematic ways of approaching AI software development and certification are needed. These fundamen-tal aspects establish the need for a discipline on "AI engi-neering". This paper presents the current perspective of rel-evant AI engineering concepts and some key challenges that need to be overcome to make significant progress in this important area.},
archivePrefix = {arXiv},
arxivId = {1910.12582},
author = {Santhanam, P. and Farchi, Eitan and Pankratius, Victor},
booktitle = {arXiv},
eprint = {1910.12582},
file = {::},
issn = {23318422},
pages = {1--8},
title = {{Engineering reliable deep learning systems}},
volume = {3},
year = {2019}
}
@inproceedings{Chen2017a,
abstract = {RRAM crossbar consisting of memristor devices can naturally carry out the matrix-vector multiplication; it thereby has gained a great momentum as a highly energy-efficient accelerator for neuromorphic computing. The resistance variations and stuck-at faults in the memristor devices, however, dramatically degrade not only the chip yield, but also the classification accuracy of the neural-networks running on the RRAM crossbar. Existing hardware-based solutions cause enormous overhead and power consumption, while software-based solutions are less efficient in tolerating stuck-at faults and large variations. In this paper, we propose an accelerator-friendly neural-network training method, by leveraging the inherent self-healing capability of the neural-network, to prevent the large-weight synapses from being mapped to the abnormal memristors based on the fault/variation distribution in the RRAM crossbar. Experimental results show the proposed method can pull the classification accuracy (10%-45% loss in previous works) up close to ideal level with ≤ 1% loss.},
author = {Chen, Lerong and Li, Jiawen and Chen, Yiran and Deng, Qiuping and Shen, Jiyuan and Liang, Xiaoyao and Jiang, Li},
booktitle = {Proceedings of the 2017 Design, Automation and Test in Europe, DATE 2017},
doi = {10.23919/DATE.2017.7926952},
isbn = {9783981537093},
pages = {19--24},
title = {{Accelerator-friendly neural-network training: Learning variations and defects in RRAM crossbar}},
year = {2017}
}
@article{Zhao2017,
abstract = {Neural Networks (NNs) have recently gained popularity in a wide range of modern application domains due to its superior inference accuracy. With growing problem size and complexity, modern NNs, e.g., CNNs (Convolutional NNs) and DNNs (Deep NNs), contain a large number of weights, which require tremendous efforts not only to prepare representative training datasets but also to train the network. There is an increasing demand to protect the NN weight matrices, an emerging Intellectual Property (IP) in NN field. Unfortunately, adopting conventional encryption method faces significant performance and energy consumption overheads. In this paper, we propose AEP, a DianNao based NN accelerator design for IP protection. AEP aggressively reduces DRAM timing to generate a device dependent error mask, i.e., a set of erroneous cells while the distribution of these cells are device dependent due to process variations. AEP incorporates the error mask in the NN training process so that the trained weights are device dependent, which effectively defects IP piracy as exporting the weights to other devices cannot produce satisfactory inference accuracy. In addition, AEP speeds up NN inference and achieves significant energy reduction due to the fact that main memory dominates the energy consumption in DianNao accelerator. Our evaluation results show that by injecting 0.1% to 5% memory errors, AEP has negligible inference accuracy loss on the target device while exhibiting unacceptable accuracy degradation on other devices. In addition, AEP achieves an average of 72% performance improvement and 44% energy reduction over the DianNao baseline.},
author = {Zhao, Lei and Zhang, Youtao and Yang, Jun},
doi = {10.1109/ICCAD.2017.8203854},
file = {::},
isbn = {9781538630938},
issn = {10923152},
journal = {IEEE/ACM International Conference on Computer-Aided Design, Digest of Technical Papers, ICCAD},
pages = {765--771},
title = {{AEP: An error-bearing neural network accelerator for energy efficiency and model protection}},
volume = {2017-Novem},
year = {2017}
}
@article{Torres-Huitzil2017a,
abstract = {Beyond energy, the growing number of defects in physical substrates is becoming another major constraint that affects the design of computing devices and systems. As the underlying semiconductor technologies are getting less and less reliable, the probability that some components of computing devices fail also increases, preventing designers from realizing the full potential benefits of on-chip exascale integration derived from near atomic scale feature dimensions. As the quest for performance confronts permanent and transient faults, device variation, and thermal issues, major breakthroughs in computing efficiency are expected to benefit from unconventional and new models of computation, such as brain-inspired computing. The challenge is then to find not only high-performance and energy-efficient, but also fault-tolerant computing solutions. Neural computing principles remain elusive, yet as source of a promising fault-tolerant computing paradigm. In the quest to fault tolerance can be translated into scalable and reliable computing systems, hardware design itself and/or to use circuits even with faults has further motivated research on neural networks, which are potentially capable of absorbing some degrees of vulnerability based on their natural properties. This paper presents a survey on fault tolerance in neural networks manly focusing on well-established passive techniques to exploit and improve, by design, such potential but limited intrinsic property in neural models, particularly for feedforward neural networks. First, fundamental concepts and background on fault tolerance are introduced. Then, we review fault types, models, and measures used to evaluate performance and provide a taxonomy of the main techniques to enhance the intrinsic properties of some neural models, based on the principles and mechanisms that they exploit to achieve fault tolerance passively. For completeness, we briefly review some representative works on active fault tolerance in neural networks. We present some key challenges that remain to be overcome and conclude with an outlook for this field.},
author = {Torres-Huitzil, Cesar and Girau, Bernard},
doi = {10.1109/ACCESS.2017.2742698},
file = {::},
issn = {21693536},
journal = {IEEE Access},
keywords = {Fault tolerance,fault masking,fault models,neural networks,redundancy,taxonomy},
pages = {17322--17341},
title = {{Fault and Error Tolerance in Neural Networks: A Review}},
volume = {5},
year = {2017}
}
@article{Ma2019,
abstract = {Convolutional Neural Network (CNN) accelerators are popular specialized platforms for efficient CNN processing. As semiconductor manufacturing technology scales down to nano scale, process variation dramatically affects the chip's quality. Process variation causes delay variation within the chip due to transistor parameter differences. CNN accelerators adopt a large number of Processing Elements (PEs) for parallel computing, which are highly susceptible to process variation effects. Fast CNN processing desires consistent performance among PEs, otherwise the processing speed is limited by the slowest PE within the chip. In this work, we first quantitatively model and analyze the impact of process variation on CNN accelerator's operating frequency. We further analyze the utilization of CNN accelerator and the characteristics of CNN models. We then leverage the PE underutilization to propose a sub-matrix reformation mechanism and leverage the pixel similarity of images to propose a weight transfer technique. Both techniques are able to tolerate the low-frequency PEs, and achieve performance improvement at chip level. Evaluation results show our techniques are able to achieve significant processing speed improvement with negligible accuracy loss.},
author = {Ma, Maodi and Tan, Jingweijia and Wei, Xiaohui and Yan, Kaige},
doi = {10.1109/ICCD46524.2019.00015},
file = {::},
isbn = {9781538666487},
journal = {Proceedings - 2019 IEEE International Conference on Computer Design, ICCD 2019},
keywords = {CNN accelerator,Process variation,Systolic array},
number = {Iccd},
pages = {47--55},
title = {{Process variation mitigation on convolutional neural network accelerator architecture}},
year = {2019}
}
