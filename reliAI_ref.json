{
  "Zahid2020": {
    "base": {
      "article_type": "article",
      "title": "FAT: Training Neural Networks for Reliable Inference under Hardware Faults",
      "authors": [
        "Zahid, Ussama",
        "Gambardella, Giulio",
        "Fraser, Nicholas J.",
        "Blott, Michaela",
        "Vissers, Kees"
      ],
      "abstract": "Deep neural networks (DNNs) are state-of-The-Art algorithms for multiple applications, spanning from image classification to speech recognition. While providing excellent accuracy, they often have enormous compute and memory requirements. As a result of this, quantized neural networks (QNNs) are increasingly being adopted and deployed especially on embedded devices, thanks to their high accuracy, but also since they have significantly lower compute and memory requirements compared to their floating point equivalents. QNN deployment is also being evaluated for safety-critical applications, such as automotive, avionics, medical or industrial. These systems require functional safety, guaranteeing failure-free behaviour even in the presence of hardware faults. In general fault tolerance can be achieved by adding redundancy to the system, which further exacerbates the overall computational demands and makes it difficult to meet the power and performance requirements. In order to decrease the hardware cost for achieving functional safety, it is vital to explore domain-specific solutions which can exploit the inherent features of DNNs. In this work we present a novel methodology called fault-Aware training (FAT), which includes error modeling during neural network (NN) training, to make QNNs resilient to specific fault models on the device. Our experiments show that by injecting faults in the convolutional layers during training, highly accurate convolutional neural networks (CNNs) can be trained which exhibits much better error tolerance compared to the original. Furthermore, we show that redundant systems which are built from QNNs trained with FAT achieve higher worse-case accuracy at lower hardware cost. This has been validated for numerous classification tasks including CIFAR10, GTSRB, SVHN and ImageNet.",
      "publication_year": "2020",
      "keywords": "FPGA,Functional Safety,Neural Networks,Quantized Neural Networks,Safety",
      "doi": "10.1109/ITC44778.2020.9325249"
    },
    "suggestion": {
      "relation": "This paper introduces a fault-aware training methodology to enhance the resilience of quantized neural networks against hardware faults, which aligns with the topic of hardware error fault tolerance in deep learning.",
      "suggestion": "Cite this paper to discuss methods for training deep learning models that are inherently tolerant to hardware faults.",
      "rating": 9
    },
    "sections": null
  },
  "Liu2019b": {
    "base": {
      "article_type": "article",
      "title": "INA: Incremental network approximation algorithm for limited precision deep neural networks",
      "authors": [
        "Liu, Zheyu",
        "Jia, Kaige",
        "Liu, Weiqiang",
        "Wei, Qi",
        "Qiao, Fei",
        "Yang, Huazhong"
      ],
      "abstract": "Approximate computing is a promising paradigm to deal with large computing workloads in fault-tolerant applications, providing opportunities to improve hardware efficiency of Deep Neural Networks (DNNs). However, it is still difficult to apply highly approximate arithmetics (e.g., multipliers) to DNNs due to the effect of error accumulation and the convergence problem in re-training phase. To tackle this limitation, we propose a hardware-software co-design algorithm, namely Incremental Network Approximation (INA). By addressing the convergence problem, INA promotes fault tolerance of DNNs, and yields more tradeoffs between accuracy and implementation cost. Experiments show that the approximate inference models re-trained by INA could achieve up to 80% hardware reduction in various hardware design level, while the classification accuracy degradation is less than 2%. Moreover, the experiments also exhibit the generality of INA algorithm for applying to various approximate multiplier design.",
      "publication_year": "2019",
      "keywords": "",
      "doi": "10.1109/ICCAD45719.2019.8942054"
    },
    "suggestion": {
      "relation": "The paper proposes a hardware-software co-design algorithm to improve fault tolerance in DNNs, which is relevant to the topic of hardware error fault tolerance.",
      "suggestion": "Reference this paper for examples of co-design algorithms that enhance fault tolerance in deep learning hardware.",
      "rating": 8
    },
    "sections": null
  },
  "Chiu1994": {
    "base": {
      "article_type": "article",
      "title": "Training Techniques to Obtain Fault Tolerant Neural Networks",
      "authors": [
        "Chiu, Ching-tai",
        "Mehrotra, Kishan",
        "Mohan, Chilukuri K",
        "Ranka, Sanjay"
      ],
      "abstract": "This paper addresses methods ofimproving the fault tolerance of feedforward neural nets. The ?rst method is to coerce weights to have low magnitudes during the backpropagation training process, since fault tolerance is degraded by the use ofhigh magnitude weights; at the same time, additional hidden nodes are added dynami- cally to the network to ensure that desired performance can be obtained. The second method is to add arti?- cial faults to various components (nodes and links) of a network during training. The third method is to re- peatedly remove nodes that do not signi?cantly a?ect the network output, and then add new nodes that share the load of the more critical nodes in the network. Ex- perimental results have shown that these methods can obtain better robustness than backpropagation training, and compare favorably with other approaches [1, 15].",
      "publication_year": "1994",
      "keywords": "",
      "doi": ""
    },
    "suggestion": {
      "relation": "This paper explores training techniques to improve fault tolerance in neural networks, which is pertinent to the topic of hardware error fault tolerance.",
      "suggestion": "Use this paper to discuss training strategies that can lead to more fault-tolerant deep learning models.",
      "rating": 7
    },
    "sections": null
  },
  "Deng2015": {
    "base": {
      "article_type": "article",
      "title": "Retraining-based timing error mitigation for hardware neural networks",
      "authors": [
        "Deng, Jiacnao",
        "Rang, Yuntan",
        "Du, Zidong",
        "Wang, Ymg",
        "Li, Huawei",
        "Temam, Olivier",
        "Ienne, Paolo",
        "Novo, David",
        "Li, Xiaowei",
        "Chen, Yunji",
        "Wu, Chengyong"
      ],
      "abstract": "Recently, neural network (NN) accelerators are gaining popularity as part of future heterogeneous multi-core architectures due to their broad application scope and excellent energy efficiency. Additionally, since neural networks can be retrained, they are inherently resillient to errors and noises. Prior work has utilized the error tolerance feature to design approximate neural network circuits or tolerate logical faults. However, besides high-level faults or noises, timing errors induced by delay faults, process variations, aging, etc. are dominating the reliability of NN accelerator under nanoscale manufacturing process. In this paper, we leverage the error resiliency of neural network to mitigate timing errors in NN accelerators. Specifically, when timing errors significantly affect the output results, we propose to retrain the accelerators to update their weights, thus circumventing critical timing errors. Experimental results show that timing errors in NN accelerators can be well tamed for different applications.",
      "publication_year": "2015",
      "keywords": "error tolerance,machine learning,neural networks,overclocking,timing errors",
      "doi": "10.7873/date.2015.0849"
    },
    "suggestion": {
      "relation": "The paper discusses retraining neural network accelerators to mitigate timing errors, which is a specific aspect of hardware fault tolerance in deep learning.",
      "suggestion": "Cite this paper to illustrate how retraining can be used as a strategy for hardware fault tolerance in neural network accelerators.",
      "rating": 6
    },
    "sections": null
  },
  "Schorn2018c": {
    "base": {
      "article_type": "article",
      "title": "Accurate neuron resilience prediction for a flexible reliability management in neural network accelerators",
      "authors": [
        "Schorn, Christoph",
        "Guntoro, Andre",
        "Ascheid, Gerd"
      ],
      "abstract": "Deep neural networks have become a ubiquitous tool for mastering complex classification tasks. Current research focuses on the development of power-efficient and fast neural network hardware accelerators for mobile and embedded devices. However, when used in safety-critical applications, for example autonomously operating vehicles, the reliability of such accelerators becomes a further optimization criterion which can stand in contrast to power-efficiency and latency. Furthermore, ensuring hardware reliability becomes increasingly challenging for shrinking structure widths and rising power densities in the nanometer semiconductor technology era. One solution to this challenge is the exploitation of fault tolerant parts in deep neural networks. In this paper we propose a new method for predicting the error resilience of neurons in deep neural networks and show that this method significantly improves upon existing methods in terms of accuracy as well as interpretability. We evaluate prediction accuracy by simulating hardware faults in networks trained on the CIFAR-10 and ILSVRC image classification benchmarks and protecting neurons according to the resilience estimations. In addition, we demonstrate how our resilience prediction can be used for a flexible trade-off between reliability and efficiency in neural network hardware accelerators.",
      "publication_year": "2018",
      "keywords": "",
      "doi": "10.23919/DATE.2018.8342151"
    },
    "suggestion": {
      "relation": "This paper proposes a method for predicting neuron resilience to hardware faults, which is directly related to the reliability management of neural network accelerators.",
      "suggestion": "Reference this paper to discuss predictive methods for neuron resilience in the context of hardware fault tolerance.",
      "rating": 8
    },
    "sections": null
  },
  "Li2019": {
    "base": {
      "article_type": "inproceedings",
      "title": "RRAMedy: Protecting ReRAM-based neural network from permanent and soft faults during its lifetime",
      "authors": [
        "Li, Wen",
        "Wang, Ying",
        "Li, Huawei",
        "Li, Xiaowei"
      ],
      "abstract": "The emerging memristor technology is considered a promising solution to the edge-oriented deep learning and neuromorphic processor chips because it enables power-efficient Computing-in-Memory (CiM) and normally-off architecture simultaneously. However, as the analog nature and the immature nano-scale fabrication technology, the memristive cells suffer from manufacturing defects, process variations and aging-induced variations, which may incur system and function failures in applications. How to detect and rescue from the permanent and soft faults poses a significant challenge to the edge ReRAM-based deep learning or neuromorphic chips. In this work, we propose an edge-cloud collaborative framework, RRAMedy, to achieve in-situ fault detection and network remedy for memristor-based neural accelerators. In this framework, we present Adversarial Example Testing, a lifetime on-device fault detection technique, which can accurately detect defected cells and memristor soft faults with high probability and at a low cost. Furthermore, the model accuracy can be restored by the proposed edge-cloud collaborative fault-masking retraining and model updating mechanism with a minimized edge-cloud communication overhead. The experimental results show that RRAMedy can effectively detect the memristor permanent and soft faults, protecting the neural accelerator from accuracy and performance degradation in its life cycle.",
      "publication_year": "2019",
      "keywords": "Fault Detection,Fault Masking,Hard Fault,Memristor,Reliability,Soft Fault",
      "doi": "10.1109/ICCD46524.2019.00020"
    },
    "suggestion": {
      "relation": "The paper presents a framework for detecting and correcting faults in ReRAM-based neural accelerators, which is relevant to the topic of hardware error fault tolerance.",
      "suggestion": "Cite this work to discuss fault detection and remedy mechanisms in the context of memristor-based deep learning hardware.",
      "rating": 7
    },
    "sections": null
  },
  "Gambardella2019a": {
    "base": {
      "article_type": "article",
      "title": "Efficient Error-Tolerant Quantized Neural Network Accelerators",
      "authors": [
        "Gambardella, Giulio",
        "Kappauf, Johannes",
        "Blott, Michaela",
        "Doehring, Christoph",
        "Kumm, Martin",
        "Zipf, Peter",
        "Vissers, Kees"
      ],
      "abstract": "Neural Networks are currently one of the most widely deployed machine learning algorithms. In particular, Convolutional Neural Networks (CNNs), are gaining popularity and are evaluated for deployment in safety critical applications such as self driving vehicles. Modern CNNs feature enormous memory bandwidth and high computational needs, challenging existing hardware platforms to meet throughput, latency and power requirements. Functional safety and error tolerance need to be considered as additional requirement in safety critical systems. In general, fault tolerant operation can be achieved by adding redundancy to the system, which is further exacerbating the computational demands. Furthermore, the question arises whether pruning and quantization methods for performance scaling turn out to be counterproductive with regards to fail safety requirements. In this work we present a methodology to evaluate the impact of permanent faults affecting Quantized Neural Networks (QNNs) and how to effectively decrease their effects in hardware accelerators. We use FPGA-based hardware accelerated error injection, in order to enable the fast evaluation. A detailed analysis is presented showing that QNNs containing convolutional layers are by far not as robust to faults as commonly believed and can lead to accuracy drops of up to 10%. To circumvent that, we propose two different methods to increase their robustness: 1) selective channel replication which adds significantly less redundancy than used by the common triple modular redundancy and 2) a fault-aware scheduling of processing elements for folded implementations.",
      "publication_year": "2019",
      "keywords": "Automotive,FPGA,Neural networks,Quantized neural networks,Safety",
      "doi": ""
    },
    "suggestion": {
      "relation": "This work evaluates the impact of permanent faults on QNNs and proposes methods to increase robustness, which is relevant to the topic of hardware fault tolerance.",
      "suggestion": "Use this paper to discuss the evaluation of permanent faults in QNNs and methods to mitigate their effects.",
      "rating": 8
    },
    "sections": null
  },
  "Mittal2020": {
    "base": {
      "article_type": "article",
      "title": "A survey on modeling and improving reliability of DNN algorithms and accelerators",
      "authors": [
        "Mittal, Sparsh"
      ],
      "abstract": "As DNNs become increasingly common in mission-critical applications, ensuring their reliable operation has become crucial. Conventional resilience techniques fail to account for the unique characteristics of DNN algorithms/accelerators, and hence, they are infeasible or ineffective. In this paper, we present a survey of techniques for studying and optimizing the reliability of DNN accelerators and architectures. The reliability issues we cover include soft/hard errors arising due to process variation, voltage scaling, timing errors, DRAM errors due to refresh rate scaling and thermal effects, etc. We organize the research projects on several categories to bring out their key attributes. This paper underscores the importance of designing for reliability as the first principle, and not merely retrofit for it.",
      "publication_year": "2020",
      "keywords": "Deep learning,Deep neural networks,Fault-injection,Permanent fault,Review,Transient fault",
      "doi": "10.1016/j.sysarc.2019.101689"
    },
    "suggestion": {
      "relation": "The paper is a survey on the reliability of DNN algorithms and accelerators, including techniques to optimize resilience against hardware errors.",
      "suggestion": "Cite this survey to provide a broader context on the state-of-the-art in DNN reliability and hardware error resilience.",
      "rating": 9
    },
    "sections": null
  },
  "Mahdiani2012": {
    "base": {
      "article_type": "article",
      "title": "Relaxed fault-tolerant hardware implementation of neural networks in the presence of multiple transient errors",
      "authors": [
        "Mahdiani, Hamid Reza",
        "Fakhraie, Sied Mehdi",
        "Lucas, Caro"
      ],
      "abstract": "Reliability should be identified as the most important challenge in future nano-scale very large scale integration (VLSI) implementation technologies for the development of complex integrated systems. Normally, fault tolerance (FT) in a conventional system is achieved by increasing its redundancy, which also implies higher implementation costs and lower performance that sometimes makes it even infeasible. In contrast to custom approaches, a new class of applications is categorized in this paper, which is inherently capable of absorbing some degrees of vulnerability and providing FT based on their natural properties. Neural networks are good indicators of imprecision-tolerant applications. We have also proposed a new class of FT techniques called relaxed fault-tolerant (RFT) techniques which are developed for VLSI implementation of imprecision-tolerant applications. The main advantage of RFT techniques with respect to traditional FT solutions is that they exploit inherent FT of different applications to reduce their implementation costs while improving their performance. To show the applicability as well as the efficiency of the RFT method, the experimental results for implementation of a face-recognition computationally intensive neural network and its corresponding RFT realization are presented in this paper. The results demonstrate promising higher performance of artificial neural network VLSI solutions for complex applications in faulty nano-scale implementation environments. {\\textcopyright} 2012 IEEE.",
      "publication_year": "2012",
      "keywords": "Artificial neural networks,digital hardware implementation,face recognition hardware,fault tolerant techniques,soft error,very large scale integration (VLSI)",
      "doi": "10.1109/TNNLS.2012.2199517"
    },
    "suggestion": {
      "relation": "This paper discusses the development of relaxed fault-tolerant techniques for VLSI implementation of neural networks, focusing on cost-effective fault tolerance that leverages the inherent resilience of neural networks.",
      "suggestion": "Cite this paper to discuss cost-efficient hardware fault tolerance approaches in neural network VLSI implementations.",
      "rating": 7
    },
    "sections": null
  },
  "Xu2019": {
    "base": {
      "article_type": "inproceedings",
      "title": "Safety design of a convolutional neural network accelerator with error localization and correction",
      "authors": [
        "Xu, Zheng",
        "Abraham, Jacob"
      ],
      "abstract": "Recently neural network accelerators have grown into prominence with significant power and performance efficiency improvements over CPU and GPU. In this paper, we proposed two safety design techniques include Algorithm Based Atomic Error Checking-1 (ABAEC-1) and ABAEC-2 for a Weight Stationary (WS) Convolutional Neural Network (CNN) accelerator focusing on low latency and low overhead error detection and correction with no performance degradation. The proposed design techniques not only detect the errors on-the-fly but also perform error diagnosis to localize the errors to a Processing Element (PE) for on-line fault management and recovery. We applied the design techniques on an industry quality CNN accelerator and demonstrated that we could achieve the required Diagnostic Coverage (DC) goal with minimal area and power overhead for selected configurations. Furthermore, we discussed methods to extend the proposed techniques to other dataflow architecture.",
      "publication_year": "2019",
      "keywords": "",
      "doi": "10.1109/ITC44170.2019.9000149"
    },
    "suggestion": {
      "relation": "The paper proposes safety design techniques for CNN accelerators, focusing on error detection, localization, and correction without performance degradation.",
      "suggestion": "Reference this paper for examples of error detection and correction techniques in CNN hardware accelerators.",
      "rating": 8
    },
    "sections": null
  },
  "Jere2020": {
    "base": {
      "article_type": "article",
      "title": "A singular value perspective on model robustness",
      "authors": [
        "Jere, Malhar",
        "Kumar, Maghav",
        "Koushanfar, Farinaz"
      ],
      "abstract": "Convolutional Neural Networks (CNNs) have made significant progress on several computer vision benchmarks, but are fraught with numerous non-human biases such as vulnerability to adversarial samples. Their lack of explain-ability makes identification and rectification of these biases difficult, and understanding their generalization behavior remains an open problem. In this work we explore the relationship between the generalization behavior of CNNs and the Singular Value Decomposition (SVD) of images. We show that naturally trained and adversarially robust CNNs exploit highly different features for the same dataset. We demonstrate that these features can be disentangled by SVD for ImageNet and CIFAR-10 trained networks. Finally, we propose Rank Integrated Gradients (RIG), the first rank-based feature attribution method to understand the dependence of CNNs on image rank.",
      "publication_year": "2020",
      "keywords": "",
      "doi": ""
    },
    "suggestion": {
      "relation": "This paper explores the generalization behavior of CNNs using SVD, focusing on model robustness rather than hardware fault tolerance.",
      "suggestion": "This paper might be tangentially referenced for context on model robustness in deep learning but is not directly related to hardware fault tolerance.",
      "rating": 2
    },
    "sections": null
  },
  "He2020": {
    "base": {
      "article_type": "article",
      "title": "Fidelity: Efficient resilience analysis framework for deep learning accelerators",
      "authors": [
        "He, Yi",
        "Balaprakash, Prasanna",
        "Li, Yanjing"
      ],
      "abstract": "We present a resilience analysis framework, called FIdelity, to accurately and quickly analyze the behavior of hardware errors in deep learning accelerators. Our framework enables resilience analysis starting from the very beginning of the design process to ensure that the reliability requirements are met, so that these accelerators can be safely deployed for a wide range of applications, including safety-critical applications such as self-driving cars. Existing resilience analysis techniques suffer from the following limitations: 1. general-purpose hardware techniques can achieve accurate results, but they require access to RTL to perform timeconsuming RTL simulations, which is not feasible for early design exploration; 2. general-purpose software techniques can produce results quickly, but they are highly inaccurate; 3. techniques targeting deep learning accelerators only focus on memory errors. Our FIdelity framework overcomes these limitations. FIdelity only requires a minimal amount of high-level design information that can be obtained from architectural descriptions/block diagrams, or estimated and varied for sensitivity analysis. By leveraging unique architectural properties of deep learning accelerators, we are able to systematically model a major class of hardware errors - transient errors in logic components - in software with high fidelity. Therefore, FIdelity is both quick and accurate, and does not require access to RTL. We thoroughly validate our FIdelity framework using Nvidia's open-source accelerator called NVDLA, which shows that the results are highly accurate - out of 60K fault injection experiments, the software fault models derived using FIdelity closely match the behaviors observed from RTL simulations. Using the validated FIdelity framework, we perform a large-scale resilience study on NVDLA, which consists of 46M fault injection experiments running various representative deep neural network applications. We report the key findings and architectural insights, which can be used to guide the design of future accelerators.",
      "publication_year": "2020",
      "keywords": "",
      "doi": "10.1109/MICRO50266.2020.00033"
    },
    "suggestion": {
      "relation": "FIdelity is a resilience analysis framework for deep learning accelerators that models transient errors in logic components, which is crucial for hardware error fault tolerance.",
      "suggestion": "Use this paper to discuss frameworks for analyzing and ensuring the resilience of deep learning accelerators against hardware errors.",
      "rating": 9
    },
    "sections": null
  },
  "Pandey2020": {
    "base": {
      "article_type": "article",
      "title": "GreenTPU: Predictive Design Paradigm for Improving Timing Error Resilience of a Near-Threshold Tensor Processing Unit",
      "authors": [
        "Pandey, Pramesh",
        "Basu, Prabal",
        "Chakraborty, Koushik",
        "Roy, Sanghamitra"
      ],
      "abstract": "The emergence of hardware accelerators has brought about several orders of magnitude improvement in the speed of the deep neural-network (DNN) inference. Among such DNN accelerators, the Google tensor processing unit (TPU) has transpired to be the best-in-class, offering more than 15\\times speedup over the contemporary GPUs. However, the rapid growth in several DNN workloads conspires to escalate the energy consumptions of the TPU-based data-centers. In order to restrict the energy consumption of TPUs, we propose GreenTPU - a low-power near-threshold (NTC) TPU design paradigm. To ensure a high inference accuracy at a low-voltage operation, GreenTPU identifies the patterns in the error-causing activation sequences in the systolic array, and prevents further timing errors from similar patterns by intermittently boosting the operating voltage of the specific multiplier-and-accumulator units in the TPU. Compared to a cutting-edge timing error mitigation technique for TPUs, GreenTPU enables 2\\times to 3\\times higher performance (TOPS) in an NTC TPU, with a minimal loss in the prediction accuracy.",
      "publication_year": "2020",
      "keywords": "Deep neural-network (DNN),near-threshold computing (NTC),neural network,predictive,tensor processing unit (TPU),timing error resilience",
      "doi": "10.1109/TVLSI.2020.2985057"
    },
    "suggestion": {
      "relation": "GreenTPU introduces a predictive design for a low-power TPU that mitigates timing errors, relevant to the discussion of error resilience in deep learning hardware.",
      "suggestion": "Cite this paper to discuss predictive design approaches for timing error resilience in TPUs.",
      "rating": 6
    },
    "sections": null
  },
  "Temam2012": {
    "base": {
      "article_type": "article",
      "title": "A defect-tolerant accelerator for emerging high-performance applications",
      "authors": [
        "Temam, Olivier"
      ],
      "abstract": "Due to the evolution of technology constraints, especially energy constraints which may lead to heterogeneous multi-cores, and the increasing number of defects, the design of defect-tolerant accelerators for heterogeneous multi-cores may become a major micro-architecture research issue. Most custom circuits are highly defect sensitive, a single transistor can wreck such circuits. On the contrary, artificial neural networks (ANNs) are inherently error tolerant algorithms. And the emergence of high-performance applications implementing recognition and mining tasks, for which competitive ANN-based algorithms exist, drastically expands the potential application scope of a hardware ANN accelerator. However, while the error tolerance of ANN algorithms is well documented, there are few in-depth attempts at demonstrating that an actual hardware ANN would be tolerant to faulty transistors. Most fault models are abstract and cannot demonstrate that the error tolerance of ANN algorithms can be translated into the defect tolerance of hardware ANN accelerators. In this article, we introduce a hardware ANN geared towards defect tolerance and energy efficiency, by spatially expanding the ANN. In order to precisely assess the defect tolerance capability of this hardware ANN, we introduce defects at the level of transistors, and then assess the impact of such defects on the hardware ANN functional behavior. We empirically show that the conceptual error tolerance of neural networks does translate into the defect tolerance of hardware neural networks, paving the way for their introduction in heterogeneous multi-cores as intrinsically defect-tolerant and energy-efficient accelerators. {\\textcopyright} 2012 IEEE.",
      "publication_year": "2012",
      "keywords": "",
      "doi": "10.1109/ISCA.2012.6237031"
    },
    "suggestion": {
      "relation": "This article presents a hardware ANN accelerator designed for defect tolerance and energy efficiency, directly addressing hardware fault tolerance.",
      "suggestion": "Include this paper to discuss the defect tolerance of hardware ANN accelerators in the context of hardware fault tolerance.",
      "rating": 7
    },
    "sections": null
  },
  "Nunez-Yanez2019": {
    "base": {
      "article_type": "article",
      "title": "Energy proportional neural network inference with adaptive voltage and frequency scaling",
      "authors": [
        "Nunez-Yanez, Jose"
      ],
      "abstract": "This research presents the extension and application of a voltage and frequency scaling framework called Elongate to a high-performance and reconfigurable binarized neural network. The neural network is created in the FPGA reconfigurable fabric and coupled to a multiprocessor host that controls the operational point to obtain energy proportionality. Elongate instruments a design netlist by inserting timing detectors to enable the exploitation of the operating margins of a device reliably. The elongated neural network is re-targeted to devices with different nominal operating voltages and fabricated with 28 nm (i.e., Zynq) and 16nm (i.e., Zynq Ultrascale) feature sizes showing the portability of the framework to advanced process nodes. New hardware and software components are created to support the 16nm fabric microarchitecture and a comparison in terms of power, energy and performance with the older 28 nm process is performed. The results show that Elongate can obtain new performance and energy points that are up to 86 percent better than nominal at the same level of classification accuracy. Trade-offs between energy and performance are also possible with a large dynamic range of valid working points available. The results also indicate that the built-in neural network robustness allows operation beyond the first point of error while maintaining the classification accuracy largely unaffected.",
      "publication_year": "2019",
      "keywords": "DVFS,FPGA,convolutional neural network,energy efficiency",
      "doi": "10.1109/TC.2018.2879333"
    },
    "suggestion": {
      "relation": "The research extends a voltage and frequency scaling framework to achieve energy proportionality in neural network inference, indirectly related to fault tolerance.",
      "suggestion": "Mention this paper for its approach to energy efficiency, which may complement discussions on fault tolerance in neural networks.",
      "rating": 4
    },
    "sections": null
  },
  "Wu2019": {
    "base": {
      "article_type": "article",
      "title": "Dynamic Adaptation of Approximate Bit-width for CNNs based on Quantitative Error Resilience",
      "authors": [
        "Wu, Chengjun",
        "Shan, Weiwei",
        "Xu, Jiaming"
      ],
      "abstract": "As an emerging paradigm for energy-efficiency design, approximate computing can reduce power consumption through simplification of logic circuits. Although calculation errors are caused by approximate computing, their impacts on the final results can be negligible in some error resilient applications, such as Convolutional Neural Networks (CNNs). Therefore, approximate computing has been applied to CNNs to reduce the high demand for computing resources and energy. Compared with the traditional method such as reducing data precision, this paper investigates the effect of approximate computing on the accuracy and power consumption of CNNs. To optimize the approximate computing technology applied to CNNs, we propose a method for quantifying the error resilience of each neuron by theoretical analysis and observe that error resilience varies widely across different neurons. On the basic of quantitative error resilience, dynamic adaptation of approximate bit-width and the corresponding configurable adder are proposed to fully exploit the error resilience of CNNs. Experimental results show that the proposed method further improves the performance of power consumption while maintaining high accuracy. By adopting the optimal approximate bit-width for each layer found by our proposed algorithm, dynamic adaptation of approximate bit-width reduces power consumption by more than 30% and causes less than 1% loss of the accuracy for LeNet-5.",
      "publication_year": "2019",
      "keywords": "approximate computing,configurable adder,convolutional neural network,dynamic adaptation of approximate bit-width,error resilience",
      "doi": "10.1109/NANOARCH47378.2019.181283"
    },
    "suggestion": {
      "relation": "The paper investigates the application of approximate computing to CNNs and proposes a method for dynamic adaptation of approximate bit-width based on quantitative error resilience.",
      "suggestion": "Reference this paper for discussions on approximate computing and dynamic bit-width adaptation in the context of error resilience for CNNs.",
      "rating": 5
    },
    "sections": null
  },
  "Torres-Huitzil2017": {
    "base": {
      "article_type": "article",
      "title": "Fault and Error Tolerance in Neural Networks: A Review",
      "authors": [
        "Torres-Huitzil, Cesar",
        "Girau, Bernard"
      ],
      "abstract": "Beyond energy, the growing number of defects in physical substrates is becoming another major constraint that affects the design of computing devices and systems. As the underlying semiconductor technologies are getting less and less reliable, the probability that some components of computing devices fail also increases, preventing designers from realizing the full potential benefits of on-chip exascale integration derived from near atomic scale feature dimensions. As the quest for performance confronts permanent and transient faults, device variation, and thermal issues, major breakthroughs in computing efficiency are expected to benefit from unconventional and new models of computation, such as brain-inspired computing. The challenge is then to find not only high-performance and energy-efficient, but also fault-tolerant computing solutions. Neural computing principles remain elusive, yet as source of a promising fault-tolerant computing paradigm. In the quest to fault tolerance can be translated into scalable and reliable computing systems, hardware design itself and/or to use circuits even with faults has further motivated research on neural networks, which are potentially capable of absorbing some degrees of vulnerability based on their natural properties. This paper presents a survey on fault tolerance in neural networks manly focusing on well-established passive techniques to exploit and improve, by design, such potential but limited intrinsic property in neural models, particularly for feedforward neural networks. First, fundamental concepts and background on fault tolerance are introduced. Then, we review fault types, models, and measures used to evaluate performance and provide a taxonomy of the main techniques to enhance the intrinsic properties of some neural models, based on the principles and mechanisms that they exploit to achieve fault tolerance passively. For completeness, we briefly review some representative works on active fault tolerance in neural networks. We present some key challenges that remain to be overcome and conclude with an outlook for this field.",
      "publication_year": "2017",
      "keywords": "Fault tolerance,fault masking,fault models,neural networks,redundancy,taxonomy",
      "doi": "10.1109/ACCESS.2017.2742698"
    },
    "suggestion": {
      "relation": "This paper provides a comprehensive survey on fault tolerance in neural networks, focusing on passive techniques and including a review of active fault tolerance methods, which is directly relevant to hardware error fault tolerance in deep learning.",
      "suggestion": "Use this paper to discuss the principles and taxonomy of fault tolerance techniques in neural networks, and to highlight the challenges and future outlook in the field.",
      "rating": 9
    },
    "sections": null
  },
  "Ozen2019": {
    "base": {
      "article_type": "article",
      "title": "Sanity-Check: Boosting the Reliability of Safety-Critical Deep Neural Network Applications",
      "authors": [
        "Ozen, Elbruz",
        "Orailoglu, Alex"
      ],
      "abstract": "The widespread usage of deep neural networks in autonomous driving necessitates a consideration of the safety arguments against hardware-level faults. This study confirms the possible catastrophic impact of hardware-level faults on DNN accuracy; the consequent need for low-cost fault tolerance methods can be met through a rigorous exploration of the mathematical properties of the associated computations. We propose Sanity-Check, which makes use of the linearity property and employs spatial and temporal checksums to protect fully-connected and convolutional layers in deep neural networks. Sanity-Check can be purely implemented on software and deployed on different execution platforms with no additional modification. We also propose Sanity-Check hardware which integrates seamlessly with modern DNN accelerators and neutralizes the small performance overhead in pure software implementations. Sanity-Check delivers perfect error-caused misprediction coverage in our experiments, which makes it a promising candidate for boosting the reliability of safety-critical deep neural network applications.",
      "publication_year": "2019",
      "keywords": "autonomous driving,deep neural networks,fault tolerance,reliability",
      "doi": "10.1109/ATS47505.2019.000-8"
    },
    "suggestion": {
      "relation": "The paper introduces Sanity-Check, a method for enhancing the reliability of deep neural networks against hardware-level faults, which aligns with the topic of hardware error fault tolerance in deep learning.",
      "suggestion": "Cite this paper for its specific method of using checksums for fault tolerance in deep learning, and its application in safety-critical systems like autonomous driving.",
      "rating": 8
    },
    "sections": null
  },
  "Shafique2020a": {
    "base": {
      "article_type": "misc",
      "title": "Robust machine learning systems: Challenges, current trends, perspectives, and the road ahead",
      "authors": [
        "Shafique, Muhammad",
        "Naseer, Mahum",
        "Theocharides, Theocharis",
        "Kyrkou, Christos",
        "Mutlu, Onur",
        "Orosa, Lois",
        "Choi, Jungwook"
      ],
      "abstract": "Currently, machine learning (ML) techniques are at the heart of smart cyber-physical systems (CPS) and Internet-of-Things (IoT). This article discusses various challenges and probable solutions for security attacks on these ML-inspired hardware and software techniques. - Partha Pratim Pande, Washington State University.",
      "publication_year": "2020",
      "keywords": "",
      "doi": "10.1109/MDAT.2020.2971217"
    },
    "suggestion": {
      "relation": "Although this article discusses challenges and solutions for security attacks on ML systems, it does not specifically address hardware error fault tolerance for deep learning.",
      "suggestion": "Reference this paper only if discussing the broader context of machine learning system robustness, including but not limited to hardware fault tolerance.",
      "rating": 3
    },
    "sections": null
  },
  "Leung2017": {
    "base": {
      "article_type": "article",
      "title": "A Regularizer Approach for RBF Networks Under the Concurrent Weight Failure Situation",
      "authors": [
        "Leung, Chi Sing",
        "Wan, Wai Yan",
        "Feng, Ruibin"
      ],
      "abstract": "Many existing results on fault-tolerant algorithms focus on the single fault source situation, where a trained network is affected by one kind of weight failure. In fact, a trained network may be affected by multiple kinds of weight failure. This paper first studies how the open weight fault and the multiplicative weight noise degrade the performance of radial basis function (RBF) networks. Afterward, we define the objective function for training fault-tolerant RBF networks. Based on the objective function, we then develop two learning algorithms, one batch mode and one online mode. Besides, the convergent conditions of our online algorithm are investigated. Finally, we develop a formula to estimate the test set error of faulty networks trained from our approach. This formula helps us to optimize some tuning parameters, such as RBF width.",
      "publication_year": "2017",
      "keywords": "Fault tolerance,prediction error,radial basis function (RBF),regularization",
      "doi": "10.1109/TNNLS.2016.2536172"
    },
    "suggestion": {
      "relation": "This paper explores fault-tolerant algorithms for radial basis function (RBF) networks under multiple weight failures, which is a subset of the broader topic of hardware error fault tolerance.",
      "suggestion": "Include this paper to cover fault tolerance in the specific context of RBF networks and concurrent weight failures in deep learning models.",
      "rating": 6
    },
    "sections": null
  },
  "Hari2020": {
    "base": {
      "article_type": "misc",
      "title": "Making Convolutions Resilient via Algorithm-Based Error Detection Techniques",
      "authors": [
        "Hari, Siva Kumar Sastry",
        "Sullivan, Michael B.",
        "Tsai, Timothy",
        "Keckler, Stephen W."
      ],
      "abstract": "\u2014The ability of Convolutional Neural Networks (CNNs) to accurately process real-time telemetry has boosted their use in safety-critical and high-performance computing systems. As such systems require high levels of resilience to errors, CNNs must execute correctly in the presence of hardware faults. Full duplication provides the needed assurance but incurs a prohibitive 100% overhead. Algorithmic techniques are known to offer low-cost solutions, but the practical feasibility and performance of such techniques have never been studied for CNN deployment platforms (e.g., TensorFlow or TensorRT on GPUs). In this paper, we focus on algorithmically verifying Convolutions, which are the most resource-demanding operations in CNNs. We use checksums to verify convolutions, adding a small amount of redundancy, far less than full-duplication. We first identify the challenges that arise in employing Algorithm-Based Error Detection (ABED) for Convolutions in optimized inference platforms that fuse multiple network layers and use reduced-precision operations, and demonstrate how to overcome them. We propose and evaluate variations of ABED techniques that offer implementation complexity, runtime overhead, and coverage trade-offs. Results show that ABED can detect all transient hardware errors that might otherwise corrupt output and does so while incurring low runtime overheads (6-23%), offering at least 1.6\u00d7 throughput to workloads compared to full duplication.",
      "publication_year": "2020",
      "keywords": "",
      "doi": ""
    },
    "suggestion": {
      "relation": "The paper focuses on algorithm-based error detection techniques for convolutions in CNNs, which is a crucial aspect of hardware error fault tolerance in deep learning.",
      "suggestion": "Cite this paper for its exploration of low-cost, algorithmic error detection techniques for CNNs, which are essential for hardware fault tolerance.",
      "rating": 8
    },
    "sections": null
  },
  "Li2020": {
    "base": {
      "article_type": "article",
      "title": "Soft Error Mitigation for Deep Convolution Neural Network on FPGA Accelerators",
      "authors": [
        "Li, Wenshuo",
        "Ge, Guangjun",
        "Guo, Kaiyuan",
        "Chen, Xiaoming",
        "Wei, Qi",
        "Gao, Zhen",
        "Wang, Yu",
        "Yang, Huazhong"
      ],
      "abstract": "Convolution neural networks (CNNs) have been widely used in many applications. Field-Programmable Gate Array (FPGA) based accelerator is an ideal solution for CNNs in embedded systems. However, the single event upset (SEU) effect in FPGA device may have a significant influence on the performance of CNNs. In this paper, we analyze the sensibility of CNNs to SEU and present a fault-tolerant design for CNN accelerators. First, we find that SEU in processing elements (PEs) has the worst effects on CNNs since it produces proportional errors and will not get refreshed. Furthermore, it is indicated that the large positive perturbation contributes almost all of the performance loss. Based on such observations, we propose an error detecting scheme to locate incorrect PEs and give an error masking method to achieve fault-tolerance. Experiments demonstrate that the proposed method achieves similar fault-tolerant performance with the triple modular redundancy (TMR) scheme while the overhead is much lower than it.",
      "publication_year": "2020",
      "keywords": "",
      "doi": "10.1109/AICAS48895.2020.9073925"
    },
    "suggestion": {
      "relation": "This paper presents a fault-tolerant design for CNN accelerators on FPGA, addressing the impact of single event upsets, which is highly relevant to the topic of hardware error fault tolerance.",
      "suggestion": "Utilize this paper to discuss fault-tolerant designs and error mitigation techniques for deep learning hardware accelerators, specifically FPGAs.",
      "rating": 8
    },
    "sections": null
  },
  "Chaudhuri2020": {
    "base": {
      "article_type": "article",
      "title": "Functional Criticality Classification of Structural Faults in AI Accelerators",
      "authors": [
        "Chaudhuri, Arjun",
        "Talukdar, Jonti",
        "Su, Fei",
        "Chakrabarty, Krishnendu"
      ],
      "abstract": "The ubiquitous application of deep neural networks (DNNs) has led to a rise in demand for artificial intelligence (AI) accelerators. This paper studies the problem of classifying structural faults in such an accelerator based on their functional criticality. We analyze the impact of stuck-At faults in the processing elements (PEs) of a 128 \\times 128 systolic array designed to perform classification on the MNIST dataset using both 32-bit and 16-bit data paths. We present a two-Tier machine-learning (ML) based method to assess the functional criticality of these faults. We address the problem of minimizing misclassification by utilizing generative adversarial networks (GANs). The two-Tier ML/GAN-based criticality assessment method leads to less than 1% test escapes during functional criticality evaluation.",
      "publication_year": "2020",
      "keywords": "",
      "doi": "10.1109/ITC44778.2020.9325272"
    },
    "suggestion": {
      "relation": "The study of structural faults in AI accelerators and their classification based on functional criticality pertains to the reliability and fault tolerance of hardware used in deep learning.",
      "suggestion": "Reference this paper to discuss the classification of faults in AI accelerators and the use of machine learning for evaluating functional criticality.",
      "rating": 7
    },
    "sections": null
  },
  "Mandal2019": {
    "base": {
      "article_type": "article",
      "title": "Criticality aware soft error mitigation in the configuration memory of SRAM based FPGA",
      "authors": [
        "Mandal, Swagata",
        "Sarkar, Sreetama",
        "Ming, Wong Ming",
        "Chattopadhyay, Anupam",
        "Chakrabarti, Amlan"
      ],
      "abstract": "Efficient low complexity error correcting code (ECC) is considered as an effective technique for mitigation of multi-bit upset (MBU) in the configuration memory (CM) of static random access memory (SRAM) based Field Programmable Gate Array (FPGA) devices. Traditional multi-bit ECCs have large overhead and complex decoding circuit to correct adjacent multi-bit error. In this work, we propose a simple multi-bit ECC which uses Secure Hash Algorithm for error detection and parity based two dimensional Erasure Product Code for error correction. Present error mitigation techniques perform error correction in the CM without considering the criticality or the execution period of the tasks allocated in different portion of CM. In most of the cases, error correction is not done in the right instant, which sometimes either suspends normal system operation or wastes hardware resources for less critical tasks. In this paper, we advocate for a dynamic priority-based hardware scheduling algorithm which chooses the tasks for error correction based on their area, execution period and criticality. The proposed method has been validated in terms of overhead due to redundant bits, error correction time and system reliability.",
      "publication_year": "2019",
      "keywords": "",
      "doi": "10.1109/VLSID.2019.00063"
    },
    "suggestion": {
      "relation": "The paper proposes an ECC for SRAM-based FPGA configuration memory fault mitigation, considering task criticality, which is relevant to hardware fault tolerance in deep learning systems.",
      "suggestion": "Cite this paper for its contribution to error correction techniques in FPGA configuration memory, which is a component of hardware fault tolerance strategies.",
      "rating": 7
    },
    "sections": null
  },
  "Gerasimou2020": {
    "base": {
      "article_type": "inproceedings",
      "title": "Importance-Driven Deep Learning System Testing",
      "authors": [
        "Gerasimou, Simos",
        "Eniser, Hasan Ferit",
        "Sen, Alper",
        "Cakan, Alper"
      ],
      "abstract": "Deep Learning (DL) systems are key enablers for engineering intelligent applications. Nevertheless, using DL systems in safety- A nd security-critical applications requires to provide testing evidence for their dependable operation. We introduce DeepImportance, a systematic testing methodology accompanied by an Importance-Driven (IDC) test adequacy criterion for DL systems. Applying IDC enables to establish a layer-wise functional understanding of the importance of DL system components and use this information to assess the semantic diversity of a test set. Our empirical evaluation on several DL systems and across multiple DL datasets demonstrates the usefulness and effectiveness of DeepImportance.",
      "publication_year": "2020",
      "keywords": "Deep Neural Networks,Software Testing",
      "doi": "10.1145/3377812.3390793"
    },
    "suggestion": {
      "relation": "This paper introduces a testing methodology for deep learning systems, which is tangentially related to hardware fault tolerance as it concerns the dependability of DL systems.",
      "suggestion": "Cite this paper to discuss the importance of testing methodologies in ensuring the reliability of deep learning systems in the context of hardware faults.",
      "rating": 4
    },
    "sections": null
  },
  "Itsuji2020": {
    "base": {
      "article_type": "article",
      "title": "Concurrent Detection of Failures in GPU Control Logic for Reliable Parallel Computing",
      "authors": [
        "Itsuji, Hiroaki",
        "Uezono, Takumi",
        "Toba, Tadanobu",
        "Ito, Kojiro",
        "Hashimoto, Masanori"
      ],
      "abstract": "The reliability of GPUs is becoming a major concern due to the increased probability of failures and the high vulnerability of GPUs compared to conventional CPUs in terms of tasks per failure. While there are extensive countermeasures against failures in GPU data units, there are fewer countermeasures for failures in GPU control logics. Currently, software-based techniques, such as inserting signature codes for detecting GPU control-logic failures by comparing the expected signature value with the current signature value, are being utilized. However, in the conventional software-based techniques, application calculations, signature calculations, and signature comparison calculations are executed in sequence, which degrades the application throughputs. We have developed a software-based technique that concurrently detects GPU control-logic failures in a running application while largely maintaining its throughput. Experimental results show that when our technique concurrently executed application calculations, signature calculations, and signature comparison calculations for a matrix multiplication application, the application throughput remains 78% of the original one, whereas 62% is reported in literature. We also developed fault injection simulators specialized for injecting GPU-specific control-logic faults into GPU intermediate codes and found that 100% of GPU-specific failures could be detected both during and after application execution. The proposed approach can be utilized for a wide variety of safety-And reliability-critical applications.",
      "publication_year": "2020",
      "keywords": "GPU,error detection,fault injection,reliability,soft error",
      "doi": "10.1109/ITC44778.2020.9325216"
    },
    "suggestion": {
      "relation": "The paper presents a software-based technique for detecting GPU control-logic failures, which is relevant to ensuring the reliability of parallel computing in deep learning.",
      "suggestion": "Reference this paper for examples of software-based fault detection methods that complement hardware fault tolerance strategies.",
      "rating": 6
    },
    "sections": null
  },
  "Tambe2020": {
    "base": {
      "article_type": "article",
      "title": "Algorithm-hardware co-design of adaptive floating-point encodings for resilient deep learning inference",
      "authors": [
        "Tambe, Thierry",
        "Yang, En Yu",
        "Wan, Zishen",
        "Deng, Yuntian",
        "{Janapa Reddi}, Vijay",
        "Rush, Alexander",
        "Brooks, David",
        "Wei, Gu Yeon"
      ],
      "abstract": "Conventional hardware-friendly quantization methods, such as fixed-point or integer, tend to perform poorly at very low precision as their shrunken dynamic ranges cannot adequately capture the wide data distributions commonly seen in sequence transduction models. We present an algorithm-hardware co-design centered around a novel floating-point inspired number format, AdaptivFloat, that dynamically maximizes and optimally clips its available dynamic range, at a layer granularity, in order to create faithful encodings of neural network parameters. AdaptivFloat consistently produces higher inference accuracies compared to block floating-point, uniform, IEEE-like float or posit encodings at low bit precision ( =8-bit) across a diverse set of state-of-the-art neural networks, exhibiting narrow to wide weight distribution. Notably, at 4-bit weight precision, only a 2.1 degradation in BLEU score is observed on the AdaptivFloat-quantized Transformer network compared to total accuracy loss when encoded in the above-mentioned prominent datatypes. Furthermore, experimental results on a deep neural network (DNN) processing element (PE), exploiting AdaptivFloat logic in its computational datapath, demonstrate per-operation energy and area that is 0.9\u00d7 and 1.14\u00d7, width, respectively that of an equivalent bit NVDLA-like integer-based PE.",
      "publication_year": "2020",
      "keywords": "",
      "doi": "10.1109/DAC18072.2020.9218516"
    },
    "suggestion": {
      "relation": "The paper discusses an algorithm-hardware co-design for resilient deep learning inference, which is highly relevant to hardware error fault tolerance.",
      "suggestion": "Use this paper to illustrate the concept of algorithm-hardware co-design in the context of fault-tolerant deep learning systems.",
      "rating": 8
    },
    "sections": null
  },
  "Libano2019a": {
    "base": {
      "article_type": "article",
      "title": "Selective hardening for neural networks in FPGAs",
      "authors": [
        "Libano, F.",
        "Wilson, B.",
        "Anderson, J.",
        "Wirthlin, M. J.",
        "Cazzaniga, C.",
        "Frost, C.",
        "Rech, P."
      ],
      "abstract": "Neural networks are becoming an attractive solution for automatizing vehicles in the automotive, military, and aerospace markets. Thanks to their low-cost, low-power consumption, and flexibility, field-programmable gate arrays (FPGAs) are among the promising devices to implement neural networks. Unfortunately, FPGAs are also known to be susceptible to radiation-induced errors. In this paper, we evaluate the effects of radiation-induced errors in the output correctness of two neural networks [Iris Flower artificial neural network (ANN) and Modified National Institute of Standards and Technology (MNIST) convolutional neural network (CNN)] implemented in static random-access memory-based FPGAs. In particular, we notice that radiation can induce errors that modify the output of the network with or without affecting the neural network's functionality. We call the former critical errors and the latter tolerable errors. Through exhaustive fault injection, we identify the portions of Iris Flower ANN and MNIST CNN implementation on FPGAs that are more likely, once corrupted, to generate a critical or a tolerable error. Based on this analysis, we propose a selective hardening strategy that triplicates only the most vulnerable layers of the neural network. With neutron radiation testing, our selective hardening solution was able to mask 40% of faults with a marginal 8% overhead in one of our tested neural networks.",
      "publication_year": "2019",
      "keywords": "Field-programmable gate array (FPGA),hardening,neural networks,reliability",
      "doi": "10.1109/TNS.2018.2884460"
    },
    "suggestion": {
      "relation": "This paper evaluates the impact of radiation-induced errors on neural networks in FPGAs and proposes a selective hardening strategy, directly addressing hardware fault tolerance.",
      "suggestion": "Cite this paper as a case study for hardware error fault tolerance strategies in neural network implementations on FPGAs.",
      "rating": 9
    },
    "sections": null
  },
  "Zhang2019a": {
    "base": {
      "article_type": "inproceedings",
      "title": "INVITED: Building robust machine learning systems: Current progress, research challenges, and opportunities",
      "authors": [
        "Zhang, Jeff Jun",
        "Liu, Kang",
        "Khalid, Faiq",
        "Hanif, Muhammad Abdullah",
        "Rehman, Semeen",
        "Theocharides, Theocharis",
        "Artussi, Alessandro",
        "Shafique, Muhammad",
        "Garg, Siddharth"
      ],
      "abstract": "Machine learning, in particular deep learning, is being used in almost all the aspects of life to facilitate humans, specifically in mobile and Internet of Things (IoT)-based applications. Due to its state-of-the-art performance, deep learning is also being employed in safety-critical applications, for instance, autonomous vehicles. Reliability and security are two of the key required characteristics for these applications because of the impact they can have on human's life. Towards this, in this paper, we highlight the current progress, challenges and research opportunities in the domain of robust systems for machine learning-based applications.",
      "publication_year": "2019",
      "keywords": "Adversarial Attacks,Deep Learning,Machine Learning,Permanent Faults,Reliability,Robustness,Security,Timing Errors",
      "doi": "10.1145/3316781.3323472"
    },
    "suggestion": {
      "relation": "The paper discusses the broader topic of building robust machine learning systems, which includes but is not limited to hardware fault tolerance.",
      "suggestion": "Reference this paper to provide context on the importance of robustness in machine learning systems, including hardware fault tolerance.",
      "rating": 5
    },
    "sections": null
  },
  "Schorn2019a": {
    "base": {
      "article_type": "article",
      "title": "An Efficient Bit-Flip Resilience Optimization Method for Deep Neural Networks",
      "authors": [
        "Schorn, Christoph",
        "Guntoro, Andre",
        "Ascheid, Gerd"
      ],
      "abstract": "Deep neural networks usually possess a high overall resilience against errors in their intermediate computations. However, it has been shown that error resilience is generally not homogeneous within a neural network and some neurons might be very sensitive to faults. Even a single bit-flip fault in one of these critical neuron outputs can result in a large degradation of the final network output accuracy, which cannot be tolerated in some safety-critical applications. While critical neuron computations can be protected using error correction techniques, a resilience optimization of the neural network itself is more desirable, since it can reduce the required effort for error correction and fault protection in hardware. In this paper, we develop a novel resilience optimization method for deep neural networks, which builds upon a previously proposed resilience estimation technique. The optimization involves only few steps and can be applied to pre-trained networks. In our experiments, we significantly reduce the worst-case failure rates after a bit-flip fault for deep neural networks trained on the MNIST, CIFAR-10 and ILSVRC classification benchmarks.",
      "publication_year": "2019",
      "keywords": "Deep neural networks,fault injection,fault tolerance,memory faults,resilience optimization",
      "doi": "10.23919/DATE.2019.8714885"
    },
    "suggestion": {
      "relation": "This paper focuses on optimizing deep neural networks for resilience against bit-flip faults, which is a specific aspect of hardware fault tolerance.",
      "suggestion": "Cite this paper to discuss optimization methods for neural networks that enhance their tolerance to hardware errors.",
      "rating": 7
    },
    "sections": null
  },
  "Li2020a": {
    "base": {
      "article_type": "inproceedings",
      "title": "FTT-NAS: Discovering Fault-Tolerant Neural Architecture",
      "authors": [
        "Li, Wenshuo",
        "Ning, Xuefei",
        "Ge, Guangjun",
        "Chen, Xiaoming",
        "Wang, Yu",
        "Yang, Huazhong"
      ],
      "abstract": "With the fast evolvement of deep-learning specific embedded computing systems, applications powered by deep learning are moving from the cloud to the edge. When deploying NNs onto the edge devices under complex environments, there are various types of possible faults: Soft errors caused by atmospheric neutrons and radioactive impurities, voltage instability, aging, temperature variations, and malicious attackers. Thus the safety risk of deploying neural networks at edge computing devices in safety-critic applications is now drawing much attention. In this paper, we implement the random bit-flip, Gaussian, and Salt-and-Pepper fault models and establish a multi-objective fault-tolerant neural architecture search framework. On top of the NAS framework, we propose Fault-Tolerant Neural Architecture Search (FT-NAS) to automatically discover convolutional neural network (CNN) architectures that are reliable to various faults in nowadays edge devices. Then we incorporate fault-tolerant training (FTT) in the search process to achieve better results, which we called FTT-NAS. Experiments show that the discovered architecture FT-NAS-Net and FTT-NAS-Net outperform other hand-designed baseline architectures (58.1%/86.6% VS. 10.0%/52.2%), with comparable FLOPs and less parameters. What is more, the architectures trained under a single fault model can also defend against other faults. By inspecting the discovered architecture, we find that there are redundant connections learned to protect the sensitive paths. This insight can guide future fault-tolerant neural architecture design, and we verify it by a modification on ResNet-20-ResNet-M.",
      "publication_year": "2020",
      "keywords": "",
      "doi": "10.1109/ASP-DAC47756.2020.9045324"
    },
    "suggestion": {
      "relation": "The paper presents a framework for discovering fault-tolerant neural network architectures, which is directly relevant to hardware fault tolerance in deep learning.",
      "suggestion": "Use this paper to discuss the role of neural architecture search in designing fault-tolerant deep learning systems.",
      "rating": 8
    },
    "sections": null
  },
  "Clemente2016": {
    "base": {
      "article_type": "article",
      "title": "Hardware implementation of a fault-tolerant Hopfield Neural Network on FPGAs",
      "authors": [
        "Clemente, Juan Antonio",
        "Mansour, Wassim",
        "Ayoubi, Rafic",
        "Serrano, Felipe",
        "Mecha, Hortensia",
        "Ziade, Haissam",
        "{El Falou}, Wassim",
        "Velazco, Raoul"
      ],
      "abstract": "This letter presents an FPGA implementation of a fault-tolerant Hopfield Neural Network (HNN). The robustness of this circuit against Single Event Upsets (SEUs) and Single Event Transients (SETs) has been evaluated. Results show the fault tolerance of the proposed design, compared to a previous non-fault-tolerant implementation and a solution based on triple modular redundancy (TMR) of a standard HNN design.",
      "publication_year": "2016",
      "keywords": "Artificial Neural Network (ANN),FPGA,Fault tolerance,Hopfield Neural Network (HNN),Single Event Transient (SET),Single Event Upset (SEU)",
      "doi": "10.1016/j.neucom.2015.06.038"
    },
    "suggestion": {
      "relation": "The paper details an FPGA implementation of a fault-tolerant Hopfield Neural Network, which is directly related to hardware fault tolerance.",
      "suggestion": "Cite this paper to discuss specific implementations of fault-tolerant neural networks in hardware.",
      "rating": 9
    },
    "sections": null
  },
  "Song2020a": {
    "base": {
      "article_type": "article",
      "title": "DRQ: Dynamic Region-based Quantization for Deep Neural Network Acceleration",
      "authors": [
        "Song, Zhuoran",
        "Fu, Bangqi",
        "Wu, Feiyang",
        "Jiang, Zhaoming",
        "Jiang, Li",
        "Jing, Naifeng",
        "Liang, Xiaoyao"
      ],
      "abstract": "Quantization is an effective technique for Deep Neural Network (DNN) inference acceleration. However, conventional quantization techniques are either applied at network or layer level that may fail to exploit fine-grained quantization for further speedup, or only applied on kernel weights without paying attention to the feature map dynamics that may lead to lower NN accuracy. In this paper, we propose a dynamic region-based quantization, namely DRQ, which can change the precision of a DNN model dynamically based on the sensitive regions in the feature map to achieve greater acceleration while reserving better NN accuracy. We propose an algorithm to identify the sensitive regions and an architecture that utilizes a variable-speed mixed-precision convolution array to enable the algorithm with better performance and energy efficiency. Our experiments on a wide variety of networks show that compared to a coarse-grained quantization accelerator like 'Eyeriss', DRQ can achieve 92% performance gain and 72% energy reduction with less then 1% accuracy loss. Compared to the state-of-the-art mixed-precision quantization accelerator 'OLAccel', DRQ can also achieve 21% performance gain and 33% energy reduction with 3% prediction accuracy improvement which is quite impressive for inference.",
      "publication_year": "2020",
      "keywords": "",
      "doi": "10.1109/ISCA45697.2020.00086"
    },
    "suggestion": {
      "relation": "The paper discusses dynamic quantization for DNN acceleration, which indirectly relates to fault tolerance by maintaining accuracy despite hardware constraints.",
      "suggestion": "Reference for discussing techniques to maintain deep learning performance in the presence of hardware limitations.",
      "rating": 5
    },
    "sections": null
  },
  "Karim2018a": {
    "base": {
      "article_type": "article",
      "title": "FPGA-based Fault-injection and Data Acquisition of Self-repairing Spiking Neural Network Hardware",
      "authors": [
        "Karim, Shvan",
        "Harkin, Jim",
        "McDaid, Liam",
        "Gardiner, Bryan",
        "Liu, Junxiu",
        "Halliday, David M.",
        "Tyrrell, Andy M.",
        "Timmis, Jon",
        "Millard, Alan G.",
        "Johnson, Anju P."
      ],
      "abstract": "Spiking Astrocyte-neuron Networks (SANNs) model the adaptive/repair feature of the human brain. They integrate astrocyte cells with spiking neurons to facilitate a distributed and fine-grained self-repair capability at the synapse level. SANNs are more complex with the addition of astrocyte cells and require longer simulation times, as they are dynamic over much longer time-scales than traditional neural networks. Therefore, dedicated FPGA accelerators offer reductions in simulation times. To support the acceleration of SANNs, the capability of fault injection to synapses and monitoring significant levels of neuron and astrocyte data for off-chip transmission to PC-based analysis, are required. This paper presents an FPGA-based monitoring platform (FMP) for injecting faults and capturing and analyzing data acquired from the SANN FPGA accelerator, Astrobyte. The FMP uses custom logic and a NIOS II based system to control fault injection and data monitoring on the FPGA. Results show accurate accelerated simulations of fault injection scenarios using FMP with speedups up to 65 times greater compared with equivalent Matlab implementations.",
      "publication_year": "2018",
      "keywords": "Astrocytes,Data Acquisition,FPGA acceleration,Fault injection,Self repair,Spiking neural network",
      "doi": "10.1109/ISCAS.2018.8351512"
    },
    "suggestion": {
      "relation": "This paper is directly related as it explores fault injection and data acquisition in hardware accelerators for spiking neural networks, which is a form of fault tolerance evaluation.",
      "suggestion": "Cite as an example of fault tolerance evaluation in neuromorphic hardware, which is a subset of deep learning hardware.",
      "rating": 7
    },
    "sections": null
  },
  "Tsai2021": {
    "base": {
      "article_type": "article",
      "title": "Non-Singular Adversarial Robustness of Neural Networks",
      "authors": [
        "Tsai, Yu-Lin",
        "Hsu, Chia-Yi",
        "Yu, Chia-Mu",
        "Chen, Pin-Yu"
      ],
      "abstract": "Adversarial robustness has become an emerging challenge for neural network owing to its over-sensitivity to small input perturbations. While being critical, we argue that solving this singular issue alone fails to provide a comprehensive robustness assessment. Even worse, the conclusions drawn from singular robustness may give a false sense of overall model robustness. Specifically, our findings show that adversarially trained models that are robust to input perturbations are still (or even more) vulnerable to weight perturbations when compared to standard models. In this paper, we formalize the notion of non-singular adversarial robustness for neural networks through the lens of joint perturbations to data inputs as well as model weights. To our best knowledge, this study is the first work considering simultaneous input-weight adversarial perturbations. Based on a multi-layer feed-forward neural network model with ReLU activation functions and standard classification loss, we establish error analysis for quantifying the loss sensitivity subject to $\\ell_\\infty$-norm bounded perturbations on data inputs and model weights. Based on the error analysis, we propose novel regularization functions for robust training and demonstrate improved non-singular robustness against joint input-weight adversarial perturbations.",
      "publication_year": "2021",
      "keywords": "",
      "doi": ""
    },
    "suggestion": {
      "relation": "The paper addresses adversarial robustness in neural networks, focusing on input and weight perturbations, which is tangentially related to hardware fault tolerance.",
      "suggestion": "Mention as a broader context of neural network robustness, including but not limited to hardware-induced errors.",
      "rating": 4
    },
    "sections": null
  },
  "Zhang2019c": {
    "base": {
      "article_type": "article",
      "title": "Fault-Tolerant Systolic Array Based Accelerators for Deep Neural Network Execution",
      "authors": [
        "Zhang, Jeff Jun",
        "Basu, Kanad",
        "Garg, Siddharth"
      ],
      "abstract": "Editor's note: Systolic array is embracing its renaissance after being accepted by Google TPU as the core computing architecture of machine learning acceleration. In this article, the authors propose two strategies to enhance fault tolerance of systolic array based deep neural network accelerators. - Yiran Chen, Duke University.",
      "publication_year": "2019",
      "keywords": "Deep Neural Networks,Fault Toerance,Reliability,Systolic Arrays,Testing",
      "doi": "10.1109/MDAT.2019.2915656"
    },
    "suggestion": {
      "relation": "The article proposes strategies for enhancing fault tolerance in systolic array-based DNN accelerators, which is highly relevant to hardware fault tolerance.",
      "suggestion": "Use as a primary example of fault tolerance design in DNN hardware accelerators.",
      "rating": 9
    },
    "sections": null
  },
  "Arechiga2018": {
    "base": {
      "article_type": "article",
      "title": "The Robustness of Modern Deep Learning Architectures against Single Event Upset Errors",
      "authors": [
        "Arechiga, Austin P.",
        "Michaels, Alan J."
      ],
      "abstract": "Neural Networks have revolutionized computer vision and the field has advanced so rapidly that in less than a decade neural networks are on par with human performance at image classification tasks. Applying neural networks to robotics should drastically increase the capabilities of robots to navigate and interact with the world around them. However, robots are often used to investigate environments too dangerous for humans. Such environments include areas with high levels of radiation such as the Fukushima reactor or Jovian moons. Radiation causes single event upset (SEU) errors such as unexpected bit flips so robots must be robust to those kinds of errors. Before robots can fully utilize the advances of neural networks, the networks must be tested for their robustness against SEUs. SEUs are most likely to cause bit flips within the trained parameters of a neural network. This is because the memory that stores the trained parameters takes up the most surface area of a computer, and thus is more likely to be hit by high energy particles. Previous papers in this field have focused on older networks such as Multi-Layer Perceptrons, but Convolutional Neural Networks are the current state of the art when it comes to object classification tasks. This paper tests several modern neural network architectures for their robustness to bit flips in their weights and examines which aspects of each different architecture lead to greater robustness. The different architectures tested are VGG16, ResNet50, and InceptionV3. The experiments show that all three networks display bimodal distributions under memory errors, meaning that the networks either retain their trained classification accuracy, or drop to very low accuracies. Additionally, it only takes a small number of memory errors compared to the total size of the network for the neural network's performance to significantly degrade. When comparing the three architectures, VGG16 was the least robust against random bit flips in its trained weights while ResNet50 and InceptionV3 had similar levels of robustness against SEU-type memory errors. Some possible reasons for ResNet50 and Inception V3's robustness are their use of batch normalization or their use of shortcut connections.",
      "publication_year": "2018",
      "keywords": "Machine Learning,Memory Error,Neural Networks,Robust,Single Event Upset",
      "doi": "10.1109/HPEC.2018.8547532"
    },
    "suggestion": {
      "relation": "Investigates the robustness of deep learning architectures against single event upset errors, which is a specific type of hardware fault.",
      "suggestion": "Cite for insights into the resilience of various neural network architectures to hardware errors.",
      "rating": 8
    },
    "sections": null
  },
  "Salami2018": {
    "base": {
      "article_type": "article",
      "title": "On the Resilience of RTL NN Accelerators: Fault Characterization and Mitigation",
      "authors": [
        "Salami, Behzad",
        "Unsal, Osman S.",
        "Kestelman, Adrian Cristal"
      ],
      "abstract": "Machine Learning (ML) is making a strong resurgence in tune with the massive generation of unstructured data which in turn requires massive computational resources. Due to the inherently compute and power-intensive structure of Neural Networks (NNs), hardware accelerators emerge as a promising solution. However, with technology node scaling below 10nm, hardware accelerators become more susceptible to faults, which in turn can impact the NN accuracy. In this paper, we study the resilience aspects of Register-Transfer Level (RTL) model of NN accelerators, in particular, fault characterization and mitigation. By following a High-Level Synthesis (HLS) approach, first, we characterize the vulnerability of various components of RTL NN. We observed that the severity of faults depends on both i) application-level specifications, i.e., NN data (inputs, weights, or intermediate) and NN layers and ii) architectural-level specifications, i.e., data representation model and the parallelism degree of the underlying accelerator. Second, motivated by characterization results, we present a low-overhead fault mitigation technique that can efficiently correct bit flips, by 47.3% better than state-of-the-art methods.",
      "publication_year": "2019",
      "keywords": "",
      "doi": "10.1109/CAHPC.2018.8645906"
    },
    "suggestion": {
      "relation": "The paper studies fault characterization and mitigation in NN accelerators at the RTL level, which is directly related to hardware fault tolerance.",
      "suggestion": "Cite as an example of fault characterization and mitigation techniques in hardware accelerators for neural networks.",
      "rating": 9
    },
    "sections": null
  },
  "Liu2019c": {
    "base": {
      "article_type": "inproceedings",
      "title": "A fault-tolerant neural network architecture",
      "authors": [
        "Liu, Tao",
        "Wen, Wujie",
        "Jiang, Lei",
        "Wang, Yanzhi",
        "Yang, Chengmo",
        "Quan, Gang"
      ],
      "abstract": "New DNN accelerators based on emerging technologies, such as resistive random access memory (ReRAM), are gaining increasing research attention given their potential of \"in-situ\" data processing. Unfortunately, device-level physical limitations that are unique to these technologies may cause weight disturbance in memory and thus compromising the performance and stability of DNN accelerators. In this work, we propose a novel fault-tolerant neural network architecture to mitigate the weight disturbance problem without involving expensive retraining. Specifically, we propose a novel collaborative logistic classifier to enhance the DNN stability by redesigning the binary classifiers augmented from both traditional error correction output code (ECOC) and modern DNN training algorithm. We also develop an optimized variable-length \"decodefree\" scheme to further boost the accuracy under fewer number of classifiers. Experimental results on cutting-edge DNN models and complex datasets show that the proposed fault-tolerant neural network architecture can effectively rectify the accuracy degradation against weight disturbance for DNN accelerators with low cost, thus allowing for its deployment in a variety of mainstream DNNs.",
      "publication_year": "2019",
      "keywords": "",
      "doi": "10.1145/3316781.3317742"
    },
    "suggestion": {
      "relation": "Proposes a fault-tolerant neural network architecture to address weight disturbances in DNN accelerators, which is relevant to hardware error tolerance.",
      "suggestion": "Reference for discussing architectural approaches to fault tolerance in emerging DNN hardware technologies.",
      "rating": 8
    },
    "sections": null
  },
  "Qin2017": {
    "base": {
      "article_type": "article",
      "title": "Robustness of Neural Networks against Storage Media Errors",
      "authors": [
        "Qin, Minghai",
        "Sun, Chao",
        "Vucinic, Dejan"
      ],
      "abstract": "We study the trade-offs between storage/bandwidth and prediction accuracy of neural networks that are stored in noisy media. Conventionally, it is assumed that all parameters (e.g., weight and biases) of a trained neural network are stored as binary arrays and are error-free. This assumption is based upon the implementation of error correction codes (ECCs) that correct potential bit flips in storage media. However, ECCs add storage overhead and cause bandwidth reduction when loading the trained parameters during the inference. We study the robustness of deep neural networks when bit errors exist but ECCs are turned off for different neural network models and datasets. It is observed that more sophisticated models and datasets are more vulnerable to errors in their trained parameters. We propose a simple detection approach that can universally improve the robustness, which in some cases can be improved by orders of magnitude. We also propose an alternative binary representation of the parameters such that the distortion brought by bit flips is reduced and even theoretically vanishing when the number of bits to represent a parameter increases.",
      "publication_year": "2017",
      "keywords": "",
      "doi": ""
    },
    "suggestion": {
      "relation": "Examines the robustness of neural networks stored in noisy media, which is related to the broader topic of hardware error effects on neural networks.",
      "suggestion": "Include as a study on the impact of storage media errors on neural network robustness, complementing the survey's scope.",
      "rating": 6
    },
    "sections": null
  },
  "Song2021": {
    "base": {
      "article_type": "article",
      "title": "ITT-RNA: Imperfection Tolerable Training for RRAM-Crossbar-Based Deep Neural-Network Accelerator",
      "authors": [
        "Song, Zhuoran",
        "Sun, Yanan",
        "Chen, Lerong",
        "Li, Tianjian",
        "Jing, Naifeng",
        "Liang, Xiaoyao",
        "Jiang, Li"
      ],
      "abstract": "Deep neural networks (DNNs) have gained a strong momentum among various applications. The enormous matrix-multiplication exhibited in the above DNNs is computation and memory intensive. Resistive random-access memory crossbar (RRAM-crossbar) consisting of memristor cells can naturally carry out the matrix-vector multiplication. RRAM-crossbar-based accelerator, therefore, has two orders of magnitude of higher energy-efficiency than conventional accelerators. The imperfect fabrication process of RRAM-crossbars, however, causes various defects and process variations. These fabrication imperfections not only result in significant yield loss but also degrade the accuracy of DNNs executed on the RRAM-crossbars. In this article, we first propose an accelerator-friendly neural-network training method, by leveraging the inherent self-healing capability of the neural network, to prevent the large-weight synapses from being mapped to the imperfect memristors. Next, we propose a dynamic adjustment mechanism to extend the above method for DNNs, such as multilayer perceptrons (MLPs), wherein the imperfect-memristor induced errors can accumulate and magnify through multiple layers. Such off-device training method is a pure software solution, and it is unable to provide enough accuracy for convolutional neural networks (CNNs). Several works propose error-tolerable hardware design by allowing the retraining of CNNs on the RRAM-crossbar. Although this hardware-based on-device training method is effective, the frequent write operation on RRAM-crossbar hurt the endurance of RRAM-crossbars. Consequently, we propose a software and hardware co-design methodology to effectively preserve the classification accuracy of CNN with few on-device training iterations. The experimental results show that the proposed method can guarantee \u22641.1% loss of accuracy for resistance variations in MLP and CNN. Moreover, the proposed method can guarantee \u22641% loss of accuracy even when stuck-at-faults (SAFs) rate = 20%.",
      "publication_year": "2021",
      "keywords": "Accelerator,deep neural network (DNN),memristor,resistance variations,resistive random-access memory crossbar (RRAM-cros,stuck-at-faults (SAFs)",
      "doi": "10.1109/TCAD.2020.2989373"
    },
    "suggestion": {
      "relation": "This paper discusses a training method and a software-hardware co-design methodology to address imperfections in RRAM-crossbar-based DNN accelerators, which is directly related to hardware fault tolerance in deep learning.",
      "suggestion": "Cite this paper to discuss methods for training DNNs to be tolerant to hardware imperfections and the balance between software solutions and hardware endurance.",
      "rating": 8
    },
    "sections": null
  },
  "Schorn2018b": {
    "base": {
      "article_type": "article",
      "title": "Accurate neuron resilience prediction for a flexible reliability management in neural network accelerators",
      "authors": [
        "Schorn, Christoph",
        "Guntoro, Andre",
        "Ascheid, Gerd"
      ],
      "abstract": "Deep neural networks have become a ubiquitous tool for mastering complex classification tasks. Current research focuses on the development of power-efficient and fast neural network hardware accelerators for mobile and embedded devices. However, when used in safety-critical applications, for example autonomously operating vehicles, the reliability of such accelerators becomes a further optimization criterion which can stand in contrast to power-efficiency and latency. Furthermore, ensuring hardware reliability becomes increasingly challenging for shrinking structure widths and rising power densities in the nanometer semiconductor technology era. One solution to this challenge is the exploitation of fault tolerant parts in deep neural networks. In this paper we propose a new method for predicting the error resilience of neurons in deep neural networks and show that this method significantly improves upon existing methods in terms of accuracy as well as interpretability. We evaluate prediction accuracy by simulating hardware faults in networks trained on the CIFAR-10 and ILSVRC image classification benchmarks and protecting neurons according to the resilience estimations. In addition, we demonstrate how our resilience prediction can be used for a flexible trade-off between reliability and efficiency in neural network hardware accelerators.",
      "publication_year": "2018",
      "keywords": "",
      "doi": "10.23919/DATE.2018.8342151"
    },
    "suggestion": {
      "relation": "The paper introduces a method for predicting neuron resilience in DNNs, which can be used for managing reliability in neural network hardware accelerators, aligning with the theme of hardware fault tolerance.",
      "suggestion": "Reference this paper for its approach to predicting and managing hardware faults in neural network accelerators.",
      "rating": 7
    },
    "sections": null
  },
  "Hoang2019": {
    "base": {
      "article_type": "article",
      "title": "FT-ClipAct: Resilience analysis of deep neural networks and improving their fault tolerance using clipped activation",
      "authors": [
        "Hoang, Le Ha",
        "Hanif, Muhammad Abdullah",
        "Shafique, Muhammad"
      ],
      "abstract": "Deep Neural Networks (DNNs) are widely being adopted for safety-critical applications, e.g., healthcare and autonomous driving. Inherently, they are considered to be highly error-tolerant. However, recent studies have shown that hardware faults that impact the parameters of a DNN (e.g., weights) can have drastic impacts on its classification accuracy. In this paper, we perform a comprehensive error resilience analysis of DNNs subjected to hardware faults (e.g., permanent faults) in the weight memory. The outcome of this analysis is leveraged to propose a novel error mitigation technique which squashes the high-intensity faulty activation values to alleviate their impact. We achieve this by replacing the unbounded activation functions with their clipped versions. We also present a method to systematically define the clipping values of the activation functions that result in increased resilience of the networks against faults. We evaluate our technique on the AlexNet and the VGG-16 DNNs trained for the CIFAR-10 dataset. The experimental results show that our mitigation technique significantly improves the resilience of the DNNs to faults. For example, the proposed technique offers on average 68.92% improvement in the classification accuracy of resilience-optimized VGG-16 model at 1 \u00d7 10-5fault rate, when compared to the base network without any fault mitigation.",
      "publication_year": "2019",
      "keywords": "DNN,Error Mitigation,Fault-Tolerance,Machine Learning,Reliability,Resilience,System-Level Optimization",
      "doi": ""
    },
    "suggestion": {
      "relation": "This paper presents an analysis of DNN resilience to hardware faults and proposes a fault mitigation technique, which is highly relevant to the topic of hardware error fault tolerance.",
      "suggestion": "Use this paper to illustrate a specific technique for improving DNN fault tolerance against hardware errors.",
      "rating": 9
    },
    "sections": null
  },
  "Choi2019a": {
    "base": {
      "article_type": "article",
      "title": "Sensitivity based error resilient techniques for energy efficient deep neural network accelerators",
      "authors": [
        "Choi, Wonseok",
        "Shin, Dongyeob",
        "Park, Jongsun",
        "Ghosh, Swaroop"
      ],
      "abstract": "With inherent algorithmic error resilience of deep neural networks (DNNs), supply voltage scaling could be a promising technique for energy efficient DNN accelerator design. In this paper, we propose novel error resilient techniques to enable aggressive voltage scaling by exploiting different amount of error resilience (sensitivity) with respect to DNN layers, filters, and channels. First, to rapidly evaluate filter/channel-level weight sensitivities of large scale DNNs, first-order Taylor expansion is used, which accurately approximates weight sensitivity from actual error injection simulation. With measured timing error probability of each multiply-accumulate (MAC) units considering process variations, the sensitivity variation among filter weights can be leveraged to design DNN accelerator, such that the computations with more sensitive weights are assigned to more robust MAC units, while those with less sensitive weights are assigned to less robust MAC units. Based on post-synthesis timing simulations, 51% energy savings has been achieved with CIFAR-10 dataset using VGG-9 compared to state-of-the-art timing error recovery technique with the same constraint of 3% accuracy loss.",
      "publication_year": "2019",
      "keywords": "",
      "doi": "10.1145/3316781.3317908"
    },
    "suggestion": {
      "relation": "The paper proposes error resilient techniques for DNN accelerators by exploiting inherent algorithmic error resilience, which is pertinent to the topic of hardware fault tolerance.",
      "suggestion": "Cite this paper to discuss layer, filter, and channel-level sensitivity to hardware errors and techniques to mitigate them.",
      "rating": 7
    },
    "sections": null
  },
  "Kim2017": {
    "base": {
      "article_type": "article",
      "title": "MATIC: Learning around errors for efficient low-voltage neural network accelerators",
      "authors": [
        "Kim, Sung",
        "Howe, Patrick",
        "Moreau, Thierry",
        "Alaghi, Armin",
        "Ceze, Luis",
        "Sathe, Visvesh"
      ],
      "abstract": "As a result of the increasing demand for deep neural network (DNN)-based services, efforts to develop dedicated hardware accelerators for DNNs are growing rapidly. However, while accelerators with high performance and efficiency on convolutional deep neural networks (Conv-DNNs) have been developed, less progress has been made with regards to fully-connected DNNs (FC-DNNs). In this paper, we propose MATIC (Memory Adaptive Training with In-situ Canaries), a methodol-ogy that enables aggressive voltage scaling of accelerator weight memories to improve the energy-efficiency of DNN accelerators. To enable accurate operation with voltage overscaling, MATIC combines the characteristics of destructive SRAM reads with the error resilience of neural networks in a memory-adaptive training process. Furthermore, PVT-related voltage margins are eliminated using bit-cells from synaptic weights as in-situ canaries to track runtime environmental variation. Demonstrated on a low-power DNN accelerator that we fabricate in 65 nm CMOS, MATIC enables up to 60-80 mV of voltage overscaling (3.3\u00d7 total energy reduction versus the nominal voltage), or 18.6\u00d7 application error reduction.",
      "publication_year": "2017",
      "keywords": "'Deep neural networks,Machine learning acceleration.,SRAM,Voltage scaling",
      "doi": ""
    },
    "suggestion": {
      "relation": "The paper discusses a methodology for improving energy efficiency in DNN accelerators while maintaining accuracy under voltage overscaling, which indirectly relates to fault tolerance.",
      "suggestion": "Reference this paper for its contribution to the discussion on energy efficiency and error resilience in the context of low-voltage operations.",
      "rating": 5
    },
    "sections": null
  },
  "Chu2020": {
    "base": {
      "article_type": "article",
      "title": "PIM-Prune: Fine-Grain DCNN pruning for crossbar-based process-in-memory architecture",
      "authors": [
        "Chu, Chaoqun",
        "Wang, Yanzhi",
        "Zhao, Yilong",
        "Ma, Xiaolong",
        "Ye, Shaokai",
        "Hong, Yunyan",
        "Liang, Xiaoyao",
        "Han, Yinhe",
        "Jiang, Li"
      ],
      "abstract": "Deep Convolution Neural network (DCNN) pruning is an efficient way to reduce the resource and power consumption in a DCNN accelerator. Exploiting the sparsity in the weight matrices of DCNNs, however, is nontrivial if we deploy these DC-NNs in a crossbar-based Process-In-Memory (PIM) architecture, because of the crossbar structure. Structural pruning-exploiting a coarse-grained sparsity, such as filter/channel-level pruning-can result in a compressed weight matrix that fits the crossbar structure. However, this pruning method inevitably degrades the model accuracy. To solve this problem, in this paper, we propose PIM-PRUNE to exploit the finer-grained sparsity in PIM-architecture, and the resulting compressed weight matrices can significantly reduce the demand of crossbars with negligible accuracy loss.Further, we explore the design space of the crossbar, such as the crossbar size and aspect-ratio, from a new point-of-view of resource-oriented pruning. We find a trade-off existing between the pruning algorithm and the hardware overhead: a PIM with smaller crossbars is more friendly for pruning methods; however, the resulting peripheral circuit cause higher power consumption. Given a specific DCNN, we can suggest a sweet-spot of crossbar design to the optimal overall energy efficiency. Experimental results show that the proposed pruning method applied on Resnet18 can achieve up to 24.85\u00d7 and 3.56\u00d7 higher compression rate of occupied crossbars on CifarlO and Imagenet, respectively; while the accuracy loss is negligible, which is 4.56\u00d7 and 1.99\u00d7 better than the state-of-art methods.",
      "publication_year": "2020",
      "keywords": "Crossbar,DCNN pruning,PIM",
      "doi": "10.1109/DAC18072.2020.9218523"
    },
    "suggestion": {
      "relation": "This paper explores DCNN pruning for PIM architectures to reduce resource and power consumption, which is tangentially related to hardware fault tolerance.",
      "suggestion": "Mention this paper in the context of optimizing hardware design for efficiency, which can indirectly affect fault tolerance.",
      "rating": 4
    },
    "sections": null
  },
  "Li2017a": {
    "base": {
      "article_type": "article",
      "title": "Understanding error propagation in Deep Learning Neural Network (DNN) accelerators and applications",
      "authors": [
        "Li, Guanpeng",
        "Hari, Siva Kumar Sastry",
        "Sullivan, Michael",
        "Tsai, Timothy",
        "Pattabiraman, Karthik",
        "Emer, Joel",
        "Keckler, Stephen W."
      ],
      "abstract": "Deep learning neural networks (DNNs) have been successful in solving a wide range of machine learning problems. Specialized hardware accelerators have been proposed to accelerate the execution of DNN algorithms for high-performance and energy efficiency. Recently, they have been deployed in datacenters (potentially for business-critical or industrial applications) and safety-critical systems such as self-driving cars. Soft errors caused by high-energy particles have been increasing in hardware systems, and these can lead to catastrophic failures in DNN systems. Traditional methods for building resilient systems, e.g., Triple Modular Redundancy (TMR), are agnostic of the DNN algorithm and the DNN accelerator's architecture. Hence, these traditional resilience approaches incur high overheads, which makes them challenging to deploy. In this paper, we experimentally evaluate the resilience characteristics of DNN systems (i.e., DNN software running on specialized accelerators). We find that the error resilience of a DNN system depends on the data types, values, data reuses, and types of layers in the design. Based on our observations, we propose two efficient protection techniques for DNN systems.",
      "publication_year": "2017",
      "keywords": "Deep learning,Reliability,Silent data corruption,Soft error",
      "doi": "10.1145/3126908.3126964"
    },
    "suggestion": {
      "relation": "The paper evaluates the resilience characteristics of DNN systems on specialized accelerators and proposes protection techniques, which is relevant to hardware error fault tolerance.",
      "suggestion": "Cite this paper for its experimental evaluation of DNN resilience and the proposed efficient protection techniques.",
      "rating": 8
    },
    "sections": null
  },
  "Bose2021": {
    "base": {
      "article_type": "article",
      "title": "Secure and Resilient SoCs for Autonomous Vehicles",
      "authors": [
        "Bose, Pradip",
        "Vega, Augusto",
        "Watson, I B M T J",
        "Brooks, David",
        "Reddi, Vijay Janapa"
      ],
      "abstract": "Embedded systems-on-chip (SoCs) that are targeted to power autonomous vehicles of the future must meet stringent power, real-time performance, reliability, security and safety criteria in order to qualify for deployment in the field. General purpose processor cores, supported by carefully selected accelerators are needed to meet the power-performance requirements. Such heterogeneity at the hardware processing level immediately points to challenges in user- level programmability. In this paper, we present our solution strategy, and we present an updated results summary achieved after the first two phases of DARPA\u2019s DSSoC (Domain-Specific System-on-Chip) program.",
      "publication_year": "2021",
      "keywords": "accelerators,agile design,ease of programmability,efficiency and resilience,intelligent vehicles,system-on-chip",
      "doi": ""
    },
    "suggestion": {
      "relation": "The paper addresses the broader requirements for SoCs in autonomous vehicles, including reliability, but does not focus specifically on hardware fault tolerance for deep learning.",
      "suggestion": "Reference this paper for context on the reliability requirements of SoCs in safety-critical applications like autonomous vehicles.",
      "rating": 3
    },
    "sections": null
  },
  "Yang2018a": {
    "base": {
      "article_type": "article",
      "title": "Bit Error Tolerance of a CIFAR-10 Binarized Convolutional Neural Network Processor",
      "authors": [
        "Yang, Lita",
        "Bankman, Daniel",
        "Moons, Bert",
        "Verhelst, Marian",
        "Murmann, Boris"
      ],
      "abstract": "Deployment of convolutional neural networks (ConvNets) in always-on Internet of Everything (IoE) edge devices is severely constrained by the high memory energy consumption of hardware ConvNet implementations. Leveraging the error resilience of ConvNets by accepting bit errors at reduced voltages presents a viable option for energy savings, but few implementations utilize this due to the limited quantitative understanding of how bit errors affect performance. This paper demonstrates the efficacy of SRAM voltage scaling in a 9-layer CIFAR-10 binarized ConvNet processor, achieving memory energy savings of 3.12x with minimal accuracy degradation (~99% of nominal). Additionally, we quantify the effect of bit error accumulation in a multi-layer network and show that further energy savings are possible by splitting weight and activation voltages. Finally, we compare the measured error rates for the CIFAR-10 binarized ConvNet against MNIST networks to demonstrate the difference in bit error requirements across varying complexity in network topologies and classification tasks.",
      "publication_year": "2018",
      "keywords": "BinaryNet,Convolutional neural networks,approximate SRAM,energy-accuracy trade-off,error resiliency",
      "doi": "10.1109/ISCAS.2018.8351255"
    },
    "suggestion": {
      "relation": "This paper explores the resilience of ConvNets to bit errors in hardware memory, which is directly related to the fault tolerance of hardware in deep learning applications.",
      "suggestion": "Cite this paper to discuss the potential of SRAM voltage scaling for energy savings and its impact on deep learning accuracy.",
      "rating": 7
    },
    "sections": null
  },
  "Wang2021": {
    "base": {
      "article_type": "article",
      "title": "Adversarial Testing : A Novel On-line Testing Method for Deep Learning Processors",
      "authors": [
        "Wang, Ying"
      ],
      "abstract": "Deep neural networks have shown outstanding performance on complex tasks. Recently, various researches have been developed to pursue fast and energy-efficient deep learning accelerators. However, devices may suffer from hard defects and hardware variability during its lifetime, which poses severe challenges to deep learning accelerators. To protect edge deep learning accelerators from fault-induced failures, we leverage the adversarial deep learning technique to tailor a lightweight online fault detection method for neural network accelerator chips. The proposed Adversarial Testing scheme (AT) is a function-level testing method outcompeting conventional test in several ways: negligible run-time overhead, super sensitivity to subtle hardware variations, which reduces chip over-kills and also the unnecessary diagnosis operations. The evaluation results show that AT can accurately detect fault occurrence and ensure the normal use of deep learning accelerator during its lifetime.",
      "publication_year": "2021",
      "keywords": "",
      "doi": ""
    },
    "suggestion": {
      "relation": "The paper introduces an online fault detection method for deep learning accelerators, which aligns with the theme of hardware fault tolerance in deep learning systems.",
      "suggestion": "Use this paper to highlight novel online testing methods that enhance the reliability of deep learning processors.",
      "rating": 8
    },
    "sections": null
  },
  "Khoshavi2020": {
    "base": {
      "article_type": "inproceedings",
      "title": "SHIELDeNN: Online accelerated framework for fault-tolerant deep neural network architectures",
      "authors": [
        "Khoshavi, Navid",
        "Roohi, Arman",
        "Broyles, Connor",
        "Sargolzaei, Saman",
        "Bi, Yu",
        "Pan, David Z"
      ],
      "abstract": "We propose SHIELDeNN, an end-to-end inference accelerator frame-work that synergizes the mitigation approach and computational resources to realize a low-overhead error-resilient Neural Network (NN) overlay. We develop a rigorous fault assessment paradigm to delineate a ground-truth fault-skeleton map for revealing the most vulnerable parameters in NN. The error-susceptible parameters and resource constraints are given to a function to find superior design. The error-resiliency magnitude offered by SHIELDeNN can be adjusted based on the given boundaries. SHIELDeNN methodology improves the error-resiliency magnitude of cnvW1A1 by 17.19% and 96.15% for 100 MBUs that target weight and activation layers, respectively.",
      "publication_year": "2020",
      "keywords": "",
      "doi": "10.1109/DAC18072.2020.9218697"
    },
    "suggestion": {
      "relation": "SHIELDeNN offers a framework for fault-tolerant deep neural network architectures, which is highly relevant to the topic of hardware error fault tolerance.",
      "suggestion": "Reference this paper for its approach to mitigating errors in neural network parameters and improving fault tolerance.",
      "rating": 9
    },
    "sections": null
  },
  "Koppula2019": {
    "base": {
      "article_type": "inproceedings",
      "title": "EDEN: Enabling energy-efficient, high-performance deep neural network inference using approximate DRAM",
      "authors": [
        "Koppula, Skanda",
        "Orosa, Lois",
        "Yaglikci, A. Giray",
        "Azizi, Roknoddin",
        "Shahroodi, Taha",
        "Kanellopoulos, Konstantinos",
        "Mutlu, Onur"
      ],
      "abstract": "The effectiveness of deep neural networks (DNN) in vision, speech, and language processing has prompted a tremendous demand for energy-efficient high-performance DNN inference systems. Due to the increasing memory intensity of most DNN workloads, main memory can dominate the system's energy consumption and stall time. One effective way to reduce the energy consumption and increase the performance of DNN inference systems is by using approximate memory, which operates with reduced supply voltage and reduced access latency parameters that violate standard specifications. Using approximate memory reduces reliability, leading to higher bit error rates. Fortunately, neural networks have an intrinsic capacity to tolerate increased bit errors. This can enableenergy-efficient and high-performance neural network inference using approximate DRAM devices. Based on this observation, we propose EDEN, the first general framework that reduces DNN energy consumption and DNN evaluation latency by using approximate DRAM devices, while strictly meeting a user-specified target DNN accuracy. EDEN relies on two key ideas: 1) retraining the DNN for a target approximate DRAM device to increase the DNN's error tolerance, and 2) efficient mapping of the error tolerance of each individual DNN data type to a corresponding approximate DRAM partition in a way that meets the user-specified DNN accuracy requirements. We evaluate EDEN on multi-core CPUs, GPUs, and DNN accelerators with error models obtained from real approximate DRAM devices. We show that EDEN's DNN retraining technique reliably improves the error resiliency of the DNN by an order of magnitude. For a target accuracy within 1% of the original DNN, our results show that EDEN enables 1) an average DRAM energy reduction of 21%, 37%, 31%, and 32% in CPU, GPU, and two different DNN accelerator architectures, respectively, across a variety of state-ofthe- art networks, and 2) an average (maximum) speedup of 8% (17%) and 2.7% (5.5%) in CPU and GPU architectures, respectively, when evaluating latency-bound neural networks.",
      "publication_year": "2019",
      "keywords": "DRAM,Deep neural networks,Energy efficiency,Error tolerance,Machine learning,Memory systems",
      "doi": "10.1145/3352460.3358280"
    },
    "suggestion": {
      "relation": "The paper discusses the use of approximate DRAM to improve energy efficiency in DNNs, addressing the trade-off between hardware error rates and performance.",
      "suggestion": "Include this paper to discuss how approximate memory can be leveraged for efficient DNN inference while maintaining fault tolerance.",
      "rating": 6
    },
    "sections": null
  },
  "Banerjee2019": {
    "base": {
      "article_type": "article",
      "title": "Towards a Bayesian Approach for Assessing Fault Tolerance of Deep Neural Networks",
      "authors": [
        "Banerjee, Subho S.",
        "Cyriac, James",
        "Jha, Saurabh",
        "Kalbarczyk, Zbigniew T.",
        "Iyer, Ravishankar K."
      ],
      "abstract": "This paper presents Bayesian Deep Learning based Fault Injection (BDLFI), a novel methodology for fault injection in neural networks (NNs) and more generally differentiable programs. BDLFI uses (1) Bayesian Deep Learning to model the propagation of faults, and (2) Markov Chain Monte Carlo inference to quantify the effect of faults on the outputs of a NN. We demonstrate BDLFI on two representative networks and present our results that challenge pre-existing results in the field.",
      "publication_year": "2019",
      "keywords": "Fault Injection,Neural Networks",
      "doi": "10.1109/DSN-S.2019.00018"
    },
    "suggestion": {
      "relation": "This paper presents a methodology for assessing the fault tolerance of neural networks using Bayesian Deep Learning, which is pertinent to evaluating hardware error resilience.",
      "suggestion": "Cite this paper to discuss advanced techniques for fault injection and assessment in deep learning systems.",
      "rating": 7
    },
    "sections": null
  },
  "Santhanam2019": {
    "base": {
      "article_type": "misc",
      "title": "Engineering reliable deep learning systems",
      "authors": [
        "Santhanam, P.",
        "Farchi, Eitan",
        "Pankratius, Victor"
      ],
      "abstract": "Recent progress in artificial intelligence (AI) using deep learning techniques has triggered its wide-scale use across a broad range of applications. These systems can already perform tasks such as natural language processing of voice and text, visual recognition, question-answering, recom-mendations and decision support. However, at the current level of maturity, the use of an AI component in mission-critical or safety-critical applications can have unexpected consequences. Consequently, serious concerns about relia-bility, repeatability, trust, and maintainability of AI applica-tions remain. As AI becomes pervasive despite its short-comings, more systematic ways of approaching AI software development and certification are needed. These fundamen-tal aspects establish the need for a discipline on \"AI engi-neering\". This paper presents the current perspective of rel-evant AI engineering concepts and some key challenges that need to be overcome to make significant progress in this important area.",
      "publication_year": "2019",
      "keywords": "",
      "doi": ""
    },
    "suggestion": {
      "relation": "Although focused on AI engineering, this paper indirectly touches on the reliability and maintainability of AI applications, which can include hardware fault tolerance.",
      "suggestion": "Reference this paper for a broader perspective on the challenges and needs in developing reliable deep learning systems.",
      "rating": 4
    },
    "sections": null
  },
  "Chen2017a": {
    "base": {
      "article_type": "inproceedings",
      "title": "Accelerator-friendly neural-network training: Learning variations and defects in RRAM crossbar",
      "authors": [
        "Chen, Lerong",
        "Li, Jiawen",
        "Chen, Yiran",
        "Deng, Qiuping",
        "Shen, Jiyuan",
        "Liang, Xiaoyao",
        "Jiang, Li"
      ],
      "abstract": "RRAM crossbar consisting of memristor devices can naturally carry out the matrix-vector multiplication; it thereby has gained a great momentum as a highly energy-efficient accelerator for neuromorphic computing. The resistance variations and stuck-at faults in the memristor devices, however, dramatically degrade not only the chip yield, but also the classification accuracy of the neural-networks running on the RRAM crossbar. Existing hardware-based solutions cause enormous overhead and power consumption, while software-based solutions are less efficient in tolerating stuck-at faults and large variations. In this paper, we propose an accelerator-friendly neural-network training method, by leveraging the inherent self-healing capability of the neural-network, to prevent the large-weight synapses from being mapped to the abnormal memristors based on the fault/variation distribution in the RRAM crossbar. Experimental results show the proposed method can pull the classification accuracy (10%-45% loss in previous works) up close to ideal level with \u2264 1% loss.",
      "publication_year": "2017",
      "keywords": "",
      "doi": "10.23919/DATE.2017.7926952"
    },
    "suggestion": {
      "relation": "The paper proposes a training method for neural networks that accounts for hardware defects in RRAM crossbars, relevant to hardware fault tolerance.",
      "suggestion": "Cite this paper to discuss strategies for training neural networks that are robust to specific hardware faults.",
      "rating": 8
    },
    "sections": null
  },
  "Zhao2017": {
    "base": {
      "article_type": "article",
      "title": "AEP: An error-bearing neural network accelerator for energy efficiency and model protection",
      "authors": [
        "Zhao, Lei",
        "Zhang, Youtao",
        "Yang, Jun"
      ],
      "abstract": "Neural Networks (NNs) have recently gained popularity in a wide range of modern application domains due to its superior inference accuracy. With growing problem size and complexity, modern NNs, e.g., CNNs (Convolutional NNs) and DNNs (Deep NNs), contain a large number of weights, which require tremendous efforts not only to prepare representative training datasets but also to train the network. There is an increasing demand to protect the NN weight matrices, an emerging Intellectual Property (IP) in NN field. Unfortunately, adopting conventional encryption method faces significant performance and energy consumption overheads. In this paper, we propose AEP, a DianNao based NN accelerator design for IP protection. AEP aggressively reduces DRAM timing to generate a device dependent error mask, i.e., a set of erroneous cells while the distribution of these cells are device dependent due to process variations. AEP incorporates the error mask in the NN training process so that the trained weights are device dependent, which effectively defects IP piracy as exporting the weights to other devices cannot produce satisfactory inference accuracy. In addition, AEP speeds up NN inference and achieves significant energy reduction due to the fact that main memory dominates the energy consumption in DianNao accelerator. Our evaluation results show that by injecting 0.1% to 5% memory errors, AEP has negligible inference accuracy loss on the target device while exhibiting unacceptable accuracy degradation on other devices. In addition, AEP achieves an average of 72% performance improvement and 44% energy reduction over the DianNao baseline.",
      "publication_year": "2017",
      "keywords": "",
      "doi": "10.1109/ICCAD.2017.8203854"
    },
    "suggestion": {
      "relation": "AEP focuses on an NN accelerator that incorporates hardware errors for energy efficiency and model protection, which is related to fault tolerance.",
      "suggestion": "Use this paper to discuss how intentional hardware errors can be used to protect intellectual property and improve energy efficiency.",
      "rating": 5
    },
    "sections": null
  },
  "Torres-Huitzil2017a": {
    "base": {
      "article_type": "article",
      "title": "Fault and Error Tolerance in Neural Networks: A Review",
      "authors": [
        "Torres-Huitzil, Cesar",
        "Girau, Bernard"
      ],
      "abstract": "Beyond energy, the growing number of defects in physical substrates is becoming another major constraint that affects the design of computing devices and systems. As the underlying semiconductor technologies are getting less and less reliable, the probability that some components of computing devices fail also increases, preventing designers from realizing the full potential benefits of on-chip exascale integration derived from near atomic scale feature dimensions. As the quest for performance confronts permanent and transient faults, device variation, and thermal issues, major breakthroughs in computing efficiency are expected to benefit from unconventional and new models of computation, such as brain-inspired computing. The challenge is then to find not only high-performance and energy-efficient, but also fault-tolerant computing solutions. Neural computing principles remain elusive, yet as source of a promising fault-tolerant computing paradigm. In the quest to fault tolerance can be translated into scalable and reliable computing systems, hardware design itself and/or to use circuits even with faults has further motivated research on neural networks, which are potentially capable of absorbing some degrees of vulnerability based on their natural properties. This paper presents a survey on fault tolerance in neural networks manly focusing on well-established passive techniques to exploit and improve, by design, such potential but limited intrinsic property in neural models, particularly for feedforward neural networks. First, fundamental concepts and background on fault tolerance are introduced. Then, we review fault types, models, and measures used to evaluate performance and provide a taxonomy of the main techniques to enhance the intrinsic properties of some neural models, based on the principles and mechanisms that they exploit to achieve fault tolerance passively. For completeness, we briefly review some representative works on active fault tolerance in neural networks. We present some key challenges that remain to be overcome and conclude with an outlook for this field.",
      "publication_year": "2017",
      "keywords": "Fault tolerance,fault masking,fault models,neural networks,redundancy,taxonomy",
      "doi": "10.1109/ACCESS.2017.2742698"
    },
    "suggestion": {
      "relation": "This paper provides a comprehensive survey on fault tolerance in neural networks, focusing on passive techniques and touching upon active fault tolerance, which is directly related to the resilience of deep learning systems against hardware errors.",
      "suggestion": "Use this paper to discuss the principles of fault tolerance in neural networks and review passive and active techniques relevant to hardware error resilience.",
      "rating": 8
    },
    "sections": null
  },
  "Ma2019": {
    "base": {
      "article_type": "article",
      "title": "Process variation mitigation on convolutional neural network accelerator architecture",
      "authors": [
        "Ma, Maodi",
        "Tan, Jingweijia",
        "Wei, Xiaohui",
        "Yan, Kaige"
      ],
      "abstract": "Convolutional Neural Network (CNN) accelerators are popular specialized platforms for efficient CNN processing. As semiconductor manufacturing technology scales down to nano scale, process variation dramatically affects the chip's quality. Process variation causes delay variation within the chip due to transistor parameter differences. CNN accelerators adopt a large number of Processing Elements (PEs) for parallel computing, which are highly susceptible to process variation effects. Fast CNN processing desires consistent performance among PEs, otherwise the processing speed is limited by the slowest PE within the chip. In this work, we first quantitatively model and analyze the impact of process variation on CNN accelerator's operating frequency. We further analyze the utilization of CNN accelerator and the characteristics of CNN models. We then leverage the PE underutilization to propose a sub-matrix reformation mechanism and leverage the pixel similarity of images to propose a weight transfer technique. Both techniques are able to tolerate the low-frequency PEs, and achieve performance improvement at chip level. Evaluation results show our techniques are able to achieve significant processing speed improvement with negligible accuracy loss.",
      "publication_year": "2019",
      "keywords": "CNN accelerator,Process variation,Systolic array",
      "doi": "10.1109/ICCD46524.2019.00015"
    },
    "suggestion": {
      "relation": "The paper addresses the issue of process variation in CNN accelerators and proposes mitigation techniques to ensure consistent performance, which is a crucial aspect of hardware fault tolerance in deep learning systems.",
      "suggestion": "Cite this paper for specific examples of fault tolerance techniques in the context of CNN accelerators affected by process variation.",
      "rating": 7
    },
    "sections": null
  }
}