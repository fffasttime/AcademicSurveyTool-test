Title: A survey of hardware error fault tolerance for deep learning

This literature review provides a comprehensive survey of hardware error fault tolerance strategies for deep learning (DL) systems, a critical area of research necessitated by the increasing deployment of DL in various applications and the susceptibility of advanced hardware platforms to faults. As DL technologies are integrated into safety-critical domains, ensuring computational accuracy despite hardware faults becomes paramount. This survey covers fault-tolerant hardware design tailored for DL, including novel architectures and redundancy schemes, and extends to software and algorithmic strategies for fault mitigation. It also explores methodologies for evaluating and enhancing the reliability of DL hardware, addressing the challenges in GPU and FPGA-based systems, and highlighting the role of error correction codes and fault tolerance techniques. Furthermore, the review examines future directions in fault-tolerant hardware design, such as neuromorphic computing and novel computing paradigms, aiming to illuminate the current achievements, ongoing challenges, and future prospects in developing robust and fault-tolerant DL systems.

\section{Introduction}

The relentless pursuit of advancements in deep learning (DL) technologies has ushered in an era where artificial intelligence (AI) systems are increasingly being deployed in a myriad of applications, ranging from autonomous vehicles and healthcare diagnostics to financial forecasting and natural language processing. At the heart of these DL systems are sophisticated hardware platforms, including Graphics Processing Units (GPUs), Field-Programmable Gate Arrays (FPGAs), and specialized Deep Learning Accelerators (DLAs), designed to handle the computational complexity and data-intensive nature of deep learning algorithms. However, as these hardware platforms grow more complex and are manufactured with ever-smaller nanometer-scale processes, they become more susceptible to hardware faults. These faults can arise from various sources, including manufacturing defects, wear-out over time, and transient errors caused by environmental factors such as radiation. The implications of such hardware-induced errors are particularly concerning in safety-critical applications, where computational accuracy is paramount.

In response to these challenges, the field of hardware error fault tolerance for deep learning has emerged as a critical area of research. This domain focuses on developing and evaluating strategies to ensure that deep learning computations can proceed correctly and reliably, even in the presence of hardware faults. The overarching goal is to design hardware platforms that are not only efficient and powerful but also resilient to errors that could compromise the integrity of DL computations. This literature survey aims to provide a comprehensive overview of the state-of-the-art in hardware error fault tolerance for deep learning. It encompasses a wide range of topics, including fault-tolerant hardware design, software and algorithmic strategies for fault mitigation, and methodologies for evaluating and enhancing the reliability of deep learning hardware.

The survey is structured to cover several key areas of interest. Initially, it delves into the design of fault-tolerant hardware specifically tailored for deep learning applications, exploring novel architectures, redundancy schemes, and error-resiliency mechanisms. Subsequent sections address the challenges and solutions related to ensuring fault tolerance in GPU and FPGA-based systems, which are pivotal in accelerating deep learning computations. The discussion then extends to software and algorithmic strategies that complement hardware fault tolerance, highlighting the role of error correction codes, software-implemented fault tolerance techniques, and algorithm-based fault tolerance methods. Furthermore, the survey examines methodologies for evaluating the reliability of deep learning hardware and strategies for enhancing this reliability, with a focus on fault injection tools, resilience analysis frameworks, and the impact of hardware faults on deep learning performance.

Finally, the survey looks ahead to future directions and emerging technologies that hold promise for advancing fault-tolerant hardware design for deep learning. This includes exploring the potential of neuromorphic computing, memristors, novel computing paradigms such as quantum computing and analog neural networks, and the development of hybrid and specialized architectures. Through this comprehensive examination, the survey aims to shed light on the current achievements, ongoing challenges, and future prospects in the quest for robust and fault-tolerant deep learning systems.
\section{Designing Fault-Tolerant Hardware for Deep Learning}

This section explores various approaches and architectures proposed for enhancing the fault tolerance of hardware specifically designed for deep learning applications. It covers novel architectures, redundancy schemes, and error-resiliency mechanisms that aim to mitigate the impact of hardware faults on deep learning computations. The discussion is structured around several key themes, including lightweight error-resiliency mechanisms, novel fault-tolerant architectures, selective mitigation techniques, and innovative coding strategies.

\subsection{Lightweight Error-Resiliency Mechanisms}

A primary concern in designing fault-tolerant hardware for deep learning is the balance between reliability and resource overhead. \cite{goldstein_lightweight_2021} introduces a low-cost error-resiliency scheme targeting Multiply-and-Accumulate (MAC) units in DNN accelerators. This approach achieves significant fault coverage with minimal area and power overhead, demonstrating the feasibility of enhancing reliability in power-constrained environments. Similarly, \cite{ozen_boosting_2020} proposes a median feature selection technique as a fine-grained modular redundancy scheme that leverages the inherent redundancy of neural networks without requiring additional parameters or operations, significantly improving bit-error resilience with negligible overheads.

\subsection{Novel Fault-Tolerant Architectures}

The development of novel architectures that inherently support fault tolerance is another critical area of research. \cite{bal_novel_2023} presents a fault-tolerant architecture for tiled matrix multiplication, a core operation in CNNs, by incorporating Algorithm-Based Fault Tolerance (ABFT) directly into hardware. This approach allows for concurrent error detection at the tile level without impacting performance, showcasing a direct application to deep learning computations. In a similar vein, \cite{barbirotta_fault-tolerant_2023} explores the integration of redundant vector co-processors in a RISC-V architecture, enhancing the fault tolerance of high-performance edge-computing nodes for deep learning applications.

\subsection{Selective Mitigation and Advanced Coding Strategies}

Selective mitigation techniques and advanced coding strategies offer targeted solutions to enhance fault tolerance. \cite{adam_selective_2021} demonstrates a selective mitigation technique for soft errors in DNN models used in healthcare applications, significantly reducing error-induced malfunctions with minimal performance overhead. This approach underscores the importance of fault tolerance in safety-critical systems. Furthermore, \cite{yu_cola_nodate} explores error coding and learning strategies to improve neural network inference robustness against hardware defects, introducing a comprehensive error decorrelation framework that enhances both clean and robust accuracy.

\cite{nguyen_craft_2023} proposes CRAFT, a criticality-aware fault-tolerance enhancement technique for emerging memories-based DNNs, addressing stuck-at faults to enhance reliability with minimal storage overhead. This technique exemplifies the potential of encoding methods in improving fault tolerance. Additionally, \cite{fuengfusin_efficient_2023} introduces an efficient repetition coding method for protecting DNNs from write-errors in non-volatile memory devices, showcasing an innovative approach to fault tolerance in emerging memory technologies.

\subsection{Discussion and Future Directions}

The literature reviewed in this section highlights significant advancements in designing fault-tolerant hardware for deep learning. While these studies demonstrate the feasibility of various strategies��from lightweight error-resiliency mechanisms to novel fault-tolerant architectures and selective mitigation techniques��the challenge remains in balancing reliability, performance, and resource overhead. Future research should focus on developing more adaptive and scalable fault-tolerance solutions that can be easily integrated into existing and emerging deep learning hardware platforms. Additionally, exploring the synergy between hardware fault tolerance and algorithmic resilience could offer new pathways to robust deep learning systems capable of operating reliably in increasingly complex and unpredictable environments.

\section{Fault Tolerance in GPU and FPGA-based Systems}

This section delves into the challenges and solutions for ensuring fault tolerance in GPU and FPGA-based systems, which are commonly used for deep learning computations. It discusses strategies for mitigating soft errors, permanent faults, and the impact of such faults on the reliability of CNNs and other deep learning models. The discussion is structured around the exploration of fault tolerance techniques in FPGA-based systems and the evaluation and mitigation of faults in GPU-accelerated deep learning applications.

\subsection{Fault Tolerance Techniques in FPGA-based Systems}

Field-Programmable Gate Arrays (FPGAs) are increasingly utilized in deep learning applications due to their parallel computing capabilities and good reconfigurability. However, FPGAs are susceptible to single-event upsets (SEUs) and other faults that can significantly impact the reliability of deep learning computations. \cite{gao_methodology_2023} presents a methodology for protecting parallel digital channelizers on SRAM-FPGAs against SEUs, introducing an enhanced coding scheme that minimizes the effect of quantization noise and improves fault detection probability. This approach, while initially designed for satellite communications, offers valuable insights into fault tolerance strategies that could be adapted for deep learning hardware.

Automated design methods for fault-tolerant hardware, as discussed in \cite{lojda_automated_2023}, highlight the potential of design automation in minimizing human intervention in the incorporation of fault tolerance mechanisms into FPGA-based systems. This method's application in designing an experimental fault-tolerant dynamic partial reconfiguration controller for FPGAs demonstrates the feasibility of automated fault tolerance design, which could be particularly beneficial for deep learning accelerators.

Efficient error detection techniques, as proposed in \cite{libano_efficient_2023} for matrix multiplication with systolic arrays on FPGAs, underscore the importance of algorithm-based hardening solutions. These solutions offer a more efficient alternative to coarse-grain duplication, with minimal overhead, and are directly relevant to deep learning computations.

\cite{garcia-astudillo_error_2024} introduces Optimized Redundancy for Composite Algorithms (ORCA), a novel hardening technique that uses complementary modules for error mitigation. This technique not only reduces overhead but also maintains precision, showcasing an innovative approach to fault tolerance in FPGA-based deep learning systems.

\subsection{Evaluating and Mitigating Faults in GPU-accelerated Deep Learning}

Graphics Processing Units (GPUs) are pivotal in accelerating deep learning applications. However, their susceptibility to both transient and permanent faults necessitates comprehensive strategies for fault tolerance. \cite{condia_multi-level_2022} evaluates the impact of permanent GPU faults on CNN reliability, highlighting the significance of understanding how hardware errors affect deep learning performance over time.

A software-based strategy for handling hardware-induced errors in GPUs is presented in \cite{rech_efficient_2013}, offering a case study of fault tolerance in GPU-based deep learning computations. This work emphasizes the need for optimized and experimentally tuned hardening strategies to maintain computational reliability.

\cite{santos_analyzing_2019} evaluates and proposes strategies to improve CNN reliability on GPUs, focusing on fault propagation and correction. The study's findings on error correcting codes and algorithm-based fault-tolerance techniques provide a foundation for improving deep learning reliability on GPU hardware.

The integration of fault tolerance into core deep learning computational operations on GPUs is explored in \cite{wu_anatomy_2023}, which presents a high-performance GPU-based General Matrix Multiplication (GEMM) with an algorithm-based fault tolerance scheme. This work demonstrates the feasibility of maintaining high performance while ensuring fault tolerance, which is crucial for deep learning computations.

\cite{kosaian_arithmetic-intensity-guided_2021} proposes an adaptive fault tolerance approach for neural network inference on GPUs, considering arithmetic intensity. This novel approach to adaptive fault tolerance highlights the potential for optimizing fault tolerance mechanisms based on the computational characteristics of different neural network layers.

\subsection{Discussion and Future Directions}

The exploration of fault tolerance techniques in FPGA and GPU-based systems reveals a diverse range of strategies, from enhanced coding schemes and automated design methods to software-based strategies and algorithm-based fault tolerance. These approaches not only address the immediate challenges of ensuring computational reliability in deep learning applications but also open avenues for future research.

One potential direction is the further development of adaptive fault tolerance mechanisms that dynamically adjust based on the computational context and the specific requirements of deep learning models. Additionally, the integration of fault tolerance techniques into the design of new hardware accelerators for deep learning could significantly enhance the reliability of these systems.

Moreover, the exploration of cross-layer fault tolerance strategies, which span hardware, software, and algorithmic levels, could offer comprehensive solutions to the challenges of ensuring deep learning reliability. Such strategies would necessitate close collaboration between hardware designers, software developers, and machine learning researchers to achieve optimal fault tolerance without compromising performance.

In conclusion, as deep learning applications continue to proliferate across various domains, the importance of hardware fault tolerance cannot be overstated. The ongoing research and development in this field will play a critical role in enabling the reliable deployment of deep learning models in safety-critical and high-stakes environments.

\printbibliography
\section{Software and Algorithmic Strategies for Fault Tolerance}

This section reviews software and algorithmic strategies that complement hardware fault tolerance in deep learning systems. It includes discussions on error correction codes, software-implemented fault tolerance techniques, and algorithm-based fault tolerance (ABFT) methods that enhance the resilience of deep learning computations against hardware errors. The emergence of deep neural networks (DNNs) in safety-critical domains has brought to the forefront the need for reliable and fault-tolerant systems. As hardware errors become increasingly prevalent with the miniaturization of electronic components, software and algorithmic strategies offer a viable path to ensuring the robustness of deep learning applications.

\subsection{Innovative Error Correction and Fault Mitigation Strategies}

Chen et al. \cite{chen_low-cost_2021} introduce Ranger, a low-cost fault corrector for DNNs, which mitigates the impact of hardware transient faults through range restriction. This innovative approach transforms critical faults into benign ones, leveraging the inherent resilience of DNNs without re-computation. Ranger exemplifies cost-effective strategies for correcting hardware-induced errors in DNNs, emphasizing the importance of range restriction.

Traiola et al. \cite{traiola_machine-learning-guided_2023} propose a machine-learning-guided framework for fault-tolerant DNNs, which assesses the fault tolerance of DNN parameters and protects them cost-effectively. By using statistical fault injection and machine learning methods, this framework selectively inserts Error Correction Codes (ECCs) to protect only the critical parameters, demonstrating the potential of machine learning in enhancing fault tolerance efficiently.

Gambardella et al. \cite{gambardella_accelerated_2022} discuss the resilience of quantized neural networks (QNNs) to single-event-effects in FPGA, highlighting the benefits of fault-aware training (FAT). FAT enhances hardware fault tolerance in deep learning models, showcasing innovative training methodologies that account for soft errors during neural network training.

\subsection{Software-Level Approaches and Dynamic Error Handling}

Shi et al. \cite{shi_automated_2023} evaluate the impact of space-environment-induced software errors on object detection algorithms and propose an automated model hardening framework with reinforcement learning. This framework searches for error-sensitive kernels in a CNN and applies fine-grained modular-level redundancy, significantly improving fault tolerance. This study underscores the potential of software-level approaches in enhancing the resilience of deep learning systems in harsh environments.

Shi et al. \cite{shi_average_2023} introduce dynamic approaches for minimizing execution time under soft error constraints in safety-critical systems. By leveraging error history and actual error probabilities, these approaches optimize error handling dynamically, complementing hardware fault tolerance in deep learning systems.

Hsieh et al. \cite{hsieh_cost-effective_2023} present a cost-effective memory protection method for machine learning systems based on machine error tolerance. By identifying error-sensitive memory blocks and protecting only these blocks, the proposed ECC and TMR methods significantly reduce the incurred cost, highlighting the importance of exploiting inherent error tolerance in developing memory protection strategies.

\subsection{Cross-Layer and Software-Hardware Cooperative Approaches}

Hanif et al. \cite{hanif_cross-layer_2020} explore software and hardware modifications to improve the resilience of DNN-based systems against hardware-level faults. This cross-layer approach emphasizes the need for cooperative strategies that leverage the intrinsic characteristics of DNNs to enhance fault tolerance.

Liu et al. \cite{liu_monitor_2024} address the optimization of hardware monitor placement in systolic arrays for DNN accelerators. By optimizing monitor placement, this study contributes to improving hardware fault tolerance in DNN accelerators, showcasing the importance of strategic hardware modifications in enhancing system reliability.

In conclusion, software and algorithmic strategies play a crucial role in complementing hardware fault tolerance in deep learning systems. From innovative error correction methods and machine learning-guided frameworks to dynamic error handling and cross-layer approaches, these strategies offer diverse solutions to ensuring the robustness and reliability of deep learning applications in the face of hardware errors. As the field continues to evolve, further research into cooperative software-hardware approaches and the exploration of novel fault mitigation techniques will be essential in advancing the fault tolerance of deep learning systems.
\section{Evaluating and Enhancing Reliability in Deep Learning Hardware}

This section focuses on methodologies for evaluating the reliability of deep learning hardware and strategies for enhancing this reliability. It covers fault injection tools, resilience analysis frameworks, and the impact of hardware faults on deep learning performance, particularly in safety-critical applications. The reliability and robustness of deep learning hardware are paramount, especially as these systems are increasingly deployed in safety-critical environments such as autonomous vehicles, healthcare, and aerospace. The literature reviewed herein spans a broad spectrum of approaches, from fault injection and resilience assessment methodologies to specific fault tolerance designs in hardware components.

\subsection{Resilience Assessment and Fault Injection Methodologies}

The resilience of deep learning (DL) models and the impact of hardware faults on their performance have been a growing area of research. \cite{ruospo_survey_2023} provides a comprehensive survey on deep learning resilience assessment methodologies, highlighting the distributed and parallel structure of DL models which lends them an inherent fault tolerance. However, the resilience of these models is not absolute and varies with the choice of hardware, necessitating a thorough assessment of the entire system stack. This survey categorizes evaluation techniques and discusses their costs, implementation efforts, and the trade-offs between resilience and performance.

Fault injection tools and techniques play a crucial role in evaluating system behavior under faults. \cite{salih_survey_2022} surveys several fault injection tools, comparing them based on their functionality and feedback mechanisms. While this paper does not specifically address deep learning systems, it provides valuable context for understanding general methodologies for assessing hardware fault tolerance.

\subsection{Fault Tolerance Strategies in Hardware Design}

Checkpointing strategies, as discussed in \cite{akturk_acr_2020}, offer a general approach to fault tolerance that could be adapted for deep learning systems. The paper explores how recomputation of data values can reduce checkpointing overhead, suggesting a potential area for enhancing deep learning hardware resilience with minimal performance impact.

The dependability of DNN accelerators, especially from an aging perspective, is crucial for ensuring long-term reliability. \cite{moghaddasi_dependable_2023} reviews techniques to evaluate and mitigate aging effects in DNN accelerators, highlighting the importance of designing for dependability in safety-critical systems.

\cite{abich_applying_2021} investigates soft error mitigation in embedded mixed precision DNNs, demonstrating the trade-offs between precision, performance, and reliability. This work underscores the need for fault-tolerant designs that consider the unique constraints of low-power deep learning applications.

\subsection{Emerging Approaches and Future Directions}

\cite{kundu_diagnnose_2024} introduces DiagNNose, a framework for error localization in DL accelerators, marking a significant step towards enhancing fault tolerance by precisely identifying and managing hardware errors. This approach could dramatically improve the reliability of deep learning hardware in mission-critical applications.

The effect of hardware-level permanent faults in GPUs on the reliability of CNNs is evaluated in \cite{guerrero-balaguera_effective_2022}, providing key insights into the vulnerabilities of deep learning performance to hardware faults. This study emphasizes the need for accurate fault simulation and reliability estimation in safety-critical applications.

\cite{chen_fault_2018} introduces an efficient algorithm-based fault tolerance (ABFT) approach for matrix decomposition on GPUs, addressing soft errors and PCIe communication faults. This paper highlights advancements in fault tolerance methods that are directly applicable to deep learning computations on heterogeneous systems.

In summary, enhancing the reliability of deep learning hardware requires a multifaceted approach, encompassing resilience assessment, fault injection methodologies, and specific fault tolerance strategies in hardware design. Future research should focus on developing more efficient fault tolerance mechanisms, improving error localization techniques, and exploring the potential of machine learning enhancements to further bolster fault tolerance in deep learning systems.
\section{Future Directions and Emerging Technologies}

The final section discusses future directions in fault-tolerant hardware design for deep learning, including emerging technologies such as neuromorphic computing and memristors. It also explores the potential of novel computing paradigms, such as quantum computing and analog neural networks, for enhancing fault tolerance in deep learning systems. The exploration of these technologies and paradigms reveals a landscape rich with opportunities for innovation in the realm of hardware fault tolerance, crucial for the next generation of deep learning applications.

\subsection{Neuromorphic Computing and Memristors}

Neuromorphic computing, inspired by the neural structures of the human brain, presents a promising avenue for fault-tolerant deep learning hardware. The work by Yerima et al. on a fault-tolerant spiking neural network mapping algorithm and architecture for neuromorphic systems highlights the potential of neuromorphic computing in addressing noise and external interference, which are common issues in deep learning hardware \cite{yerima_fault-tolerant_2023}. Similarly, Jeon et al.'s investigation into hybrid precision in resistive memory-based convolutional kernels for neuromorphic systems underscores the role of novel memory technologies, like memristors, in enhancing fault resilience \cite{jeon_hybrid_2023}. These studies suggest that leveraging the inherent fault tolerance of neuromorphic computing and the unique properties of memristors could lead to more robust deep learning systems.

\subsection{Hybrid and Specialized Architectures}

The exploration of hybrid and specialized computing architectures offers another path toward fault-tolerant deep learning. The CHREC Space Processor (CSP) project, which combines commercial-off-the-shelf devices with radiation-hardened processors and fault-tolerant computing techniques, exemplifies the potential of hybrid architectures in specialized environments such as space computing \cite{rudolph_csp_nodate}. This approach could inspire similar strategies in deep learning hardware, where a mix of cutting-edge and robust technologies ensures both performance and reliability. Furthermore, the development of the Hybrid Computing Architecture (HyCA) for fault-tolerant deep learning accelerators, as proposed by Liu et al., demonstrates the effectiveness of innovative architecture designs in overcoming the limitations of conventional redundancy approaches \cite{liu_hyca_2021}.

\subsection{Fault Tolerance Techniques and Strategies}

Advancements in fault tolerance techniques and strategies specifically tailored for deep learning hardware are critical for mitigating the impact of hardware errors. The work by Yuan et al. on improving DNN fault tolerance using weight pruning and differential crossbar mapping for ReRAM-based edge AI showcases a specific approach to enhancing hardware fault tolerance \cite{yuan_improving_2021}. Similarly, Chen et al.'s pruning technique for fault-tolerant memristor-based accelerators offers insights into efficiently identifying and mitigating critical faults in hardware \cite{chen_pruning_2021}. These techniques, along with the novel methodologies proposed by Putra et al. in RescueSNN for mitigating permanent faults in SNN accelerators, underscore the importance of developing targeted strategies for enhancing the reliability of deep learning hardware \cite{putra_rescuesnn_2023}.

\subsection{Cross-Layer and System-Wide Approaches}

A comprehensive understanding of fault propagation and the development of system-wide fault tolerance approaches are essential for the next generation of deep learning systems. Liu et al.'s cross-layer fault propagation analysis method for edge intelligence systems deployed with DNNs provides a data-driven strategy for evaluating the impact of soft errors and designing more resilient systems \cite{liu_cross-layer_2021}. Additionally, the novel buffering fault-tolerance approach for Network-on-Chip (NoC) by Jafarzadeh et al. highlights the significance of ensuring communication reliability within chip multiprocessors, indirectly supporting deep learning computations \cite{jafarzadeh_novel_2023}.

In conclusion, the future of fault-tolerant hardware design for deep learning is poised at the intersection of emerging technologies, hybrid and specialized architectures, advanced fault tolerance techniques, and cross-layer approaches. The exploration and integration of these elements will be crucial for developing deep learning systems that are not only powerful and efficient but also robust and reliable in the face of hardware errors.
